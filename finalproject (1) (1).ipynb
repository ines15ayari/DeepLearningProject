{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":94327,"sourceType":"datasetVersion","datasetId":50445}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-27T13:18:06.784049Z","iopub.execute_input":"2023-12-27T13:18:06.784964Z","iopub.status.idle":"2023-12-27T13:18:07.113671Z","shell.execute_reply.started":"2023-12-27T13:18:06.784930Z","shell.execute_reply":"2023-12-27T13:18:07.112753Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/conll003-englishversion/valid.txt\n/kaggle/input/conll003-englishversion/metadata\n/kaggle/input/conll003-englishversion/test.txt\n/kaggle/input/conll003-englishversion/train.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Abstract\nOur project introduces a streamlined Named Entity Recognition (NER) system utilizing BERT for contextual understanding and Bi-LSTM for sequential data processing. Trained on the CoNLL-2003 dataset, this system efficiently identifies and categorizes key entities such as names and locations. The integration of AdamW optimization and hyperparameter tuning ensures high precision and recall. The final model undergoes rigorous evaluation, with a feedback mechanism for continuous improvement, culminating in a robust solution for extracting structured information from text.","metadata":{}},{"cell_type":"markdown","source":"# Project Outline: Named Entity Recognition with BERT and Bi-LSTM\n\n## 1. Introduction\n- Significance of NER in natural language processing.\n- Rationale for selecting BERT and Bi-LSTM models.\n\n## 2. Dataset Preparation\n- Overview of the CoNLL-2003 dataset.\n- Steps for data loading and preprocessing.\n- Visualization of tag distribution within the dataset.\n\n## 3. Model Architecture\n- Explanation of the BERT tokenizer and token classification model.\n- Architecture of the Bi-LSTM model.\n- Integration strategy for BERT embeddings with Bi-LSTM.\n\n## 4. Training Process\n- Configuration of the training environment using PyTorch and GPU setup.\n- Construction of DataLoader for batching and iteration.\n- Outline of the training loop, including loss calculation and backpropagation.\n- Strategy for hyperparameter tuning and learning rate scheduling.\n\n## 5. Evaluation\n- Metrics used for assessing model performance.\n- Validation process and implementation of the feedback loop for model refinement.\n\n## 6. Testing and Model Saving\n- Protocol for testing the model against the test dataset.\n- Method for saving the trained model for subsequent use.\n\n## 7. Inference\n- Procedure for loading the trained model for inference.\n- Process of predicting named entities on new textual data.\n- Post-processing steps for refining the output.\n\n## 8. Conclusion\n- Recap of the project's objectives and results.\n- Discussion on the effectiveness of the model and its applications.\n- Considerations for future improvements and research directions.\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction\nThe advent of deep learning has revolutionized the field of Natural Language Processing (NLP), particularly in tasks like Named Entity Recognition (NER). NER is crucial for information retrieval, summarization, and knowledge extraction, enabling the conversion of unstructured text into structured data. Our project capitalizes on this by leveraging two powerful models: BERT, which provides a deep understanding of language context, and Bi-LSTM, which excels in learning from data sequences. The CoNLL-2003 dataset, with its rich annotations, serves as the training ground for our NER system. By optimizing the training process and continuously refining the model's performance, we aim to deliver a state-of-the-art system that pushes the boundaries of what's possible with NER in various applications.","metadata":{}},{"cell_type":"markdown","source":"# Architecture of the System\n![4.png](attachment:f6713e47-5e80-43e1-acb5-4f849625df99.png)","metadata":{},"attachments":{"f6713e47-5e80-43e1-acb5-4f849625df99.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAr0AAAKVCAYAAADC21ODAAAAAXNSR0IArs4c6QAAPA50RVh0bXhmaWxlACUzQ214ZmlsZSUyMGhvc3QlM0QlMjJFbGVjdHJvbiUyMiUyMG1vZGlmaWVkJTNEJTIyMjAyMy0xMi0yOFQwMCUzQTM1JTNBMzAuNjg3WiUyMiUyMGFnZW50JTNEJTIyTW96aWxsYSUyRjUuMCUyMChXaW5kb3dzJTIwTlQlMjAxMC4wJTNCJTIwV2luNjQlM0IlMjB4NjQpJTIwQXBwbGVXZWJLaXQlMkY1MzcuMzYlMjAoS0hUTUwlMkMlMjBsaWtlJTIwR2Vja28pJTIwZHJhdy5pbyUyRjIxLjguMiUyMENocm9tZSUyRjExNC4wLjU3MzUuMjg5JTIwRWxlY3Ryb24lMkYyNS44LjElMjBTYWZhcmklMkY1MzcuMzYlMjIlMjBldGFnJTNEJTIyOHB1WElEXzQ3QjI1dzdaOF85ZlAlMjIlMjB2ZXJzaW9uJTNEJTIyMjEuOC4yJTIyJTIwdHlwZSUzRCUyMmRldmljZSUyMiUzRSUwQSUyMCUyMCUzQ2RpYWdyYW0lMjBpZCUzRCUyMkM1UkJzNDNvRGEtS2R6WmVOdHV5JTIyJTIwbmFtZSUzRCUyMlBhZ2UtMSUyMiUzRSUwQSUyMCUyMCUyMCUyMCUzQ214R3JhcGhNb2RlbCUyMGR4JTNEJTIyMjM0NyUyMiUyMGR5JTNEJTIyMjA5MyUyMiUyMGdyaWQlM0QlMjIxJTIyJTIwZ3JpZFNpemUlM0QlMjIxMCUyMiUyMGd1aWRlcyUzRCUyMjElMjIlMjB0b29sdGlwcyUzRCUyMjElMjIlMjBjb25uZWN0JTNEJTIyMSUyMiUyMGFycm93cyUzRCUyMjElMjIlMjBmb2xkJTNEJTIyMSUyMiUyMHBhZ2UlM0QlMjIxJTIyJTIwcGFnZVNjYWxlJTNEJTIyMSUyMiUyMHBhZ2VXaWR0aCUzRCUyMjgyNyUyMiUyMHBhZ2VIZWlnaHQlM0QlMjIxMTY5JTIyJTIwbWF0aCUzRCUyMjAlMjIlMjBzaGFkb3clM0QlMjIwJTIyJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTNDcm9vdCUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214Q2VsbCUyMGlkJTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMCUyMiUyMCUyRiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214Q2VsbCUyMGlkJTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUyMHBhcmVudCUzRCUyMldJeVdsTGs2R0pRc3FhVUJLVE5WLTAlMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTE1JTIyJTIwc3R5bGUlM0QlMjJlZGdlU3R5bGUlM0RvcnRob2dvbmFsRWRnZVN0eWxlJTNCcm91bmRlZCUzRDAlM0JvcnRob2dvbmFsTG9vcCUzRDElM0JqZXR0eVNpemUlM0RhdXRvJTNCaHRtbCUzRDElM0JleGl0WCUzRDAlM0JleGl0WSUzRDAlM0JleGl0RHglM0QwJTNCZXhpdER5JTNEMCUzQmVudHJ5WCUzRDElM0JlbnRyeVklM0QwLjUlM0JlbnRyeUR4JTNEMCUzQmVudHJ5RHklM0QwJTNCJTIyJTIwZWRnZSUzRCUyMjElMjIlMjBwYXJlbnQlM0QlMjJXSXlXbExrNkdKUXNxYVVCS1ROVi0xJTIyJTIwc291cmNlJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMCUyMiUyMHRhcmdldCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTExJTIyJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhHZW9tZXRyeSUyMHJlbGF0aXZlJTNEJTIyMSUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTI5JTIyJTIwc3R5bGUlM0QlMjJlZGdlU3R5bGUlM0RvcnRob2dvbmFsRWRnZVN0eWxlJTNCcm91bmRlZCUzRDAlM0JvcnRob2dvbmFsTG9vcCUzRDElM0JqZXR0eVNpemUlM0RhdXRvJTNCaHRtbCUzRDElM0JleGl0WCUzRDAuNSUzQmV4aXRZJTNEMSUzQmV4aXREeCUzRDAlM0JleGl0RHklM0QwJTNCJTIyJTIwZWRnZSUzRCUyMjElMjIlMjBwYXJlbnQlM0QlMjJXSXlXbExrNkdKUXNxYVVCS1ROVi0xJTIyJTIwc291cmNlJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMCUyMiUyMHRhcmdldCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTElMjIlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteEdlb21ldHJ5JTIwcmVsYXRpdmUlM0QlMjIxJTIyJTIwYXMlM0QlMjJnZW9tZXRyeSUyMiUyMCUyRiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQyUyRm14Q2VsbCUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214Q2VsbCUyMGlkJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMCUyMiUyMHZhbHVlJTNEJTIyVGV4dCUyMFByZS1wcm9jZXNzaW5nJTIyJTIwc3R5bGUlM0QlMjJyb3VuZGVkJTNEMCUzQndoaXRlU3BhY2UlM0R3cmFwJTNCaHRtbCUzRDElM0IlMjIlMjB2ZXJ0ZXglM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjB4JTNEJTIyMjAlMjIlMjB5JTNEJTIyMTIwJTIyJTIwd2lkdGglM0QlMjIxMjAlMjIlMjBoZWlnaHQlM0QlMjI4MCUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTE5JTIyJTIwc3R5bGUlM0QlMjJlZGdlU3R5bGUlM0RvcnRob2dvbmFsRWRnZVN0eWxlJTNCcm91bmRlZCUzRDAlM0JvcnRob2dvbmFsTG9vcCUzRDElM0JqZXR0eVNpemUlM0RhdXRvJTNCaHRtbCUzRDElM0JleGl0WCUzRDElM0JleGl0WSUzRDAuNSUzQmV4aXREeCUzRDAlM0JleGl0RHklM0QwJTNCZW50cnlYJTNEMCUzQmVudHJ5WSUzRDAuNSUzQmVudHJ5RHglM0QwJTNCZW50cnlEeSUzRDAlM0IlMjIlMjBlZGdlJTNEJTIyMSUyMiUyMHBhcmVudCUzRCUyMldJeVdsTGs2R0pRc3FhVUJLVE5WLTElMjIlMjBzb3VyY2UlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai0xJTIyJTIwdGFyZ2V0JTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotNyUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjByZWxhdGl2ZSUzRCUyMjElMjIlMjBhcyUzRCUyMmdlb21ldHJ5JTIyJTIwJTJGJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDJTJGbXhDZWxsJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhDZWxsJTIwaWQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai0xJTIyJTIwdmFsdWUlM0QlMjJXb3JkJTIwRW1iZWRkaW5nJTIwTGF5ZXIlMjZsdCUzQmJyJTI2Z3QlM0IoVG9rZW5zJTIwYXJlJTIwY29udmVydGVkJTIwaW50byUyMHZlY3RvcnMpJTIyJTIwc3R5bGUlM0QlMjJyb3VuZGVkJTNEMCUzQndoaXRlU3BhY2UlM0R3cmFwJTNCaHRtbCUzRDElM0IlMjIlMjB2ZXJ0ZXglM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjB4JTNEJTIyMTQlMjIlMjB5JTNEJTIyNDAxJTIyJTIwd2lkdGglM0QlMjIxMzAlMjIlMjBoZWlnaHQlM0QlMjI4MCUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTIlMjIlMjB2YWx1ZSUzRCUyMlVzZXIlMjIlMjBzdHlsZSUzRCUyMnNoYXBlJTNEdW1sQWN0b3IlM0J2ZXJ0aWNhbExhYmVsUG9zaXRpb24lM0Rib3R0b20lM0J2ZXJ0aWNhbEFsaWduJTNEdG9wJTNCaHRtbCUzRDElM0JvdXRsaW5lQ29ubmVjdCUzRDAlM0IlMjIlMjB2ZXJ0ZXglM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjB4JTNEJTIyNjQlMjIlMjB5JTNEJTIyLTE0OSUyMiUyMHdpZHRoJTNEJTIyMzAlMjIlMjBoZWlnaHQlM0QlMjI2MCUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTMlMjIlMjB2YWx1ZSUzRCUyMiUyMiUyMHN0eWxlJTNEJTIyZW5kQXJyb3clM0RjbGFzc2ljJTNCaHRtbCUzRDElM0Jyb3VuZGVkJTNEMCUzQiUyMiUyMGVkZ2UlM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjB3aWR0aCUzRCUyMjUwJTIyJTIwaGVpZ2h0JTNEJTIyNTAlMjIlMjByZWxhdGl2ZSUzRCUyMjElMjIlMjBhcyUzRCUyMmdlb21ldHJ5JTIyJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhQb2ludCUyMHglM0QlMjI3OSUyMiUyMHklM0QlMjItNjAlMjIlMjBhcyUzRCUyMnNvdXJjZVBvaW50JTIyJTIwJTJGJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhQb2ludCUyMHglM0QlMjI3OSUyMiUyMGFzJTNEJTIydGFyZ2V0UG9pbnQlMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteEdlb21ldHJ5JTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDJTJGbXhDZWxsJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhDZWxsJTIwaWQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai01JTIyJTIwdmFsdWUlM0QlMjJUeXBlJTIwaW5wdXQlMjIlMjBzdHlsZSUzRCUyMnRleHQlM0JodG1sJTNEMSUzQnN0cm9rZUNvbG9yJTNEbm9uZSUzQmZpbGxDb2xvciUzRG5vbmUlM0JhbGlnbiUzRGNlbnRlciUzQnZlcnRpY2FsQWxpZ24lM0RtaWRkbGUlM0J3aGl0ZVNwYWNlJTNEd3JhcCUzQnJvdW5kZWQlM0QwJTNCJTIyJTIwdmVydGV4JTNEJTIyMSUyMiUyMHBhcmVudCUzRCUyMldJeVdsTGs2R0pRc3FhVUJLVE5WLTElMjIlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteEdlb21ldHJ5JTIweCUzRCUyMjg0JTIyJTIweSUzRCUyMi00OSUyMiUyMHdpZHRoJTNEJTIyNjAlMjIlMjBoZWlnaHQlM0QlMjIzMCUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTMwJTIyJTIwc3R5bGUlM0QlMjJlZGdlU3R5bGUlM0RvcnRob2dvbmFsRWRnZVN0eWxlJTNCcm91bmRlZCUzRDAlM0JvcnRob2dvbmFsTG9vcCUzRDElM0JqZXR0eVNpemUlM0RhdXRvJTNCaHRtbCUzRDElM0JleGl0WCUzRDAuNSUzQmV4aXRZJTNEMSUzQmV4aXREeCUzRDAlM0JleGl0RHklM0QwJTNCZW50cnlYJTNEMC41JTNCZW50cnlZJTNEMCUzQmVudHJ5RHglM0QwJTNCZW50cnlEeSUzRDAlM0IlMjIlMjBlZGdlJTNEJTIyMSUyMiUyMHBhcmVudCUzRCUyMldJeVdsTGs2R0pRc3FhVUJLVE5WLTElMjIlMjBzb3VyY2UlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai02JTIyJTIwdGFyZ2V0JTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMCUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjByZWxhdGl2ZSUzRCUyMjElMjIlMjBhcyUzRCUyMmdlb21ldHJ5JTIyJTIwJTJGJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDJTJGbXhDZWxsJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhDZWxsJTIwaWQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai02JTIyJTIwdmFsdWUlM0QlMjJJbnB1dCUyMERhdGElMjZhbXAlM0JuYnNwJTNCJTI2bHQlM0JiciUyNmd0JTNCKENvTkxMLTIwMDMpJTIyJTIwc3R5bGUlM0QlMjJyb3VuZGVkJTNEMCUzQndoaXRlU3BhY2UlM0R3cmFwJTNCaHRtbCUzRDElM0IlMjIlMjB2ZXJ0ZXglM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjB4JTNEJTIyMjAlMjIlMjB3aWR0aCUzRCUyMjEyMCUyMiUyMGhlaWdodCUzRCUyMjYwJTIyJTIwYXMlM0QlMjJnZW9tZXRyeSUyMiUyMCUyRiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQyUyRm14Q2VsbCUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214Q2VsbCUyMGlkJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMjAlMjIlMjBzdHlsZSUzRCUyMmVkZ2VTdHlsZSUzRG9ydGhvZ29uYWxFZGdlU3R5bGUlM0Jyb3VuZGVkJTNEMCUzQm9ydGhvZ29uYWxMb29wJTNEMSUzQmpldHR5U2l6ZSUzRGF1dG8lM0JodG1sJTNEMSUzQmV4aXRYJTNEMSUzQmV4aXRZJTNEMC41JTNCZXhpdER4JTNEMCUzQmV4aXREeSUzRDAlM0JlbnRyeVglM0QwJTNCZW50cnlZJTNEMC41JTNCZW50cnlEeCUzRDAlM0JlbnRyeUR5JTNEMCUzQiUyMiUyMGVkZ2UlM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUyMHNvdXJjZSUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTclMjIlMjB0YXJnZXQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai04JTIyJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhHZW9tZXRyeSUyMHJlbGF0aXZlJTNEJTIyMSUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTclMjIlMjB2YWx1ZSUzRCUyMk5FUiUyME1vZGVsJTI2bHQlM0JiciUyNmd0JTNCKEJlcnQlMkMlMjBCaS1MU1RNKSUyMiUyMHN0eWxlJTNEJTIycm91bmRlZCUzRDAlM0J3aGl0ZVNwYWNlJTNEd3JhcCUzQmh0bWwlM0QxJTNCJTIyJTIwdmVydGV4JTNEJTIyMSUyMiUyMHBhcmVudCUzRCUyMldJeVdsTGs2R0pRc3FhVUJLVE5WLTElMjIlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteEdlb21ldHJ5JTIweCUzRCUyMjIzMCUyMiUyMHklM0QlMjI0MDAlMjIlMjB3aWR0aCUzRCUyMjEzMCUyMiUyMGhlaWdodCUzRCUyMjgwJTIyJTIwYXMlM0QlMjJnZW9tZXRyeSUyMiUyMCUyRiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQyUyRm14Q2VsbCUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214Q2VsbCUyMGlkJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMjUlMjIlMjBzdHlsZSUzRCUyMmVkZ2VTdHlsZSUzRG9ydGhvZ29uYWxFZGdlU3R5bGUlM0Jyb3VuZGVkJTNEMCUzQm9ydGhvZ29uYWxMb29wJTNEMSUzQmpldHR5U2l6ZSUzRGF1dG8lM0JodG1sJTNEMSUzQmV4aXRYJTNEMC41JTNCZXhpdFklM0QwJTNCZXhpdER4JTNEMCUzQmV4aXREeSUzRDAlM0JlbnRyeVglM0QwLjUlM0JlbnRyeVklM0QxJTNCZW50cnlEeCUzRDAlM0JlbnRyeUR5JTNEMCUzQiUyMiUyMGVkZ2UlM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUyMHNvdXJjZSUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTglMjIlMjB0YXJnZXQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai0yMSUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjByZWxhdGl2ZSUzRCUyMjElMjIlMjBhcyUzRCUyMmdlb21ldHJ5JTIyJTIwJTJGJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDJTJGbXhDZWxsJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhDZWxsJTIwaWQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai04JTIyJTIwdmFsdWUlM0QlMjJDbGFzc2lmaWNhdGlvbiUyMExheWVyJTI2bHQlM0JiciUyNmd0JTNCJTI2bHQlM0JkaXYlMjZndCUzQiUyNmFtcCUzQm5ic3AlM0IoRm9yJTIwcHJlZGljdGluZyUyME5FUiUyMHRhZ3MpJTI2bHQlM0IlMkZkaXYlMjZndCUzQiUyMiUyMHN0eWxlJTNEJTIycm91bmRlZCUzRDAlM0J3aGl0ZVNwYWNlJTNEd3JhcCUzQmh0bWwlM0QxJTNCJTIyJTIwdmVydGV4JTNEJTIyMSUyMiUyMHBhcmVudCUzRCUyMldJeVdsTGs2R0pRc3FhVUJLVE5WLTElMjIlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteEdlb21ldHJ5JTIweCUzRCUyMjQzMCUyMiUyMHklM0QlMjI0MDAlMjIlMjB3aWR0aCUzRCUyMjEzMCUyMiUyMGhlaWdodCUzRCUyMjgwJTIyJTIwYXMlM0QlMjJnZW9tZXRyeSUyMiUyMCUyRiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQyUyRm14Q2VsbCUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214Q2VsbCUyMGlkJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMTQlMjIlMjBzdHlsZSUzRCUyMmVkZ2VTdHlsZSUzRG9ydGhvZ29uYWxFZGdlU3R5bGUlM0Jyb3VuZGVkJTNEMCUzQm9ydGhvZ29uYWxMb29wJTNEMSUzQmpldHR5U2l6ZSUzRGF1dG8lM0JodG1sJTNEMSUzQmV4aXRYJTNEMC41JTNCZXhpdFklM0QxJTNCZXhpdER4JTNEMCUzQmV4aXREeSUzRDAlM0JlbnRyeVglM0QwLjUlM0JlbnRyeVklM0QwJTNCZW50cnlEeCUzRDAlM0JlbnRyeUR5JTNEMCUzQiUyMiUyMGVkZ2UlM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUyMHNvdXJjZSUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTklMjIlMjB0YXJnZXQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai0xMiUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjByZWxhdGl2ZSUzRCUyMjElMjIlMjBhcyUzRCUyMmdlb21ldHJ5JTIyJTIwJTJGJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDJTJGbXhDZWxsJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhDZWxsJTIwaWQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai05JTIyJTIwdmFsdWUlM0QlMjJUb2tlbml6YXRpb24lMjIlMjBzdHlsZSUzRCUyMnJvdW5kZWQlM0QwJTNCd2hpdGVTcGFjZSUzRHdyYXAlM0JodG1sJTNEMSUzQiUyMiUyMHZlcnRleCUzRCUyMjElMjIlMjBwYXJlbnQlM0QlMjJXSXlXbExrNkdKUXNxYVVCS1ROVi0xJTIyJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhHZW9tZXRyeSUyMHglM0QlMjItMTM1JTIyJTIweSUzRCUyMjE3MCUyMiUyMHdpZHRoJTNEJTIyMTAwJTIyJTIwaGVpZ2h0JTNEJTIyNDAlMjIlMjBhcyUzRCUyMmdlb21ldHJ5JTIyJTIwJTJGJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDJTJGbXhDZWxsJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhDZWxsJTIwaWQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai0xMyUyMiUyMHN0eWxlJTNEJTIyZWRnZVN0eWxlJTNEb3J0aG9nb25hbEVkZ2VTdHlsZSUzQnJvdW5kZWQlM0QwJTNCb3J0aG9nb25hbExvb3AlM0QxJTNCamV0dHlTaXplJTNEYXV0byUzQmh0bWwlM0QxJTNCZXhpdFglM0QwLjUlM0JleGl0WSUzRDElM0JleGl0RHglM0QwJTNCZXhpdER5JTNEMCUzQmVudHJ5WCUzRDAuNSUzQmVudHJ5WSUzRDAlM0JlbnRyeUR4JTNEMCUzQmVudHJ5RHklM0QwJTNCJTIyJTIwZWRnZSUzRCUyMjElMjIlMjBwYXJlbnQlM0QlMjJXSXlXbExrNkdKUXNxYVVCS1ROVi0xJTIyJTIwc291cmNlJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMTElMjIlMjB0YXJnZXQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai05JTIyJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhHZW9tZXRyeSUyMHJlbGF0aXZlJTNEJTIyMSUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTExJTIyJTIwdmFsdWUlM0QlMjJEYXRhJTIwTG9hZGluZyUyMiUyMHN0eWxlJTNEJTIycm91bmRlZCUzRDAlM0J3aGl0ZVNwYWNlJTNEd3JhcCUzQmh0bWwlM0QxJTNCJTIyJTIwdmVydGV4JTNEJTIyMSUyMiUyMHBhcmVudCUzRCUyMldJeVdsTGs2R0pRc3FhVUJLVE5WLTElMjIlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteEdlb21ldHJ5JTIweCUzRCUyMi0xNDAlMjIlMjB5JTNEJTIyMTAwJTIyJTIwd2lkdGglM0QlMjIxMDAlMjIlMjBoZWlnaHQlM0QlMjI0MCUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTE3JTIyJTIwc3R5bGUlM0QlMjJlZGdlU3R5bGUlM0RvcnRob2dvbmFsRWRnZVN0eWxlJTNCcm91bmRlZCUzRDAlM0JvcnRob2dvbmFsTG9vcCUzRDElM0JqZXR0eVNpemUlM0RhdXRvJTNCaHRtbCUzRDElM0JleGl0WCUzRDElM0JleGl0WSUzRDAuNSUzQmV4aXREeCUzRDAlM0JleGl0RHklM0QwJTNCZW50cnlYJTNEMC4yNSUzQmVudHJ5WSUzRDElM0JlbnRyeUR4JTNEMCUzQmVudHJ5RHklM0QwJTNCJTIyJTIwZWRnZSUzRCUyMjElMjIlMjBwYXJlbnQlM0QlMjJXSXlXbExrNkdKUXNxYVVCS1ROVi0xJTIyJTIwc291cmNlJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMTIlMjIlMjB0YXJnZXQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai0wJTIyJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhHZW9tZXRyeSUyMHJlbGF0aXZlJTNEJTIyMSUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTEyJTIyJTIwdmFsdWUlM0QlMjJEYXRhJTIwVHJhbnNmb3JtYXRpb24lMjZsdCUzQmJyJTI2Z3QlM0IocGFkZGluZyUyQyUyMHRydW5jYXRpb24pJTIyJTIwc3R5bGUlM0QlMjJyb3VuZGVkJTNEMCUzQndoaXRlU3BhY2UlM0R3cmFwJTNCaHRtbCUzRDElM0IlMjIlMjB2ZXJ0ZXglM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjB4JTNEJTIyLTE0MCUyMiUyMHklM0QlMjIyNDAlMjIlMjB3aWR0aCUzRCUyMjExMCUyMiUyMGhlaWdodCUzRCUyMjYwJTIyJTIwYXMlM0QlMjJnZW9tZXRyeSUyMiUyMCUyRiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQyUyRm14Q2VsbCUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214Q2VsbCUyMGlkJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMjYlMjIlMjBzdHlsZSUzRCUyMmVkZ2VTdHlsZSUzRG9ydGhvZ29uYWxFZGdlU3R5bGUlM0Jyb3VuZGVkJTNEMCUzQm9ydGhvZ29uYWxMb29wJTNEMSUzQmpldHR5U2l6ZSUzRGF1dG8lM0JodG1sJTNEMSUzQmV4aXRYJTNEMC41JTNCZXhpdFklM0QwJTNCZXhpdER4JTNEMCUzQmV4aXREeSUzRDAlM0JlbnRyeVglM0QwLjUlM0JlbnRyeVklM0QxJTNCZW50cnlEeCUzRDAlM0JlbnRyeUR5JTNEMCUzQiUyMiUyMGVkZ2UlM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUyMHNvdXJjZSUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTIxJTIyJTIwdGFyZ2V0JTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMjMlMjIlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteEdlb21ldHJ5JTIwcmVsYXRpdmUlM0QlMjIxJTIyJTIwYXMlM0QlMjJnZW9tZXRyeSUyMiUyMCUyRiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQyUyRm14Q2VsbCUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214Q2VsbCUyMGlkJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMjElMjIlMjB2YWx1ZSUzRCUyMlRyYWluaW5nJTI2bHQlM0JiciUyNmd0JTNCJTI2YW1wJTNCYW1wJTNCJTI2bHQlM0JiciUyNmd0JTNCSHlwZXIlMjBwYXJhbWV0ZXIlMjBUdW5pbmclMjIlMjBzdHlsZSUzRCUyMnJvdW5kZWQlM0QwJTNCd2hpdGVTcGFjZSUzRHdyYXAlM0JodG1sJTNEMSUzQiUyMiUyMHZlcnRleCUzRCUyMjElMjIlMjBwYXJlbnQlM0QlMjJXSXlXbExrNkdKUXNxYVVCS1ROVi0xJTIyJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhHZW9tZXRyeSUyMHglM0QlMjI0MzAlMjIlMjB5JTNEJTIyMjYwJTIyJTIwd2lkdGglM0QlMjIxMzAlMjIlMjBoZWlnaHQlM0QlMjI4MCUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTI3JTIyJTIwc3R5bGUlM0QlMjJlZGdlU3R5bGUlM0RvcnRob2dvbmFsRWRnZVN0eWxlJTNCcm91bmRlZCUzRDAlM0JvcnRob2dvbmFsTG9vcCUzRDElM0JqZXR0eVNpemUlM0RhdXRvJTNCaHRtbCUzRDElM0JleGl0WCUzRDAuNSUzQmV4aXRZJTNEMCUzQmV4aXREeCUzRDAlM0JleGl0RHklM0QwJTNCJTIyJTIwZWRnZSUzRCUyMjElMjIlMjBwYXJlbnQlM0QlMjJXSXlXbExrNkdKUXNxYVVCS1ROVi0xJTIyJTIwc291cmNlJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMjMlMjIlMjB0YXJnZXQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai0yNCUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjByZWxhdGl2ZSUzRCUyMjElMjIlMjBhcyUzRCUyMmdlb21ldHJ5JTIyJTIwJTJGJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDJTJGbXhDZWxsJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhDZWxsJTIwaWQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai0yMyUyMiUyMHZhbHVlJTNEJTIyRXZhbHVhdGlvbiUyNmx0JTNCYnIlMjZndCUzQiUyNmFtcCUzQmFtcCUzQiUyNmx0JTNCYnIlMjZndCUzQkZlZWRiYWNrJTIwbG9vcCUyMiUyMHN0eWxlJTNEJTIycm91bmRlZCUzRDAlM0J3aGl0ZVNwYWNlJTNEd3JhcCUzQmh0bWwlM0QxJTNCJTIyJTIwdmVydGV4JTNEJTIyMSUyMiUyMHBhcmVudCUzRCUyMldJeVdsTGs2R0pRc3FhVUJLVE5WLTElMjIlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteEdlb21ldHJ5JTIweCUzRCUyMjQzMCUyMiUyMHklM0QlMjIxMzAlMjIlMjB3aWR0aCUzRCUyMjEzMCUyMiUyMGhlaWdodCUzRCUyMjgwJTIyJTIwYXMlM0QlMjJnZW9tZXRyeSUyMiUyMCUyRiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQyUyRm14Q2VsbCUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214Q2VsbCUyMGlkJTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotMzElMjIlMjB2YWx1ZSUzRCUyMk91dHB1dCUyMiUyMHN0eWxlJTNEJTIyZWRnZVN0eWxlJTNEb3J0aG9nb25hbEVkZ2VTdHlsZSUzQnJvdW5kZWQlM0QwJTNCb3J0aG9nb25hbExvb3AlM0QxJTNCamV0dHlTaXplJTNEYXV0byUzQmh0bWwlM0QxJTNCZXhpdFglM0QwJTNCZXhpdFklM0QwLjI1JTNCZXhpdER4JTNEMCUzQmV4aXREeSUzRDAlM0JlbnRyeVglM0QxJTNCZW50cnlZJTNEMC41JTNCZW50cnlEeCUzRDAlM0JlbnRyeUR5JTNEMCUzQiUyMiUyMGVkZ2UlM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUyMHNvdXJjZSUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTI0JTIyJTIwdGFyZ2V0JTNEJTIyQXo3OTFudS1YcGlwVW5OUHV4emotNiUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjByZWxhdGl2ZSUzRCUyMjElMjIlMjBhcyUzRCUyMmdlb21ldHJ5JTIyJTIwJTJGJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDJTJGbXhDZWxsJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDbXhDZWxsJTIwaWQlM0QlMjJBejc5MW51LVhwaXBVbk5QdXh6ai0yNCUyMiUyMHZhbHVlJTNEJTIyUHJlZGljdGlvbiUyNmx0JTNCYnIlMjZndCUzQiUyNmFtcCUzQmFtcCUzQiUyNmx0JTNCYnIlMjZndCUzQlBvc3QlMjBQcm9jZXNzaW5nJTIyJTIwc3R5bGUlM0QlMjJyb3VuZGVkJTNEMCUzQndoaXRlU3BhY2UlM0R3cmFwJTNCaHRtbCUzRDElM0IlMjIlMjB2ZXJ0ZXglM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjB4JTNEJTIyNDMwJTIyJTIweSUzRCUyMjEwJTIyJTIwd2lkdGglM0QlMjIxMzAlMjIlMjBoZWlnaHQlM0QlMjI4MCUyMiUyMGFzJTNEJTIyZ2VvbWV0cnklMjIlMjAlMkYlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0MlMkZteENlbGwlM0UlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlM0NteENlbGwlMjBpZCUzRCUyMkF6NzkxbnUtWHBpcFVuTlB1eHpqLTMyJTIyJTIwdmFsdWUlM0QlMjIlMjZsdCUzQmIlMjZndCUzQiUyNmx0JTNCZm9udCUyMHN0eWxlJTNEJTI2cXVvdCUzQmZvbnQtc2l6ZSUzQSUyMDE4cHglM0IlMjZxdW90JTNCJTI2Z3QlM0JTeXN0ZW0lMjBBcmNoaXRlY3R1cmUlMjBmb3IlMjBORVIlMjBQcm9qZWN0JTI2bHQlM0IlMkZmb250JTI2Z3QlM0IlMjZsdCUzQiUyRmIlMjZndCUzQiUyMiUyMHN0eWxlJTNEJTIydGV4dCUzQmh0bWwlM0QxJTNCc3Ryb2tlQ29sb3IlM0Rub25lJTNCZmlsbENvbG9yJTNEbm9uZSUzQmFsaWduJTNEY2VudGVyJTNCdmVydGljYWxBbGlnbiUzRG1pZGRsZSUzQndoaXRlU3BhY2UlM0R3cmFwJTNCcm91bmRlZCUzRDAlM0IlMjIlMjB2ZXJ0ZXglM0QlMjIxJTIyJTIwcGFyZW50JTNEJTIyV0l5V2xMazZHSlFzcWFVQktUTlYtMSUyMiUzRSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUzQ214R2VvbWV0cnklMjB4JTNEJTIyOTQlMjIlMjB5JTNEJTIyLTE3OSUyMiUyMHdpZHRoJTNEJTIyMzcwJTIyJTIwaGVpZ2h0JTNEJTIyMzAlMjIlMjBhcyUzRCUyMmdlb21ldHJ5JTIyJTIwJTJGJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTNDJTJGbXhDZWxsJTNFJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTNDJTJGcm9vdCUzRSUwQSUyMCUyMCUyMCUyMCUzQyUyRm14R3JhcGhNb2RlbCUzRSUwQSUyMCUyMCUzQyUyRmRpYWdyYW0lM0UlMEElM0MlMkZteGZpbGUlM0UlMEGT+xGhAAAgAElEQVR4Xuy9D9RV13mn91NqWwM2UhJw7AwJUcgfSDKJuhhV8QSY5VGmltrG8arUCncETWzFQyQQNjIkNhoQQgNOApZsJORSD3EcUAfcmq6J00p4WsctKFVdRS0zk0ZqV6lKSldkQywLGSLbsbreo7tv9nc49/85++yz93PWYn0f99tn73c/73vP+d33vmfvq8QBAQhAAAIQgAAEIACBxAlclfj8mB4EIAABCEAAAhCAAASE6CUIIAABCEAAAhCAAASSJ4DoTd7FTBACEIAABCAAAQhAANFLDEAAAhCAAAQgAAEIJE8A0Zu8i5kgBCAAAQhAAAIQgACilxiAAAQgAAEIQAACEEieAKI3eRczQQhAAAIQgAAEIAABRC8xAAEIjCKwUNLjkm4uNVwl6alRJ9fw9zskrZa0WdLlGvqrqwufy2lJayQ9X1fnpX5WSjolaZxxBrV19j7YsN9CjTMNasfGnXuw4bjyx9shydj7x3ZJuyT5dpRtrJqnHwfjtB/nvTqqnyr7p/FB+Rx7fx+RdEKS/X6hjk4r+og5LhuaMt2WCSB6iQkIQGAYgVE3wrU9QdwUxSpR0NRYk/ZbZtMki1lF7zJJxyRdL2kcATQpC9c+1DjT2Ff14a0pIefs82Ok6gNLKNFr9oyKz1HvdeujiQ8JIURvzHE5TSxzzpQEEL1TguM0CGRAYJ6khyWtHzLXcTKPs6CKWfQ629z8mhAEZfE0Le9QN/1Q40wTU77oHSUAp+m/6pxRmeWQondUJnUc0WtzbPJDU13cy/3EHJdNzZl+KwggegkLCEBgEAFfJJQzYv4N0gTEM0MyiVU3nKqsm39THlRS4dsxSlCUx72p91Wyn7Fa4tk9TjasKqNpNtlX1IMEgS9snu1ly8pjjRLQfqb3VyX9uvdhpCxmylnhG3pfH/t+9sVz1YebQcJmmJ0uY1c1zu0VX+NbO3eOb88oXpPYW/7QUI51f57leBqVmR3kS3+MKiHpjzlM9I77AWfUtwBVjIcJ9Kpx/feS+8Awyk82xjhMB2V6J/HztHHZVDkSd5VICSB6I3UMZkEgAgKT1qy6G09ZIJdvaja1qhphe90JuEFtXN9VAss/3+oC/Rt1Fc4vSnpb7yt//+/jZLL8Of2apH/SE6HD6jarxODZIdl0X8yOysJV1YQ68TJM9J4f4gs/Gzos6+/svGWIuJ5G9FbxGtfesr8H8XO+Losm/3yfQ1W7YeK0alzfrymI3kEfpsZlWiV6B33oLX9YnDUuEb0R3GhCmoDoDUmbsSDQPQKDxGXV18PuBu/f1P2bkhOEVZmpqkyS0aoSBVVtq7669ts5my6VRKazadD4VR6rmtOwukT/5l/m5vN1AqycRbcPCP5rvsCtyuKN4luVafR9VjWXce0c9DXyoDKVYZnessDx42GUvVV+G1TeUMXbH6sqC11lW9WYvi/ul/RA70NWVbbUPag56gOOjTPsG49BV5lR5TfjjGt9V31QKMf1JEyr4s3Fyyg/zxqX3bsiY/FMBBC9M+HjZAhkQWBYxsa/+fqiwt0Yq0SQf0McVWdYJZYGCUz3uru5+6UL/k25SmhVCdlBzq2a0zDRXHUDt74HjVn1lLnPzBetw/j6Ym1UiYnPp+xH+xrf1XZX+dtfDaJO0VuOjUGitSruJhG9gwR51XiDfDkoVsofQFzG2/mmKgM+jvicVPSOUyox6bjDWEzCdNg3QU3HZRYXcCb5NwQQvUQDBCAwLoFxsr7lEoeyELUlxwZ9JVmVPa66eQ4T4TYXJ5YWDagzrhLNk4jeUeOXM2qDBMAkYw6q25xF9I4q/3DZzOMDRG9V3NQpesscx7XXMuPjit5hPqj626QPVpb95pdn2PxelrR1iiXLxhW9oz5U+pxGid7y+3OauB7nWxL/fTvo2mS21BGX4177aJcIAURvIo5kGhAITMDPglXVk9rN1q91rRK046z/G5voHVZr6Ltg1MNKwzK9Va5E9I6u0XYiPWbRazWkVR8eh9Vkj6o7rYqNSb5NcfE26oG4clwiegNfdBludgKI3tkZ0gMEUiVQVS/n5upnbAbV8O6V9K7eCaM2bvD78wXAJOUNZT8MyjrOkukdlQ1zNviZuHHEgT9nx+Kct5lBE6J33CW8Bvmmys4mM73j2jvo/Tjo/Em+ip8102sCtuqbjrpFrzEYVAc+iE9dotf6n4TpuOUNZbvriMtUr93MawABRC+hAQEIDCIwztfJdu6g1Rpcv+WvqQctoVRVIzjug2xVX5s2IXpH1XSOOwfHpuqDhW/3sIf/rI9Zyht8cVK1UoRfBzqunaOY25hV9d6Dliwr78JXxXdcsVbng2yjHgoblT0tf3iadEc26999ezJs/r7fRq1NPC5HN7dhHwCaeJCtyr5Z45Krf2YEEL2ZOZzpQmBCAoPqeF03o9b09G/O7pxRJQJVD52VM6iD6mpHPbxl/Uyb6a0So2WcVTf7YeJg2JJLVVvNlnmPK3rLzF0/Zr/bqa08F//DzLh2DhrH6jRtG+VBx7iid9gHsVG7qw3LFA+r0x6nVGXQvIYJSX/MpkSv77dRD7PVKXr9D1RVbHymVe/Hcf08a1yOKh2Z8HJJ89gJIHpj9xD2QaB9AuNsFOFbOe6NtkpolNfILY89TByUH9oZlXUctbRamfywco8qQe9s3TJgYwa//zKLURtOuJv1uKLXF/tuXMe6SjgMygqOsnPYOOXsponUM721fccVvdb/JPb6jEeVR5TtG7U5RTkLXfVOHSYkfd81JXrNpkHjDPrANkocu/PGKfUYh+mg1Vgm8fMscdn+FRYLghFA9AZDzUAQyIbAoFq7bAAwUQhAYGwCw9a4HrsTGkJgHAKI3nEo0QYCEJiEwKB6vkn6oC0EIJAHgVF18nlQYJZBCCB6g2BmEAhkQaD8VeYka4RmAYhJQgACfQLlut1RNdmgg8DMBBC9MyOkAwhAoEfAF73j1gUCDwIQyJOAXyPPB+Q8YyD4rBG9wZEzIAQgAAEIQAACEIBAaAKI3tDEGQ8CEIAABCAAAQhAIDgBRG9w5AwIAQhAAAIQgAAEIBCaAKI3NHHGgwAEIAABCEAAAhAITgDRGxw5A0IAAhCAAAQgAAEIhCaA6A1NnPFyI7Bx0aJFv3rx4sWfevXVV+dfffXVlxYsWPBn58+f/z1Jj+YGg/lCAAIQgAAE2iKA6G2LPOOmTuCXFixY8LvvfOc7r/7ABz5wzS/8wi9o4cKFunDhgv74j/9Yn/70p1/+8pe//OrFixffL+kPU4fB/CAAAQhAAAJtE0D0tu0Bxk+RwNr58+f/s6NHj1797ne/e+D8vvCFL+i9733vq5cuXfo1SUdSBMGcIAABCEAAArEQQPTG4gnsSIXAz0r6V0899ZQsuzvqsKzvypW2p4N+TtK/HtWev0MAAhCAAAQgMB0BRO903DgLApUEFi5c+MVt27b9u/fee+/YhB566CHt2bPnX164cOFdY59EQwhAAAIQgAAEJiKA6J0IF40hMJTAkvnz5/8f3/zmN6+elNOb3/xmK3P4SUlnJz2X9hCAAAQgAAEIjCaA6B3NiBYQGJfA2ne/+90H/uAP/uCacU9w7X75l3/55S984QsbqO2dlBztIQABCEAAAuMRQPSOx4lWEBiHwD/Ztm3bA7t37/6ecRr7be67777v7tmz535J/3TSc2kPAQhAAAIQgMBoAoje0YxoAYFxCSB6xyVFOwhAAAIQgEBgAojewMAZLmkClDck7V4mBwEIQAACXSaA6O2y97A9NgI8yBabR7AHAhCAAAQg0COA6CUUIFAjAZYsqxEmXUEAAhCAAARqJIDorREmXUFAEptTEAYQgAAEIACBCAkgeiN0CiZ1ngDbEHfehUwAAhCAAARSI4DoTc2jzCcWAr+0YMGC333nO9959Qc+8IFrbEvihQsX6sKFC7Kthz/96U+//OUvf/nVixcvvl/SH8ZiNHZAAAIQgAAEUiWA6E3Vs8wrFgIbFy1a9KsXL178qVdffXX+1VdffWnBggV/dv78+d+T9GgsRmIHBCAAAQhAIHUCiN7UPcz8YiLwmiTeczF5BFsgAAEIQCAbAtyAs3E1E42AAKI3AidgAgQgAAEI5EkA0Zun35l1OwQQve1wZ1QIQAACEIAAX7USAxAISADRGxA2Q0EAAhCAAAR8AmR6iQcITEZgp6T7JzulltYPSLKxOSAAAQhAAAIQmIIAoncKaJwCgSkJkOmdEhynQQACEIAABGYlgOidlSDnQ2B8Aoje8VnREgIQgAAEIFArAURvrTjpDAJDCSB6CRAIQAACEIBASwQQvS2BZ9gsCSB6s3Q7k4YABCAAgRgIIHpj8AI25EIA0ZuLp5knBCAAAQhERwDRG51LMChhAojehJ3L1CAAAQhAIG4CiN64/YN1aRFA9KblT2YDAQhAAAIdIoDo7ZCzMLXzBBC9nXchE4AABCAAga4SQPR21XPY3UUCiN4ueg2bIQABCEAgCQKI3iTcyCQ6QgDR2xFHYSYEIAABCKRHANGbnk+ZUbwEEL3x+gbLIAABCEAgcQKI3sQdzPSiIoDojcodGAMBCEAAAjkRQPTm5G3m2jYBRG/bHmB8CEAAAhDIlgCiN1vXM/EWCCB6W4DOkBCAAAQgAAEjgOglDiAQjgCiNxxrRoIABCAAAQjMIYDoJSAgEI4Aojcca0aCAAQgAAEIIHqJAQi0RADR2xJ4hoUABCAAAQiQ6SUGIBCOAKI3HGtGggAEIAABCJDpJQYg0BIBRG9L4BkWAhCAAAQgQKaXGIBAGAJ/V9LnJd0m6U/CDMkoEIAABCAAAQg4AoheYgECYQg8I+nPJP2UpBvCDMkoEIAABCAAAQggeokBCIQjcLA31HpJ/u/hLGAkCEAAAhCAQOYEyPRmHgBMv3EC/1iS/fOzu5b1/c97/xo3gAEgAAEIQAACEGBzCmIAAk0SsDpeE7gmeP063kGvN2kLfUMAAhCAAASyJkCmN2v3M/mGCQzL6FZlgBs2h+4hAAEIQAAC+RJA9Obre2beLIFxanfHadOslfQOAQhAAAIQyIQAojcTRzPNoAQmyeJS3xvUNQwGAQhAAAK5EkD05up55t0UgUnrdSdt35Td9AsBCEAAAhBImgCiN2n3MrkWCEyTuZ0kM9zClBgSAhCAAAQg0H0CiN7u+5AZxENglhrdWc6NhwCWQAACEIAABCIlgOiN1DGY1TkCdWRrp8kSdw4UBkMAAhCAAATaIIDobYM6Y6ZGoK663Lr6SY0v84EABCAAAQjMTADROzNCOoBAsQFFXTus1ZExxiUQgAAEIAABCJQIIHoJCQjMRqCJWtwm+pxtlpwNAQhAAAIQ6DgBRG/HHYj5rRJoMitbZ/a4VUgMDgEIQAACEIiBAKI3Bi9gQxcJNF1/23T/XWSOzRCAAAQgAIGpCSB6p0bHiZkTCJGJbTKTnLn7mD4EIAABCORGANGbm8eZbx0EQtbchhyrDjb0AQEIQAACEIiSAKI3SrdgVMQE2si+hsgqR4wc0yAAAQhAAAKzE0D0zs6QHvIisFPSFyT9ScBpW33vuyXZ2BwQgAAEIAABCExBANE7BTROgQAEIAABCEAAAhDoFgFEb7f8hbUQgAAEIAABCEAAAlMQQPROAY1TIAABCEAAAhCAAAS6RQDR2y1/YS0EIAABCEAAAhCAwBQEEL1TQOMUCEAAAhCAAAQgAIFuEUD0dstfWAsBCEAAAhCAAAQgMAUBRO8U0DgFAhCAAAQgAAEIQKBbBBC93fIX1kIAAhCAAAQgAAEITEEA0TsFNE6BQI/AMkn7JW2S9LxHZbukM5IehxQEIAABCEAAAnEQQPTG4Qes6CYBRG83/YbVEIAABCCQIQFEb4ZOZ8q1ERhH9FqbY5Kul3Ra0hovK2wZ4V09a9b2MsPzJD0s6Xt7bVdJeqo2i+kIAhCAAAQgkCkBRG+mjmfatRAYJXqP9wTs4Z5wXSnpJkkPSrpD0mpJmyUt6QnjDZKe7Z1zrteuFkPpBAIQgAAEIJA7AURv7hHA/GchMKnodWO5bO5Jr+7X1QE7oez/bRYbORcCEIAABCAAAUmIXsIAAtMTGCV67UG2hT1he7OkE70M76VeNnd9aegdkvaVssPTW8eZEIAABCAAAQj0CSB6CQYITE/ABO2jknZ6dbpVWVw3gpU3rJO0TdIeSa7swbfAnV/1t+kt5UwIQAACEIBA5gQQvZkHANOfiYATqH79rQnbA72H0M6XRPGgmt75vWywCd1yHfBMBnIyBCAAAQhAAAKvE0D0EgkQmI2AE76uVKG8QoMJ3VO9IVx5w4Xe//3VG6y0wR5wI9M7mz84GwIQgAAEIFBJANFLYEAAAhCAAAQgAAEIJE8A0Zu8i5kgBCAAAQhAAAIQgACilxiAAAQgAAEIQAACEEieAKI3eRczQQhAAAIQgAAEIAABRC8xAAEIQAACEIAABCCQPAFEb/IuZoIQgAAEIAABCEAAAoheYgACEIAABCAAAQhAIHkCiN7kXcwEIQABCEAAAhCAAAQQvcQABCAAAQhAAAIQgEDyBBC9ybuYCUIAAhCAAAQgAAEIIHqJAQhAAAIQgAAEIACB5AkgepN3MROMiMBOSfaPAwIQgAAEIACBwAQQvYGBM1zWBF6TxHsu6xBg8hCAAAQg0BYBbsBtkWfcHAkgenP0OnOGAAQgAIEoCCB6o3ADRmRCANGbiaOZJgQgAAEIxEcA0RufT7AoXQKI3nR9y8wgAAEIQCByAojeyB2EeUkRQPQm5U4mAwEIQAACXSKA6O2St7C16wQQvV33IPZDAAIQgEBnCSB6O+s6DO8gAURvB52GyRCAAAQgkAYBRG8afmQW3SCA6O2Gn7ASAhCAAAQSJIDoTdCpTClaAojeaF2DYRCAAAQgkDoBRG/qHmZ+MRFA9MbkDWyBAAQgAIGsCCB6s3I3k22ZAKK3ZQcwPAQgAAEI5EsA0Zuv75l5eAKI3vDMGRECEIAABCBQEED0EggQCEcA0RuONSNBAAIQgAAE5hBA9BIQEAhHANEbjjUjQQACEIAABBC9xAAEWiKA6G0JPMNCAAIQgAAEyPQSAxAIRwDRG441I0EAAhCAAATI9BIDEGiJAKK3JfAMCwEIQAACECDTSwxAIBwBRG841owEAQhAAAIQINNLDECgJQKI3pbAMywEIAABCECATC8xAIFwBBC94VgzEgQgAAEIQIBMLzEAgZYIDBO9KyWdGmDXKklPNWSzjbtO0mZJl2seY7ukM5Ier7lfuoMABCAAAQhMTIBM78TIOAECUxMYN9NrYtGOB6ceKY4TEb1x+AErIAABCECAHdmIAQgEJTCt6C2Lxzt6Vj8jaaekr0h6SNJpSWskPd/7u589Pjggm+syvdsk7ZF0TtKu3vk7esJ72ZBxyplis21pL8N7pNfPWrK9QeOMwSAAAQhAoIIAmV7CAgLhCEwren1hadY+IOlQz+xjkj7fE6cmOFf3xO0SSfslbZJ0VtLDPUFbzh6XRa91a6UOKyQd6Iloe23QONbOL49wotfGIdMbLrYYCQIQgAAERhBA9BIiEAhHYFrRu1DSbkn3SVok6U5J90vyha1ld63do72s7A2eALZa3UG1u2XRe7KXlfX7MkJOQJfHMXsQveFiiJEgAAEIQGBKAojeKcFxGgSmIDCt6LWhLGv6JUnX9ca1h8Nc2cFGSRcqRK8rL3CmnpBkmVhr646y6D3ce2iuLHqtjKJqHETvFIHAKRCAAAQgEJ4Aojc8c0bMl8AsotfE6Xt66Ky0wTKuJnr9DKz/f8v0Wm3tqIfhxhW9g8Ypi17/ITzKG/KNdWYOAQhAIDoCiN7oXIJBCROYRfRa5tWyuy94D6SZyLVa2729vw2q6TWBbAJ0ccXDbOOK3kHj+LW/53t22PJq1PQmHMhMDQIQgEAXCSB6u+g1bO4qgVlEr825nDl15Q3fkLReUrl8wV+9oaq0wfocV/RaecOgccwuW/HBxrB/b/EerLMSC1Zv6GrEYjcEIACBhAggehNyJlOJnsC4ordqIn6NrVuSrFzT2xSAUOM0ZT/9QgACEIAABIToJQggEI7AtKK3XMbgLA4lRkONE84TjAQBCEAAAtkRQPRm53Im3CKBaUVviyYzNAQgAAEIQCANAojeNPzILLpBANHbDT9hJQQgAAEIJEgA0ZugU5lStAQQvdG6BsMgAAEIQCB1Aoje1D3M/GIigOiNyRvYAgEIQAACWRFA9GblbibbMgFEb8sOYHgIQAACEMiXAKI3X98z8/AEEL3hmTMiBCAAAQhAoCCA6CUQIBCOAKI3HGtGggAEIAABCMwhgOglICAQjgCiNxxrRoIABCAAAQggeokBCLREANHbEniGhQAEIAABCJDpJQYgEI4Aojcca0aCAAQgAAEIkOklBiDQEgFEb0vgGRYCEIAABCBAppcYgEA4AojecKwZCQIQgAAEIECmlxiAQEsEEL0tgWdYCEAAAhCAAJleYgAC4QggesOxZiQIQAACEIAAmV5iAAItEUD0tgSeYSEAAQhAAAJkeokBCIQjgOgNx5qRIAABCEAAAmR6iQEItEQA0dsSeIaFAAQgAAEIkOklBiAQjgCiNxxrRoIABCAAAQiQ6SUGINASAURvS+AZFgIQgAAEIECmlxiAQDgCiN5wrBkJAhCAAAQgQKaXGIBAIAIflrRL0jZJn5TkRO8HJX1M0nZJHw9kC8NAAAIQgAAEsiZApjdr9zP5hgkskHRB0iuSviXpbZJelPQmSW+RtFDSxYZtoHsIQAACEIAABCQhegkDCDRLwDK6WyS9oSdwTQh/R9I+SR9tdmh6hwAEIAABCEDAEUD0EgsQaJaAidyvSbraG+ZVSW8ly9sseHqHAAQgAAEI+AQQvcQDBJonYNleq+99o6Rv9+p4yfI2z50RIAABCEAAAn0CiF6CAQLNE/CzvWR5m+fNCBCAAAQgAIErCCB6CQoIhCHganup5Q3Dm1EgAAEIQAACcwggegkICIQhYNnez0h6H7W8YYAzCgQgAAEIQMAngOglHtomYGvXcqRNgOtM2v5ldhCAAAQ6QYCbUSfclLSRr732Gro3VQ9fdVVxieE6k6qDmRcEIACBDhHgZtQhZyVqKqI3UccWahfRm7B3mRoEIACBbhFA9HbLXylai+hN0au9OSF6E3YuU4MABCDQMQKI3o45LEFzEb0JOtVNCdGbsHOZGgQgAIGOEUD0dsxhCZqL6E3QqYjehJ3K1CAAAQh0lACit6OOS8hsRG9CzixPhUxvws5lahCAAAQ6RgDR2zGHJWguojdBp5LpTdipTA0CEIBARwkgejvquITMRvQm5EwyvQk7k6lBAAIQ6DgBRG/HHZiA+YjeBJw4aAqUNyTsXKYGAQhAoGMEEL0dc1iC5iJ6E3Qq5Q0JO5WpQaB7BNgBqXs+a8RiRG8jWOl0AgKI3glgda0pmd6ueQx7IZAkAe4zSbp1sknZ/QjROxkzWtdPgItR/Uyj6RHRG40rMAQCORPgPpOz93tzR/QSBDEQ4GIUgxcasgHR2xBYuoUABCYhwH1mElqJtkX0JurYjk2Li1HHHDaJuYjeSWjRFgIQaIgA95mGwHapW0Rvl7yVrq1cjNL1rRC9CTuXqUGgOwS4z3THV41ZiuhtDC0dT0CAi9EEsLrWFNHbNY9hLwSSJMB9Jkm3TjYpRO9kvGjdDAEuRs1wjaJXRG8UbsAICOROgPtM7hEgFd88snoDgdA2gdYuRs8//7x27typRx99VAsXLqyVw4ULF7Rx48ai/2XLls3p28Zds2aNTp8+3X/9+uuv17Fjx65oWzZqWL+1TqCmzhC9NYGkGwhAYBYCrd1nZjGac+slgOitlye9TUegtYtRm6J306ZN2r9/f1/kPvXUU9qwYcNI4YvonS7IOAsCEMiaQGv3maypRzZ5RG9kDsnUnNYuRr7oPX/+fJGVvfbaa3Xw4MHCFadOndLKlStlgvTw4cO65pprtHfvXt188816/PHHNX/+fG3evFnr1q0r2jlB+pGPfESf+tSnin6qMrg2bln02ngPPvhgMe727duLvu644w6dOHGieG3Xrl3asmVLMZ7f76JFi65oZ+fHcpDpjcUT2AGBrAm0dp/Jmnpkk0f0RuaQTM1p7WJUFr1WcrB169ZCRJoAPXfunB5++GE9++yzWrVqlY4cOdL/m/nKidCy6DXxbGJ0WHlDleh14nrPnj3atm2bVq9eXYzni2S/3yVLlhQiuKpduaSirdhC9LZFnnEhAAGPQGv3GbwQDwFEbzy+yNmS1i5GZdHrC1EnQJ3oNRFs2V2r/XXn7du3rxDHdYteG3PevHn9mPBLGoaJ6RhLHxC9Ob+1mTsEoiHQ2n0mGgIYwoNsxEAUBFq7GFWVN7iH2sqi18obnBhtSvSaqD558uSc7LLzkCuTKItes9Oy0OV2ZHqjiG2MgAAE4iDQ2n0mjuljhREg00scxECgtYvRJKLXz/Sa0LT/Hzp0aE6md1AZQtXqDcNqeu++++6irMFqc/1a4XLZhKvnrWqH6I0htLHBCLT2BvfwX9X7ygGPZEsghjDMFn4sE0f0xuKJvO1o7WI0iei1bKp7sM09cOZqehcvXlwIVMvU2oNutvTYpDW9/uoNZTE7qN9h7RC9eb+pYpp9a29wRG9MYdC2LTGEYdsMsh8f0Zt9CEQBoLWL0SSi18oe7Dh69KjWr18/p9TBrbn70EMP6Stf+UqxCoR7yOzpp5++YhmycdbpNaG7du3aYkxbrcEeprPa4RUrVhQPr7l+n3nmmcp2liGO4RhR07tA0j+W9PEYbMWGZgi4N7h9WNyxY0cxyLjrUs+yrKBf4758+XLWpG/GvV3ptbX7TFcA5WAnojcHL8c/x+gvRn59r/+AWfxo27dwgOg1sfsbkn5T0nckzW/fUixoioC9wf3VUOw9NO661K6UyD1EOomNfrkRoks2qX4AACAASURBVHcSckm2jf4+kyT1yCaF6I3MIZmaE/3FCNE7fWSWRK8Tu1us1LP37z5Jn5h+BM6MncBzzz332rAadlcm5Jbec+83Kxm68847i7WqbZ3qpUuXFrsYvvzyy8W3H/aaKytyD4BeunSpWCrQXyvbvpk5ePCgfbC6HDsr7GuMQPT3mcZmTsd9AohegiEGAlyMYvBCQzb0RO81vcyuE7tuPbYLkhY1NDTdRkLg1KlTRaa3nK0dtC71oOUCn3zyyTk18+5hzxdeeKG/6okTvVZiZIcT22R6IwmG9szgPtMe+2hGRvRG44qsDeFilLD7e6L3qKRbJb3Jm+p3JX1PwlNnaj0C9gBoXaL3zJkzRXbXDuvTsr92lDO9FaIXf2RO4LXX7MsljpwJIHpz9n48c0f0xuOL2i0h01s70s51OG15Q3ljGMv0ziB6eZCtc5FTq8HR3mf8BzzdjN1W97YZ0rSHXw//3HPPyV9rvqpP9yHSvkFJtaQP0TttNHFenQSivRjVOclc+xqjpvefSHo4Vz45zHucB9ncMoCWxS1vAe6yxCZ67cZtZRJ2uPIGl/W11+3mvmHDhmLFFDsob8ghwsaaY7T3GT/23Uwslv0PeGPNsNRo0odAfdE7zXhdOAfR2wUvpW9joxcj/wZpKMs7mB05cqS4eY467IJg5/p1if5ySv5NuLxcmP8U+aj1c8v2ubWBrX9bgslstQd7/GXT7G/+MmjuAZ/y6372wNrb5hoPPPDAnC2PR3GY9O8jVm+wFRz+mtUbJqXarfbuDe4vw1dessyPe4vfc+fOFcsCnj17VrYk4G233VaUMvz+7/++XnzxxeKBNhfn5ffFN77xjWLZQLeO9XXXXceDbN0KmSasbfQ+M4vBVaK3XO/+0ksvFR/k7H7glq20hznt8O8R5fuAu2eVM71+dtnOt7p4t0Sm3RPtPeMywzaGLZNZHs/d/6699tor/jYLjybPRfQ2SZe+xyXQ2MWoLOzKyyRdvny5eDO7p8aHGewuEr5Irlv02s37vvvu0+7du2Vfa/mf1OfPnz/HVv9C6W76liVzF0Sb06233qr777+/eALexLa/zbEtG1X+QDCuwyZpxzq9k9BKs21db/By/E5Cix3ZJqGVZNu6wrB2OFWid9AGSDa4375qUyNbz92SI36ixhe9x48f72eR/XvYY489VnywLJc37Nu3r/8h1NaL979JsQ+kW7du7Y/nPqzGurQnorf28KXDKQg0djGyN/1NN91UbOU7SOCWF7/3M05+NtX6unjxYpFh2r9/fyEi6xa9ZXa+mF2+fHmxFJNlsGxsXxCfP3++eN020DCxPEgclOdaFtlT+G7kKSNE78jzadB9AnW9wRG93Y+FFmdQVxjWPoWqml5373FZVpeY8e8J/n3NhK59s+HfB/zrvRO9e/bs0bZt24qNjsrfSFbV9Jbb+/fRG264oV8+5O5Jo+qGa4c3YYeI3gmB0bwRAo1cjMqCbpwSA/fV0IEDB/oZU7fFsP+kuKu1alr0+jYb+fIFzdUrmuj1LzaDarmqRIP/waAJ7yJ6m6DarT4beYNPiIBM74TA0mseQxhWUq3K9LqGTmQ6keqX8viduZIEf5WUKtHr1r62n+OI3qr27l5oote/J3Xh4TdEb3pv7C7OqJGLUbm0YZztTMtvWl88uq9+brnllv4DNP4nawPvHqyZpaa3fLHzF+wvX9Cc6LWtiN2STW63K7+tf6H0679srKZLHBC9XXxLYjMEkiPQyH2mDkqTil7/Gz9//PI9LkSmF9FbRwTQR24EGrkYlQXsOJneciZ0VL3Tr//6r+u3f/u3i7KCaUSvX0rhP2TmPt27LLP1XXVBmzTTW/5qDNGb21uN+UIgWwKN3GfqoDmJ6LXx/PZV3066RMmgml6r0bXDsrj+ffFzn/vcxDW9iN46IoA+ciPQyMWonOkdVNPrl0GUn3CtyvRaNtf1dc011+jP//zPpxa9VY52wtQ9jODa2Ouz1vRWMSDTm9vbjflCIEsCjdxn6iA5qeh113G3mkL54Wp7uMyePXnooYcKUWsPRvv3NrO5ajUGt7rKpKs3uGdJKG+oIxroIwcCjVyMqh7SGrR6g8uojlPT65Y3c23f/va3X7Fu6LTlDcNWkyj/bZzVG6wUwxfKVdluanpzeIsxRwhkT6CR+0z2VDsGgJrejjksUXMbuxhVCbryOrj+mrbGd9jqDW45F+cH+2RcXizf1tD1D6uhtdpf9+nb/a28Tqm97q+xWO7DhPSs6/Ran+V1f/0l0pqIL2p6m6BKnxCAwIQEGrvPTGgHzVskgOhtET5D9wk0djEKtQFDV33ZdGmDcUH0djU6sBsCSRFo7D6TFKXEJ4PoTdzBHZleoxejEMKuI5znmBnqAwGit4vRgc0QSI5Ao/eZ5GglOiFEb6KO7di0uBh1zGGTmIvonYQWbSEAgYYIcJ9pCGyXukX0dslb6drKxShd31LekLBvmRoEOkQg2/uM2/HNXxazQ36r1VREb6046WxKAtlejKbk1anTyPR2yl0YC4FUCWR5n/GXETt+/LjcbqKpOnnUvBC9owjx9xAEsrwYhQAbwxiI3hi8gA0QyJ5AlvcZRO/cuEf0Zn8diAJAlhejKMgHMALRGwAyQ0AAAqMIZHmfceu7v/TSS7J/9mD3woULR7FK9u+I3mRd26mJZXkx6pSHZjAW0TsDPE6FAATqIpDlfcZf+91fo70uqF3rB9HbNY+laW+WF6M0XXnlrBC9uXiaeUIgagLZ3WdM8O7cuVO2TbBtQ7xhwwYdOHCg+L+9vmzZsqgd1oRxiN4mqNLnpASyuxhNCqjL7RG9XfYetkMgGQLZ3WeslMF/cM3+v3btWpV3IU3Gw2NMBNE7BiSaNE4gu4tR40QjGgDRG5EzMAUC+RLI7j7jZ3qtjtcealu1apXWr1+vhx9+WPPmzcsuGhC92bk8yglndzGK0gsNGYXobQgs3UIAApMQyPI+44SugXLr9D722GNaunSp7rjjjkn4JdEW0ZuEGzs/iSwvRp332pgTQPSOCYpmEIBAkwS4zzRJtyN9I3o74qjEzeRilLCDEb0JO5epQaA7BLjPdMdXjVmK6G0MLR1PQICL0QSwutYU0ds1j2EvBJIkwH0mSbdONilE72S8aN0MAS5GzXCNoldEbxRuwAgI5E6A+0zuESAJ0UsQxECAi1EMXmjIBkRvQ2DpFgIQmIQA95lJaCXaFtGbqGM7Ni0uRh1z2CTmInonoUVbCECgIQLcZxoC26VuEb1d8la6tr6W7tSYWY/AVZCAAAQg0CKBRkSvv82vP7cjR45MvSTYgw8+WLmk2IULF4o+T5w4MQdjjJtNXL58WZs3b9a6deu0cuXKFt0+d2hEbzSuwJCMCJjIRwRm5HCmCgEItE6gMdG7adMm7d+/v7+tr4nTjRs3Tr3V7yjRu3379r6QjFVctu7tAQYgemP1DHalTADRm7J3mRsEIBAjgWCityxE/Q0i/N3QqjaOePLJJ4utgu0oZ4tdptcXvdbOieQbbrhBJsDtMHFn2w6fP39ea9as0enTp/ubU9jubHb4WepBdo2y19/pzfp0G2DMnz+/n+ldtGhR8QHg2muv1cGDB4uxT5061Rfubnvk66+/Xu9617u0YMEC2RybOBC9TVClTwgMJ4DoJUIgAAEIhCUQTPSamHTZX5uiic4DBw5oxYoVhRBcvHix7r777jnZYBN+dlj5wiSZXl8Im7h0Y1lJgfublRi4fs+dO1dsQXzp0qU545eFs2WulyxZMtLeW265pXIet9566xzRa3Zt3br1CjvOnj3bZ2X2m51mO6I37JuD0SDQJAFEb5N06RsCEIDAlQQaE70uk+oP6TKZls09fPhwITTnzZsn+78JzEceeUQ7duyoLIEYJXrLNb0uI+yL7WXLlhWZXMuwPvroo7KMrP93ywD7djnbTXyfPHlyjr3WzkToli1brrB3UCmHn+02MeuXgPhMjh8/rjNnzvRFro3v/7/uQCbTWzdR+oPAaAKI3tGMaAEBCECgTgKNid5yTa9vdFlE+kLULz3wywhGid5yeYMbryxyncA2G0z0+gL1mWeemSNufdHryivca65kYZC9VWUSdq57kM2VNzjx7Yveffv2FcO4zC6it86Qpy8IxEEA0RuHH7ACAhDIh0ArondQptcJUV9sugxnXaJ32kzvOJnWQeLUvW5Z4XFEL5nefN6AzDRfAojefH3PzCEAgXYItCJ6XRa0XNN7++23zyk9mLam10dZFrnDanr9WlorhTChbYfZ5Weu7XWrA77rrrv0W7/1W/1SCWevPTznl1C418s1vX4b/4MANb3tvBkYFQIhCSB6Q9JmLAhAAAJSK6LXwA9avcGtWmBt/PIG9/q4qzc455ZFr73ulx64MgW3esMkq0rYOaPs9edhv4+T6bU6Z9ev2Wf/XnnlFR5k4x0LgYQIIHoTciZTgQAEOkGgEdHbiZl3xEj38Nvq1aun3thj1FR5kG0UIf4OgfoJIHrrZ0qPEIAABIYRQPRGGB9+ttnMa3p3OURvhEGASckTQPQm72ImCAEIREYA0RuZQ9owB9HbBnXGzJ0Aojf3CGD+EIBAaAKI3tDEIxwP0RuhUzApeQKI3uRdzAQhAIHICCB6I3NIG+Ygetugzpi5E0D05h4BzB8CEAhNANEbmniE4yF6I3QKJiVPANGbvIuZIAQgEBkBRG9kDmnDHERvG9QZM3cCiN7cI4D5QwACoQkgekMTj3A8RG+ETsGk5AkgepN3MROEAAQiI4DojcwhbZiD6G2DOmPmTgDRm3sEMH8IQCA0AURvaOIRjofojdApmJQ8AURv8i5mghCAQGQEEL2ROaQNcxC9bVBnzNwJIHpzjwDmDwEIhCaA6A1NPMLxEL0ROgWTkieA6E3exUwQAhCIjACiNzKHtGEOorcN6oyZOwFEb+4RwPwhAIHQBBC9oYlHOB6iN0KnYFLyBBC9ybuYCUIAApERQPRG5pA2zEH0tkGdMXMngOjNPQKYPwQgEJoAojc08QjHQ/RG6BRMSp4Aojd5FzNBCEAgMgKI3sgc0oY5iN42qDNm7gQQvblHAPOHAARCE0D0hiYe4XiI3gidgknJE0D0Ju9iJggBCERGANEbmUPaMAfR2wZ1xsydAKI39whg/hCAQGgCiN7QxCMcD9EboVMwKXkCiN7kXcwEIQCByAggeiNzSBvmIHrboM6YuRNA9OYeAcwfAhAITcCuuxwQ0FUwgAAEghJA9AbFzWAQgAAEsiewQNJnJL1P0sWcaSB6c/Y+c2+DAKK3DeqMCQEIQCBfArslbZW0V9J9+WIQmd6cnc/cWyGA6G0FO4NCAAIQyJKAZXm/JulqSa9KemvO2V4yvVm+B5h0iwQQvS3CZ2gIQAACmRGwLO8WSW+S9C1J+3LO9iJ6M4t+pts6AURv6y7AAAhAAAJZEPCzvG7CWWd7Eb1ZxD2TjIgAojciZ2AKBCAAgYQJ7JH04V6W9xVJb+llez8uaVvC8x44NURvjl5nzm0SQPS2SZ+xIQABCORBwLK8FySZ2LWyhrdJ+qqkN/bE78Ica3sRvXkEP7OMhwCiNx5fYAkEIACBVAlYhndXL6P7SUnu3vNBSZYB3iHJMr5ZHYjerNzNZCMggOiNwAmYAAEIQCAzAtx79PqSZexUEl/kt/Fh5J9L+kVJ3w2M43lJt0t6MfC4bQ3Hhact8owLAQhAIF8C3Huc6H3tNXRvLO8D2xu655fQJlmtz0299fxCjv1vJP1Mr9Yo5LhtjcWFpy3yjAsBCEAgXwLcexC98UV/i6L3LyRd30LG1RbN/ukWxHZbzufC0xZ5xoUABCCQLwHuPYje+KI/Q9F7XtJySfYzh4MLTw5eZo4QgAAE4iLAvQfRG1dEFnUN7ZU3tJXptSVVfrK3tEp8DqnfIi489TOlRwhAAAIQGE6Aew+iN773SIai9y8l/bgk+5nDwYUnBy8zRwhAAAJxEeDeg+iNKyIzzfR+XdKPSnopPm80YhEXnkaw0ikEIAABCAwhwL0H0RvfGyTDTK+J3R+R9I34vNGIRVx4GsFKpxCAAAQggOgdHgPFOr0sWRbPGyVD0Wti94clvRyPFxq1BNHbKF46hwAEIACBCgLce8j0xvfGyFD0mthdnNEe4Fx44nvbYREEIACB1Alw70H0xhfjGYrei5J+UNIr8XmjEYu48DSClU4hAAEIQIDyBsobOvUuyFD0flPSWyVd6pSjpjcW0Ts9O86EAAQgAIHpCHDvIdM7XeQ0eVaGotfE7kJJl5vk2mLfH5a0S9I2SZ+0Gvre++6Dkj4mabukj7doH0NDAAIQgED6BBC9iN74ojxD0Wti9/sk/VV83qjFogW9jTesfONbkt7W2+r5TZLe0hP8VuLBAQEIQAACEGiKAKIX0dtUbE3fb4ai18TutZJenZ5a9GdaRneLpDf0HtgzIfwdSfskfTR66zEQAhCAAAS6TgDRO6voffDBB7Vjx445gbBr1y5t327f2A4/nn/+ee3cuVOPPvqoFi60b7dHHxcuXNAdd9xR9L9y5crRJ0zY4qmnnpLN6fHHH9dzzz2nw4cP6+GHH9a8efMm7Gn65hmKXhO7JgItC5rqYfP7mqSrvQnavK2WmSxvql5nXhCAAATiIYDorUP0mj+dyL18+bI2b96sxYsXjxS+sYvecYV43fE8QvT+LUl3SXq47nEl/YWk63tfvTfQ/cAuTezO72U+Q44beizL9lp97xslfbtXx0uWN7QXGA8CEIBAngQQvXWLXosjE7ObNm3S/v37tWzZMln2dNWqVf0QO3XqlJYvX15kbE+cOKGbb765n1kttytnc0dlev2x1q9f38/SuvNsPDv8bLTZu2bNGp0+fbp43frwM7179uzRtm3bCiHvstr++f6Y9vq5c+dmyg4PEL0mCrdK+kjPZ5Z6tgCu82hL9JoANDH/13VOJsK+/GwvWd4IHYRJEIAABBImgOhtQvS6bO+6deu0aNGiOQLYxOTJkycLUXj27Nl+ecP58+cHtvNLC4aJXideDxw4oBUrVvQzzlu2bCl+X716dSG0fVFu9tlrZqv9tNKGQaLX3ghm97PPPqsNGzbo2LFjxfxcuYUb07WbtiSiJHrfLOk3eoLXuragvb9XC1r3e7Mt0Wu1rfZQ13frnlCE/bnaXmp5I3QOJkEAAhBImACit2nRW87UmqB0dbK+6C2XEvjtxhW95XP8+ly/fxPOGzduLAS3HX5dsV9y4Wp6XabXiWb/fBPrft3vILsneRP1RO81PaFr2V0LVFdU/HVJ3z9JfxO0bUv0WobXHvCqO3M9wdSDNbVs72ckvY9a3mDMGQgCEIAABP5mucysWVxlYuO116bTG5YZtcN/cK2cjS0/7ObKDsqid1C7cUWvn0W2c8oC1i+duP7664tMrYlW9+CaCeNhoteywSbifdH7zDPP9DPXNmaNoveopFt7GVAXoC4TaplR5zD76f9z2eBJX7Pz7EGrX2yhpne64Mv6bdu5ydt1hgMCEIAABNojQKa3iUyvXz5QFpWDMr2WVfXFZ52Z3kceeUT33HNPf8WHaTO9VaK3wUyvLeH1m70Hn0zsukzveUk/1HvPmJBwYsL9Pur/duqwti+28H6c+kNXC7Yy5IQEWlyNZEJLaQ4BCEAgaQKI3rpFb3n1Br/EYP78+UVtrR3lml5f9JbbjZvpHVTTe/fdd89Z5swywnv37i0yvUuWLJlT7zusprdK9Aao6fXFr3vQy8TwIwm9NRG9CTmzPBVEb8LOZWoQgECXCCB66xC9w9bpdSL44MGDspKCBx54QEePHi3W5rXDHgKz49ChQ0Wmt6pduR7XrfrgR5qtCGGlB4NWbzChu3bt2uIUG8MeRnMi1l+94aGHHipKHHbv3t1fp9fV9FaJXn91CpufCWzre5a1fQeIBNuxzGV+bYkve8AtlQPRm4onK+aB6E3YuUwNAhDoEgFE76yit0veDmFrVY3zpOOOEAn2ENudkvZO2m/E7RG9ETtnVtMQvbMS5HwIQAACtRBA9CJ6ZwskP0tsPflrA0/bc4YiAdE7bbB04LwM47kDXsFECEAgQwKIXkRvfGGfoUhA9MYXhrVZlGE818aOjiAAAQjUSADRi+itMZxq6ipDkYDorSl2Yuwmw3iO0Q3YBAEIQADRi+iN712QoUhA9MYXhrVZlGE818aOjiAAAQjUSADRi+itMZxq6ipDkYDorSl2Yuwmw3iO0Q3YBAEIQADRi+iN712QoUhA9MYXhrVZlGE818aOjiAAgdoIsPNnbSi73dFM2xB3e+pxWp+hSED0xhmKtViVYTzXwo1OIACBWglwn6kVZzc7s/sRojcy32UoErgYRRaDdZqTYTzXiY++IACBeghwn6mHY6d7QfRG6L4MRQIXowjjsC6TMoznutDRDwQgUB8B7jP1sexsT4jeCF2XoUjgYhRhHNZlUobxXBc6+oEABOojwH2mPpad7QnRG6HrMhQJXIwijMO6TMownutCRz8QgEB9BLjP1Meysz0heiN0XYYigYtRhHFYl0kZxnNd6OgHAhCojwD3mfpYdrYnRG+ErstQJHAxijAO6zIpw3iuCx39QAAC9RHgPlMfy872hOiN0HUZigQuRhHGYV0mZRjPdaGjHwhAoD4C3GfqY9nZnhC9EbouQ5HAxSjCOKzLpAzjuS509AMBCNRHgPtMfSw721Nf9HZ2Bukabusn53JwMUrY04jehJ3L1CDQHQLcZ7rjq8YsdaK3sQFa6PiNkt4gyf280NtquQVTGHJMAlyMxgTVxWaI3i56DZshkBwB7jPJuXTyCaUoessUbL/tnLKmk0dB+2dwMWrfB41ZgOhtDC0dQwAC4xPgPjM+q2RbInqTdW2nJsbFqFPumsxYRO9kvGgNAQg0QoD7TCNYu9Upordb/krVWi5GqXrWvma5qviihW9bEvYxU4NABwhwn+mAk5o2EdHbNGH6H4cAF6NxKHW0DaK3o47DbAikRYD7TFr+nGo2iN6psHFSzQS4GNUMNKbuEL0xeQNbIJAtAe4z2br+byaO6CUIYiDAxSgGLzRkA6K3IbB0CwEITEKA+8wktBJti+hN1LEdmxYXo445bBJzEb2T0KItBCDQEAHuMw2B7VK3iN4ueStdW7kYpetbHmRL2LdMDQIdIsB9pkPOaspURG9TZOl3EgJcjCah1bG2ZHo75jDMhUCaBLjPpOnXiWaF6J0IF40bIsDFqCGwMXSL6I3BC9gAgewJRHefuXDhgu644w6dOHFijnOuv/56HTt2TMuWLZvYaU899ZQOHz6shx9+WPPmzZv4/Oeff147d+7Uo48+qvPnz/d/X7hw4cR9xXgCojdGr+RnU3QXo/xc0NyMEb3NsaVnCEBgbALR3Wec6N2+fbtWrlw59kSGNaxT9KYidH1eiN5awoxOZiQQ3cVoxvlwukcA0Us4QAACERCI7j4zSvQ++OCDWrp0aZENtuPxxx/XmTNnZCLZxO2qVav6WE+dOlUIZyd69+zZo23btmndunXF6zbWxo0bi8ytZZCrzl++fHk/83zzzTfrgQce0Cc+8Yki62sC2D9n/fr1RTbZjs2bN2vx4sXasWNH8f9du3YVNsZ4IHpj9Ep+NkV3McrPBc3NGNHbHFt6hgAExiYQ3X1mlOj1s7ZOXJqIXbRokTZt2qT9+/cXAtbE8MmTJwsR+uyzzxblDcNEr/U16PyzZ89WljdYqcOaNWt04MABrVixoi90t2zZUvxuhxt/w4YNU5dnjO3NKRsieqcEx2m1Ehh4MbJPuu7Toz+ifQq1N/okX7/4tUrl86rGmWaMWqkE7GwYm1nNQPTOSpDzIQCBGghEK3rLNb0ui2oC1IlTm7+rtS3fv3xxPI7oLdcK++cPEr3PPffcnFphO8fum4cOHSp+rl69usgSlzPKNfit1i4QvbXipLMpCYy8GI36RDzOuKNEr/XhfyXjf5U0Tv+0qSaA6CUyIACBCAiMvM+EtnHUfe3y5ctFFtWyuy+88EK/tMHsLCdqnFAeV/QOOn+Q6H3yySf72WR7QM7dT/ft21fYMqiMIjTTUeMhekcR4u8hCIy8GFVdHOxNZ1+3nD59Wi4rO3/+/P5Fwq9j+tCHPqT777+/eEq2KoNrb9qy6C3XRr300kvFVzZWO+W+3jl48GBxnqunKsNyF4Ybb7xR9957r/yncq1/q3168cUX9Y53vKP/1ZCr03IXMfcErn+RcuO5i2KVHX57v8aq6nX/A4F9orcaLjuOHj06x2b/aWOz7xvf+Ea/RmxQoCB6Q7yFGAMCEBhBYOR9JjTBUaLX7HHJF/v9pptu6tft2nXcfds5TqbXrvEua2ylCoPOnzbTi+gNHT2Dx3tN0lXxmIMlFQRGXozKF4fy/+0NfO7cuUI4Xrp0Sffdd592794t+3R63XXXFReKSTO9Tgi7miUr1HeZYF8k2wVnUA2TE+a33XZbcW659so/z7Ut10y589wDDP48Hnvssb5Y9+2wi5pbtsYamOC/8847iyVoql63Nu6rMxO9Jrx9ce/m7s/b5rJ3796RtVuIXt7zEIBABARG3mdC2ziO6HX3hbe//e1zRK4TrS7RY7ZX1fS6a7d/vfZFb/n8aWt6Eb2howfRGw/xyS0ZeTEqXxxcPZH7pFsWtC6LaplVewLV/zrGPYnqm1lV01t+OtWvWbLaJbfMjP8VVHnZGf/TtdVR+fVOwz5tm73lmil3UXF2l5n4dlibqrUaBy1nU870+lkAl2m4++675zz9O27tFqJ38jcEZ0AAArUTGHmfqX3EER0OWqfXTit/m+cnXfxv+Nw9zr6Vs3ubX3trAtZ9G/rQQw/pK1/5SpHcWLJkSfGNqH1DWD7fxnarRUyyegOiN3T0IHrjIT65JSMvRlWi11+uxYb0SweqhOikmV43jXJfgy5UR44cKWqu/GVb2GtS+QAAIABJREFUbr/99jkPH5RFry9M/SywL9KtNOGee+7pi+yy6C0/BGF22EXL+lu7dm3R3L1mv1e9Xha9ZbtsXjYX/4lfRO/kgc4ZEIBAawRG3mdas4yBgxGgpjcYagYaQmDkxahK9A7bdcaE3Ve/+tWipMHKHOxp1zpFr7/e4TDPljO95boqfw7lLOw4md5x7BiUifZftyVw/PKGKtFLppf3MAQg0GECI+8zHZ4bpo9JIEXR+2FbG1nSNkmflORqej8o6WP2gL6kj4/Jh2ZhCIy8GI2q6TWRa0LNflrZgC2jYl/N2JOsX/rSl4osaV2i15D4ta1+LW5VeYN9vbR169Z+9rW8nqLbLnJYTW95PJdx/dznPld4yM3PraPoP+lr4tbV9D7zzDP9J4D9162PUaLXxqCmN8wbglEgAIHaCYy8z9Q+Ih1GRyBF0btA0gVJr0j6lqS3SXpR0pskvUWSbSB9MTpP5G3QyIvRqNUbXGmDZSzL2U+3q80tt9wyZ2cbf63DqtUbnEuqMqXlVRP88gHflU5oX3vttUX9lL9yRFV9bdWON1bqMGiVhkF2TPr6OOUNJnr90g6/RmzYHvHU9Ob95mb2EIiEwMj7TCR2YkaDBFIUvYbLMrpbJL2hJ3BNCH9H0j5JH22QJ11PRyDZi1GTmz5Mh7q+s8oPEw7qGdFbH3N6ggAEpiaQ7H1maiIZnpiq6DWR+zVJV3s+fVXSW8nyRhnlyV6MUhK95eyx/+DgsKhC9Eb5nsMoCORGINn7zChHutWJctpldFgSJtU1bC3ba/W9b5T07V4dL1neUe+Odv6e7cWoHdxhR0X0huXNaBCAQCWBLO8zfhnd8ePH5+zqlmOcpJrpNV/62V6yvHFHd5YXo7hdUp91iN76WNITBCAwNYEs7zOI3rnxkrLotZm62l5qeae+TgQ5McuLURCyEQyC6I3ACZgAAQhkeZ9xZWkvvfSS7J/b0CnXcEhd9Fq29zOS3kctb9QhnuXFKGqP1GgcordGmHQFAQhMSyDL+4xbCvP06dP9Xd6mBZjCecNEr61vyxEXgVTrr7O8GMUVWs1Zg+htji09QwACYxPI7j5TXopyw4YNOnDgQLFdsa3LPmypybGpdqzhUNH72mvo3lj8mbhwyO5iFEtchbAj8dgNgZAxIACB2Qlkd5+xUgbbQt7WWLfDbUFvW9u712bH2q0eEL0d8VfiwiG7i1FHwq4WMxOP3VoY0QkEINA4gezuM+UlM93mR+vXr5fbCbRx6pENgOiNzCGDzElcOGR3MepI2NViZuKxWwsjOoEABBonkOV9xt/l063T+9hjj2np0qX9HUobJx/RAIjeiJwxzJTEhUOWF6OOhN7MZiYeuzPzoQMIQCAIAe4zQTDHPQiiN27/9K1LXDhwMepIHE5jZuKxOw0SzoEABMIT4D4Tnnl0IyJ6o3NJtUGJCwcuRh2Jw2nMTDx2p0HCORCAQHgC3GfCM49uRERvdC5B9HbEJZg5JgFE75igaAYBCDRJANHbJN2O9I3o7ZCjJLFOb0f8hZl/QwDRSzRAAAIREED0RuCEtk1A9LbtgTHHT1w4cDEaMw662Czx2O2iS7AZAjkS4D6To9dLc0b0diQIEhcOXIw6EofTmJl47E6DhHMgAIHwBILcZ/xtf/0p1rUhxIULF4qlxmxziUWLFhU7q9kOawsXLpyJ6OXLl7V582atW7dOK1euHNiXLYF2+PDhzq7zi+idKUzCnZy4cAhyMQrnLUbyCSQeuzgbAhDoBoEg9xkTvZs2bdL+/fsb2eYX0TtbsCF6Z+MX7OzEhUOQi1EwZzHQHAKJxy7ehgAEukEgyH1mlOj1M8FuswiXpfU3kvB3TXNZ2IMHD8pef+GFF+Zkem+88Ubde++9uv7663Xs2LG+2HbbDpt7yn978MEHtWPHjsJzp06d0ooVK+Zkes2WDRs2zOnP2vqZXvu/ZYfNLtePyxIPmqc7/5prrtHevXtVZtB0KCF6myZcU/+JC4cgF6OaXEE3ExJIPHYnpEFzCECgJQJB7jPDRK+fpTVxaMLz3LlzRanA2bNn+xniJUuWFGJy8eLFhbj12z377LNatWpVIVStvGHNmjW67bbbinYmck+ePFn0Z+3sPHvNRLX9bodrd+bMmeJ3t1Xxvn37ijZW3mDHoBIGX/TaOc5+G8+JZLPLSjCsL/tZZf+RI0f6f3N2hYgLRG8IyjWMkbhwCHIxqsENdDEFgcRjdwoinAIBCLRAIMh9pqqm12Vty0LUCU6ryX3yySf7gnXevHn9jOqePXu0bdu2fq2tX3tr4tIvpTBRvXHjxqLOd9myZXMQm/g1obtly5bK2l3Xr2V8/+iP/mhgnbATvYPsWr16tW644YY5tcb+B4Hz58/PEeM+g1nrkseJqcZEr5+OLxvip+39v41bSD3OxKzNLAXXfvC4T12jCrzHtWuadokLhyAXo2m4c87sBBKP3dkB0QMEIBCCQJD7zLBMr1++4Cbsyg6eeeYZrV27dg4H++r/kUce0T333FNkZS07XBa9/oNsVbrFlR5Yx/Yw3d13391/EM5/YM31a+2s9OA973lP5QNtTleZPXfeeWffLjvPMrpLly7VddddN0fY+naZ6PWzyMmIXt9zo2pcXNu6Re8s76Jhn5hm6XfacxMXDkEuRtOy57zZCCQeu7PB4WwIQCAUgSD3mVGid1DZgMvEmpgclgwclun1xzYR7UodLHM8bqbXknvDVoWoO9Nr/fllGE0HQ2OZ3lGit6pg287xl8zwC6ldtrVcMO0+JVx77bVXFFM759x11136lV/5FZ0+fbpvlss2X7p0qfjUc+LEieJv9knIpf9tLPsU9tnPflaf+tSn+l8vDCvQtq8p7Dh69OgVheOzODNx4RDkYjQLf86dnkDisTs9GM6EAARCEghyn5mkpteEqIlg+2kZUL9Uwa+DPX78+Jxa3XJN79atWwsd49f0+uc4nWOZXVcjbOBdTa+N+zu/8ztzdI7L2lq//lFXTa/VJLu6ZmdLiGBoRfQ60XjgwIH+E4NWsO3Xmtjk/U9EfhG2L4atnRVyO6eXC6arPlU5Z956662FyLYaFHOsH6z2ScfVxvjlDcuXLx9ZoO0/CekK0Wd1ZuLCIcjFaFYfcP50BBKP3emgcBYEIBCaQJD7zKhvtv2kWXlFBT8Z6K9qUF69wZUfuIysW73BP8c9NGcJPXvdkn9PPPFE8ZCbSzD6ScTy6g2Dvu2uY/UGPzk4qNy1qeBoRfSWa21devvQoUNFmrtcSF1+4nFYet/v24rGy6LXF89lqL6TB4nectp/WIH2oK8rpnFm4sIhyMVoGu6cMzuBxGN3dkD0AAEIhCDAfSYE5RFjzPKsVR3mtyJ6/RS81ZqUl8ywifmF1P4nFn/StuRF+SnBYaK3PK71VS4sd5+8BoleO8evPxlWoI3oHTtEuRiNjap7DRG93fMZFkMgQQLcZyJwapaid1Smt1xIbX4atAxH+cm/QaK3KutbziDXken1M8uI3rHfYVyMxkbVvYaI3u75DIshkCAB7jMJOnXSKbWS6R2nptcVONvyF25xY5ucK7y2Ol6rCS6XG1SJ3ttvv70o0naLNDtIZdFrf7cdQmxHk2lrehG9k4Zg0Z6L0VTYunESorcbfsJKCCROgPtM4g4eZ3qtiF4zbJzVG4atOed28xgn0/vzP//zev/73z+Hhyv4tgWh3dp4VtRtGWHLNLui7qeffnqi1RsQveOE3RVtuBhNha0bJyF6u+EnrIRA4gS4zyTu4HGmF0T0jmMIbYYTSFw4cDFK+A2QeOwm7DmmBoGkCHCfScqd000G0Tsdt+BnJS4cuBgFj6hwAyYeu+FAMhIEIDALAe4zs9BL5FxEb0ccmbhw4GLUkTicxszEY3caJJwDAQiEJ8B9Jjzz6EZE9EbnkmqDEhcOXIw6EofTmJl47E6DhHMgAIHwBLjPhGce3YiI3uhcgujtiEswc0wCiN4xQdEMAhBokgCit0m6Hekb0dshR0m6qiPmTmomF6NJiXWoPaK3Q87CVAikS4D7TLq+HXtmiN6xUbXbMHHh8Fq7dBk9AIFUP7AFQMcQEIBADQQQvTVA7HoXiN6OeDBx0duGFxZI+oyk90m6GNgAE/mIwMDQGQ4CEMiaAKI3a/e/PnlEb0eCANFbu6N2S9oqaZ+kbbX3PrxDE73fYzvRBR6X4SAAAQjkSgDRm6vnvXkjejsSBIjeWh1lWd6vSbpa0quSfkDSy7WOMLyz70r6txC9AYkzFAQgkDsBRG/uEUCmtzsRgOit1Vf/tJflfZOkb0l6SNJHax0B0RsQJ0NBAAIQGEkA0TsSUfoNyPR2xMeI3toc9ZZelvdveT2a8LVs7zdqGwXRGwglw0AAAhAYiwCidyxMaTdC9HbEv4je2hzlZ3ntATYrdTDR+7Ckj9Q2CqI3EEqGgQAEIDAWAUTvWJjSboTo7Yh/Eb21OOrNkv5S0jclfUfSIkkv9mp7Tfy+VdJLtYw0vJO/lvRGSVbbywEBCEAAAs0TQPQ2zzj6ERC90bvodQMRvbU46kOS9ki6r5fZNeFrD7PdLeljknb06ntrGWxIJ4jepgnTPwQgAIG5BBC9RARLlnUlBhC9jXjq25KsttdEaMgD0RuSNmNBAAIQkBC9RMFw0Quf6AiwoUG9LkH01suT3iAAAQjESoB10WP1TGC7UhdS7H4VOKA6NJyJ3nm9+t6QZlum15ZLC51hDjlHxoIABCAAgXgItLkLaTwUMtgOFdEbVbhFZYyt2jC/BdHraokRvVGFA8ZAAAIQSJaA24V0b++5lmQnOmpiZHpHEeLvqRJA9KbqWeYFAQhAAAKOQHkXUlupyJbszPJA9GbpdibdW5/XljGzMoeQB5nekLQZCwIQgEDeBCzLu6VXVmfJnn05Z3sRvXm/GXKe/auSbIc2RG/OUcDcIQABCKRLwM/yulnavS/bbC+iN91gZ2bDCbQlett6gI54gAAEIACBvAjY2vQf7mV5X+kleizb+3FJ2/JC8fpsEb05ep05GwETvW4b4pBEEL0haTMWBCAAgTwJ2P3tgiQTuyZ03ybpq70dQe1bzoU51vYievN8MzBr6a8kXdO7GITkgegNSZuxIAABCORJwDK8u3oZ3U9KcqtZfbC3O6ntQmoZ36wORG9W7mayHgFEL+EAAQhAAAK5EGAJV8obcol15llBwETvtb0yh5CALNNr6wOHfoAu5BwZCwIQgAAE4iKA6EX0xhWRWBOUQFui12qr2lgqLShcBoMABCAAgagIIHoRvVEFJMaEJXBZ0ve2kOlF9Ib1M6NBAAIQgMDf1PRmzYKa3qzdn/XkTfR+n15/oC3kgegNSZuxIAABCEDACJDpJdPLOyFjAm2KXlsuxsQvBwQgAAEIQCAEAUQvojdEnDFGpAQuSfr+FjK9ba0PHKkbMAsCEIAABAIQQPQiegOEGUPESsBEry3ObRnfkAeiNyRtxoIABCAAASOA6EX08k7ImACiN2PnM3UIQAACmRFA9CJ6Mwt5pusT+KakRWR6CQoIQAACEMiAAKIX0ZtBmDPFQQTaEr1trQ9MJEAAAhCAQL4EEL2I3nyjn5nLRO9bJVmZQ8gD0RuSNmNBAAIQgIARQPQienknZEzgFUk/gOjNOAKYOgQgAIF8CCB6ExS9H5a0S9I2SZ/0Ptl8UNLHJG2X9PF8YpyZDiGA6CU8IAABCEAgFwKI3gRF7wJJFySZoLHF/98m6UVJb5JkGwLYElUXc4lw5jmUgMWIxYeVOYQ8rLzBtj8OvRNcyDkyFgQgAAEIxEUA0Zug6LUQs4zuFklv6AlcE8LfkbRP0kfjikGsaZGAffh5ewuit62d4FpEzdAQgAAEINAyAURvoqLXRO7XJF3tBZhtCGAPLZHlbfldF9HwiN6InIEpEIAABCDQKAFEb6Ki12V7rb73jZK+3avjJcvb6Pupc52b6P3BXilMSOPJ9IakzVgQgAAEIGAEEL0Ji14/20uWlzd8FYGXJf1tRC/BAQEIQAACGRBA9CYsel2212p7qeXN4N08xRTbEr1tbX88BSJOgQAEIACBRAggehMXvZbt/Yyk91HLm8hbtt5pmOhd3EJsIHrr9SO9QQACEIDAaAKI3p7oNRAc3SRwVTfNjsJqRG8UbsAICEAAAhAIQADR60Tva6+hewMEXK1DXHVVoXcRvdNT/YakHyLTOz1AzoQABCAAgc4QQPQiejsTrFcYiuid2Xcmen9YkmV8Qx62GYYtn2dlDhwQgAAEIACBEAQQvYjeEHHWzBiI3pm5InpnRkgHEIAABCDQEQKI3q6J3gcffFA7duyYE1+7du3S9u3bR8bc888/r507d+rRRx/VwoW2G3G3D0TvzP57SdISMr0zc6QDCEAAAhCInwCit4ui1+LKidzLly9r8+bNWrx48Ujhi+iN/x0Z2EITvT8iyTK+IQ/KG0LSZiwIQAACEDACiN6ui17zoonZTZs2af/+/Vq2bJmeeuoprVq1qh/ip06d0vLly3XHHXfoxIkTuvnmm/X444/rueeeu6LdypUrO/PWINM7s6sQvTMjpAMIQAACEOgIAURvCqLXZXvXrVunRYsWzRHAJm5Pnjyphx9+WGfPnu2XN5w/f35gu3nz5nUifhG9M7vp65KuayHT+4qkt0myjC8HBCAAAQhAIAQBRG9qorecqbWs7+HDh68QveWaXr8dojfEey+KMUz0/qgky/iGPBC9IWkzFgQgAAEIGAFEbwqi98KFC0XpgtX5mugtP+y2fv36StE7qB2iN5urA6I3G1czUQhAAALZE0D0piB6/ZpeK1swMWtlDZbNHZTptXreQe0QvdlcGP5S0lIyvdn4m4lCAAIQyJkAorfrore8eoOJXCdm58+fX6zsYEe5ptcXveV2iN5srgkmen9MkmV8Qx4XJf2gJCtz4IAABCAAAQiEIIDo7aLoHbZOrxPBBw8e1PXXX68HHnhAR48eLdbmtcPKIOw4dOhQIY6r2nVlDV8eZJv5GoHonRkhHUAAAhCAQEcIIHq7Jno7ElhBzET0zoz5gqQfJ9M7M0c6gAAEIACB+AkgehG98UfpIAsRvTP7zkTvT0iyjG/Ig/KGkLQZCwIQgAAEjACiF9Hb3XcCondm37Ulel+WtFiSiV8OCEAAAhCAQAgCiF5Eb4g4a2YMRO/MXM9L+skWMr2I3pldRwcQgAAEIDAhAUQvonfCkImoOaJ3ZmeY6F0myTK+IQ9Eb0jajAUBCEAAAkYA0Tur6C1v8ODi6uabb+6vldtUrLmx3eYToZYas3WBd+7cWawIYesCu99Dr/qA6J05shC9MyOkAwhAAAIQ6AgBRO+sotc5urwrWtMBYEuT3X///brzzju1bJkl68IdvugNLXT9WSJ6Z/b51yQtJ9M7M0c6gAAEIACB+AkgepsUvbZRxK5du/Tiiy/qHe94R7FBxPHjx7V27doiNGwd3WPHjhWi1bK2Fy9e1Be/+EWdPn1afvbWzyZbf1u2bCk2nbA1du04derUFdsPHzlypFiT1+3WZu1MJNq6vZ/4xCd044036t577y1sOHDgQDH+iRMnCnttO2M7zP5Vq1b1w9jGWb58edGvtbVstuvPsr5uBzh3TnkOg+Y37fsE0Tstuf55Jnp/SpJlfEMe35D0w5KszIEDAhCAAASaJ2CCjwMCusrqPF57bbZ4qMr0mmjcsGFDX9j6u6WZQDShaYeJTPvd/m7bB1vJwJo1awoxasfhw4cLwWyHy+4uWbKkEL7r1q0rBK+dZ+3K5y9atKjfl7UzEWx9b926tRCvNu7nP//5wkY7Nm3apP3798/53US59Xvy5Mkrdnbzyxt8u1esWFHYt3jx4qHzM5umPRC905JD9M5Mjg4gAAEIdIvAzDqnW9PF2ioCppsaFb1uS+CqMgATkmfOnOmLQieA3a5qJmh90evX7PptTDjaOEuXLu3vuOYE9e23394XsiZeXebXhK0Ts84GE+4bN24sanTLJRMmyJ34Pnv2bGVNr21t7NqYrb7If+yxx/oCv2z7tKGJ6J2WHKJ3ZnJ0AAEIQKBbBBC93fJXI9Y2Lnp9EehvEexm48oJfNFaFoUmjl1JhCtb8Nu4rOrq1av7otcJahO9/oNm5XpcX3iXRW/5IT1XrjBI9D755JP9bLCJXn8sE71OlCN6G4nlaTr9qqSfprxhGnScAwEIQKBTBBC9nXJXM8YGFb1+iYCJwnKmd5QorBK6rrxhWKZ3GtFrpQp+lrqOTO+o+U3qYjK9kxK7or2J3p+RZLW9IY+XJP2IJKvt5YAABCAAgeYJIHqbZxz9CK2J3kuXLhVZWStNcDW9VaLwhRde6JdA+Cs2TFLTO6vonT9/flGfa4fVFg/K9I6q6UX0Rvd+QPRG5xIMggAEINAIAURvI1i71WlQ0esednMrH9x111164oknCiG5b9++yq//XemCW6mhqrzBPQzmlyP4qzdMI3qdqLZxbYUHW6Xh6NGjxdq8dphgt2OS1RsQvdG9OV6U9HfI9EbnFwyCAAQgUDcBRG/dRDvYX22it4Nz77zJlDfM7EITvT8ryTK+IQ/KG0LSZiwIQAACNaxSBcTuE0D0dtiHiN6ZndeW6P26pOuo6Z3Zf3QAAQhAYFwCZHrHJZVwO0Rvh52L6J3ZeX8h6edayPSa6P1RSZbx5YAABCAAgeYJIHqbZxz9CIje6F002MCe6HUN/N1FyjuNTPO3Ovow26YZe9h5ddplO7H9oiTL+IY8EL0haTMWBCAAAcobiIHezry1bE4BzfAEvEyv+dAd/u/2Wh1/q6ufGG0LLXiNJaI3/NuFESEAgbwJkOnN2//F7Mn0djgIKG/orPMQvZ11HYZDAAIdJYDo7ajj6jQ7CtFrm1TY4ZYBm2WCblk0W/vXLWXm+vO3BS5vGTzLmG2di+hti/zM4/6lpB/rZXxn7owOIAABCEBgJAFE70hE6TdoXfTaVr2HDh0q1ru1XdpmPcYVvQsXLpx1qNbPR/S27oJpDUD0TkuO8yAAAQhMR6Azotd00Zo1a3T69OkrZur2IBgHgfXj71Mw6Jzyjrbj9N3VNq2LXoN90003FVlZ56Abb7xR9957b7EpxLFjx7Rs2bKCr2WE165dW/zu/81tT2wbSaxfv162i5vL9PrBs2vXLlm21/pxmd49e/Zo27ZtWrx4sXbs2FH0be3sfDus/apVq/qvnzt3rthMow6BPmvQIHpnJdja+Yje1tAzMAQgkCmBzohe3z+mV06ePBmN7uh67LQqei0re99992n37t2yzKsTqLfddlshOn1nP/vsszKBbK9ZW/vdDreFsROj1s5E6qlTp7R8+fKiZGLdunXFTztnkOi1vkzM2vkbNmwoxPaiRYuK82wMtzOca4fo7Xrot2o/ordV/AwOAQhkSCAZ0Wta5k//9E8LnWKZ31tvvVWbN2+W27nWkn+mZ86ePdvP9Fqiz+0oa7vL+olDl+m94YYbivbXXnttvy/TUq5U1CUe7dx3vetdWrBgQT9B2JV4alX0lksb7P+bNm3S/v37i+yuieKNGzcWTnDZXgfW4J85c0ZbtmwpnG3C1hzjsr72fxOtg7YgLmd6V69eXQhcf8zz58/r8OHD/U9YJpj9/7ftZDK9bXtg6vEvSPpxanqn5seJEIAABCYlkJTo9b919pOAfomnr4FM87iEoEvi2TfcLnG4dOlSmei1soqtW7f2E4VuHBPQTp+5hKBpLvet+KTOaKt9q6K3LCLL9Se+AF2yZMmcTzIGzMoQ7r777n42tix6rY2fHfb7L4teJ5r9MZ955pk5XysgetsK0+TGNdH7E5Is48sBAQhAAALNE0hK9BquKsE5KPFnmsfXQy5xWBa9fuLR1zzHjx8vEo1uTP/85l1X3witit5RmV4/81sWoHVneqtEL5ne+gKNnuYQQPQSEBCAAATCEkhK9Fpm1q14VfXgm5UllDO9/jfVg0Sv/+24L3r37dtXeAvRO0PQDqrpdal1v6bXPmW4Yu5Lly4Vznap9XLtbzmF70oXhtX0VoleanpncC6nDiOA6CU+IAABCIQlkKTodZldp3OGZXpnEb1kemsK1qrVG1wR9c0339x/cM3VqZw4cUL2+l133aUnnniiqLe1wxVxWwH3Nddco/e85z39FSHc0h8PPfRQ8bCcPTg3TnmD1RG71RuscNtKKexBt3KBeFvLn1HTW1MQhu/Gtj/+ScobwoNnRAhAIFsCWYhep1nqzvRS01vT+8YvcfCfNGxLSA6bll8sbu0+/elPF09NtmUroremIAzfjYleW4fPMr4cEIAABCDQPIEkRa9hKy+taq+5B9NcuUJ5Q65JyxtsxSq3eoMlHu3fK6+8woNs08St25HNLZdhy2q0JSR9+8t1Mm4ZEHO+ZZ4t3f+BD3xgminXcg6itxaMbXSC6G2DOmNCAAI5E+ik6I3RYeWSihhtHGRTqw+ydQlUjLYiemP0ylg2IXrHwkQjCEAAArURQPTOgNLPJls3/iZeM3Qb/FREb3Dk9Q2I6K2PZeCeEL2BgTMcBCCQPQFEb/YhICF6OxwEiN7OOu9rkn5KkolfDghAAAIQaJ4Aord5xtGPEFz0ljegaINQ1aYXbsmySe2x2pb7779fd9555xW7xk3a16TtEb2TEoumPaI3GldgCAQgkAkBRG8mjh42zexFb3l742liorzJxjR9THMOoncaalGcg+iNwg0YAQEIZEQgW9FrK0/t2LGjWHHBFg6IYaGAtuIuqOgtr7X7wAMPFI6wwwyx/3/iE5+QW73B3w3E1se11+04evSobN3cY8eO9bOrzqn2d1ufzjau8Mez163wesuWLf01fa2Pz372s/rUpz4ll+n1V2zwA8RsGTX+TTfdVIwb6kD0hiJd+ziI3tqR0iEEIACBoQSyFL3DthLOMV6Cil4D7Jc32Da/tnHEgQMH+htJDNoCz0RveacwsFcCAAAgAElEQVS1xYsXF2vE+evNuf5tyzwTwm6XEn9LY9tpbePGjbKxlixZUohgE73Lly8vdnqz3+2nnX/u3LliM4ph49u8LLC+9KUvBV2zDtHb2bfsVyX9NDW9nfUfhkMAAt0jgOg9flxnzpwJqlNiC5PWRe+mTZu0f//+ImNbrvctZ3pNhLrUvBO6LnM7qibXr+MdJHr9faot/e8LZRPoVeO7fajbKHFA9Mb2dhrbHhO9PyPJMr4cEIAABCDQPIEsRa9bU/ell16S/aO84SpdJSlYMJQzvX5md5Tordo32rYGtqysic9yaUF5XTlXEjFI9Np7zhe2vlA20Tto32qXwT506FBRomGbV4Q4EL0hKDcyBqK3Eax0CgEIQGAggWA6JyYf+CWbrvQzJvtC29J6pneY6LVPJCdPnuyXF1SJzkGZXlfP68RwHZleRG/o8Ex2PERvsq5lYhCAQKQEshO9fiLRtiHesGFDUU5qzyeZ9qrjQf5IfT3QrOhEr6vxXbFiRVFra4erqR0kOi07a4cJXFeSYA+t2VJiTvSagN67d2/x8Nu0Nb3DRC81vV0L/VbtfVHS36G8oVUfMDgEIJAXgexEr/+8k7na/r927drO7qZWR7gGF70uA2vGl1dr8J1ipQhWumAPkI0Sva5m5eDBgwUTl8J3DrbX7G/Wl9X+OkH99NNPT7R6wzDRa8Kb1RvqCMks+jDR+7OSLOPLAQEIQAACzRPITvRWlYzaggDr168vdFWoUszmXTv+CMFF7/imdadlGw+xGR1qersTIyVLEb2ddR2GQwACHSWQneg1P/nPNrllWB977DEtXbq0eB4qtwPRO6PH2ZFtRoB5no7ozdPvzBoCEGiPQJaitz3ccY6M6I3TL2NZRaZ3LEwxNkL0xugVbIIABFImgOhN2btjzg3ROyaoGJshemP0ylg2/YWkn6OmdyxWNIIABCBQBwFEbx0UO95HX/R2fB45m2/rLHN0i4Blen9e0gvdMhtrIQABCHSWAKK3s66rz3AneuvrkZ4gAIFRBFiybBQh/g4BCECgXgKI3np5drI3RG8n3YbRHSdwXtJPSvrLjs8D8yEAAQh0hQCityueatBORG+DcOkaAgMIfF3Sj0p6CUIQgAAEIBCEQK2it7zrq5tBeUOIIDPrwCD+rrht7gSH6O1AsGBicgRelrRY0sXkZsaEIAABCMRJANHbol8QvS3CZ2gItEzgFUk/IOlSy3YwPAQgAIFcCAQVvVu2bNHmzZuLXWBXrlxZMHY7t9rvtsPrNddco71798ptGrFw4UIN2mHWNsHatGlT0Y9lKy2jbO3tcOfYbrO28cTp06d15MiR/uYT/gYV1t7tWmuv79q1Sy+++KLe8Y53FLu0HT9+vNiq2A7bGffYsWNasmRJMZdy/2fOnNGOHTtG2u92wbWdcct9lnfSHTbPOgKVTG8dFOkDApMRuCzpeyW9OtlptIYABCAAgSkJBBW927dvL4SpCUP73TKd9913n3bv3q3nnntOth2wE6Ymhu2wdv7vJko3bNhQCE871qxZowMHDvRFtOPgRO8LL7xQjHn+/PlCIO/fv79o4n63sgL7+8mTJwuB++yzz/b7t7/ZeDa+E9TOFifgrS93nrP/1ltvLQTx6tWrC5E9yP5FixZp48aN2rlzp2ysaeY5pd/nnIborYMifUBgMgLfkvRmSd+e7DRaQwACEIDAlAQaEb0nTpy4whzLnpqAtazloUOH9MADDxQC80tf+lLxellcWjsTg3bePffcU7Sx7LATs5YtNtHoi1d/UNfOCU/7m4nKqq2GbWzLMjvx6ovc8kScaHei1/Xv7H300UeLbLMb65ZbbimEb5X9y5cv74tem8ugdsPmOaXfEb11gKMPCMxA4K8lvVHSd2fog1MhAAEIQGB8Ao2IXifwnBl+dtfE6P33368777xTn/vc53TTTTcVYtYXnvPmzSvEsS96y0LaMsI33HBD0cYJzSrRWy6lcKLXRKmVIbhj/fr1fdHrBLDZUS6tsPYmxMulGqNEb5X9JohdpteJ3knnOb6rB7ck01sHRfqAwPgEbDMRE7tsKjI+M1pCAAIQmJVAcNFrBpsI/upXv1oIWyttsMxoOdPr/v/II48U4tSVAPgTLgvNKtHrMrF+5ve6666bU7JQzvT6otcvfTARXM70OlE9TPT6JQy+jf6DbOVSh3HnOWsA2PmI3joo0gcExifwBkl/Jcl+ckAAAhCAQBgCrYheE4hWi3vbbbcVX/vb4R4scw+UDarpdedaHa8JxVGZXuvbyhbOnj3bL4Ww+l5XwjB//vyi/ta1s5KLQaL30qVLRQmCZabHzfSWa3p9+/3yhnJN77jzrCNMEL11UKQPCIxP4GpJtmSZ/eSAAAQgAIEwBFoRvX5drlvFwUSvlSnYcfToUblyg6oSA/ew2ziZ3sWLF/fLGJyg9ksWbOUEqy+2MW18e6DOF71u7WErO7AVJe666y498cQT2rNnj7Zt29ZfiWJQptdEb7lEwtnvXn/66afnrAjhVm8YZ551hAmitw6K9AGB8QnMk3RB0vzxT6ElBCAAAQjMSKBW0TuuLVVitVzTO25fg9pVCetZ+0z1fERvqp5lXrESeIukv5BkPzkgAAEIQCAMgeCit1zG4KaJ6A3j8KpREL3tsWfkPAlcK+msJPvJAQEIQAACYQgEF71hpsUokxBA9E5Ci7YQmJ3A90n6vyR9/+xd0QMEIAABCIxJANE7JqiUmyF6U/Yuc4uRwCJJfybprTEah00QgAAEEiWA6E3UsZNMC9E7CS3aQmB2Am+T9K8k2U8OCEAAAhAIQ6Czore8uYTDZSssuC2DJ0E4bCWISfrpYltEbxe9hs1dJvC3JT0jyX5yQAACEIBAGAKdFb0Oj1tSrLwLXBh8aYyC6E3Dj8yiOwR+SNL/JOmHu2MylkIAAhDoPIEkRW95JQh/G2TLEF+8eFFf/OIXdfr06f56wLZ5hdvowtbq9dcMtrV8jx07JttAwl9z973vfW8RALbjmltvuIsRgejtotewucsEfkTS/yDpui5PAtshAAEIdIxAlqLXRLEJYduZzXaGK+/uZqJ31apVss0sVqxYUezYZptcWDbZ35bYdm9z7RC9HYt8zIVAiwSWSvqXkn6sRRsYGgIQgEBuBLIUveZkE7D+Bhb+lsYmet02xQsXLiyE7pkzZ67YejiVDTDI9Ob2tme+bRP4CUn/rST7yQEBCEAAAmEIZCl6ly5dKn974HXr1qksev2tiJ3ovfvuu4vzXP0wojdMkDIKBFIjsEzSv5C0PLWJMR8IQAACERPIQvRa1tZld+33aUXvli1bilIHE8lWzoDojTiyMQ0CERP4aUn/paSfidhGTIMABCCQGoFkRe+GDRuKh88sg2vZWROplqGdRfRS05ta+DMfCLRD4GclPS7p59oZnlEhAAEIZEkgSdFrnnTr+Nq6vfbvlVdeqUX0+qs3bN26VS+//HI/89vVCKKmt6uew+6uEvi3Jf2eJPvJAQEIQAACYQh0XvSGwVQ9im1osWnTJu3fv79YzqyrB6K3q57D7q4SWCHp05L+blcngN0QgAAEOkgA0Tuh08o7wdmyZl1ersymj+idMAhoDoEZCfw7kg5IunHGfjgdAhCAAATGJ4DoHZ9Vsi0Rvcm6lolFSuAdkh6W9PcitQ+zIAABCKRIANGbolcnnBOid0JgNIfAjARWSvodSfaTAwIQgAAEwhBA9IbhHPUoiN6o3YNxCRJYLWm3pL+f4NyYEgQgAIFYCSB6Y/VMQLsQvQFhMxQEJL1T0v2S/gE0IAABCEAgGAFEbzDU8Q6E6I3XN1iWJoFflPRRSf8wzekxKwhAAAJREkD0RumWsEYhesPyZjQIvEvSFkn2kwMCEIAABMIQQPSG4Rz1KIjeqN2DcQkSuEXSByX9ewnOjSlBAAIQiJUAojdWzwS0C9EbEDZDQUDSfyDpLkm/BA0IQAACEAhGANEbDHW8AyF64/UNlqVJ4Jcl3SnpPWlOj1lBAAIQiJIAojdKt4Q1CtEbljejQeA/lPSfSrKfHBCAAAQgEIYAojcM56hHQfRG7R6MS5DAbZL+E0n/UYJzY0oQgAAEYiWA6I3VMwHtQvQGhM1QEJB0uyQTvmugAQEIQAACwQi8FmwkBoqawFVRW4dxEEiLgGV53y3pH6U1LWYDAQhAAAIRE1gg6TOS3ifpYsR2Nm4aordxxAwAgT6BtZJs2TL7yQEBCEAAAhAIQWC3pK2S9kq6L8SAsY6B6I3VM9iVIgF7iM12ZfuVFCfHnCAAAQhAIDoCluX9mqSrJb0q6a05Z3sRvdHFJwYlTMC+Wlot6f0Jz5GpQQACEIBAPAQsy2s7gb5J0rck7cs524vojScwsSR9Ar8m6eclfSD9qTJDCEAAAhBomYCf5XWmZJ3tRfS2HJEMnxWB9ZJWSLKfHBCAAAQgAIEmCeyR9OFelvcVSW/pZXs/LmlbkwPH2jeiN1bPYFeKBGwL4p+VdHeKk2NOEIAABCAQDQHL8l6QZGLXyhreJumrkt7YE78Lc6ztRfRGE58YkgGBjZKWSbong7kyRQhAAAIQaI+AZXh39TK6n5RkaxWb5vugJMsA75BkGd+sDkRvVu5msi0TsIvNj0r6UMt2MDwEIAABCORFwInevGZdmi2iN2v3M/nABO6VtLhXYxV4aIaDAAQgAIGMCSB6e6nujGOAqUMgKAFbNsbqqmyRcA4IQAACEIBAKAKIXkRvqFhjHAgUBH5T0vdJ+gg8IAABCEAAAgEJIHoRvQHDjaEg8PoSMW/OeWFwggACEIAABFohgOhF9LYSeAyaL4HtveVi7KlZDghAAAIQgEAoAoheRG+oWGMcCBQE7u+953bCAwIQgAAEIBCQAKIX0Rsw3BgKAq+vmfhtSQ8CAwIQgAAEIBCQAKIX0Rsw3BgKAtJuSd/sLQwODwhAAAIQgEAoAoheRG+oWGMcCBQEfkvS1yX9NjwgAAEIQAACAQkgehG9AcONoSAg/Y6kr0naCwwIQAACEIBAQAKIXkRvwHBjKAi8vs/5OUkPAQMCEIAABCAQkACiF9EbMNwYCgLSJyT935I+CQwIQAACEIBAQAKIXkRvwHBjKAhIj0h6XtKjwIAABCAAAQgEJIDoRfQGDDeGgoD0mKR/o9d/ckAAAhCAAARCEUD0InpDxRrjQKAg8J9J+l8lHYQHBCAAAQhAICABRC+iN2C4MRQEpE9L+p8l/TNgQAACEIAABAISQPQiegOGG0NBQPpdSSclfQYYEIAABCAAgYAEEL2I3oDhxlAQkD4r6Ut6/ScHBCAAAQhAIBQBRC+iN1SsMQ4ECgKHJZ2QdAQeEIAABCAAgYAEEL2I3oDhxlAQkP4LSV+Q9M+BAQEIQAACEAhIANGL6A0YbgwFAemYpM9L+hwwIAABCEAAAgEJIHoRvQHDjaEgIP1Xko7q9Z8cEIAABCAAgVAEEL2I3lCxxjgQKAj815J+v/cTJBCAAAQgAIFQBBC9iN5QscY4ECgI/AtJhyT9ATwgAAEIQAACAQkgehG9AcONoSAg/aGkT0n6b4ABAQhAAAIQCEgA0YvoDRhuDAUB6QlJn5T0JDAgAAEIQAACAQkgehG9AcONoSAgfVHSPr3+kwMCEIAABCAQigCiF9EbKtYYBwIFgf9O0sck/ffwgAAEIAABCAQkgOhF9AYMN4aCgPRHkh6Q9GVgQAACEIAABAISQPQiegOGG0NBQPofJd0n6SQwIAABCEAAAgEJIHoRvQHDjaEgID0l6Tf0+k8OCEAAAhCAQCgCiF5Eb6hYYxwIFASelvSh3k+QQAACEIAABEIRQPQiekPFGuNAoCDwFUkbJP0v8IAABCAAAQgEJIDoRfQGDDeGgoD0J5I+IOlZYEAAAhCAAAQCEkD0InoDhhtDQUD63yT9ql7/yQEBCEAAAhAIRQDRi+gNFWuMA4GCwL+W9I96P0ECAQhAAAIQCEUA0YvoDRVrjAOBgsCfSvqPJf3v8IAABCAAAQgEJIDoRfQGDDeGgoD0nKT3SHoeGBCAAAQgAIGABBC9iN6A4cZQEJD+T0n/vl7/yQEBCEAAAhAIRQDRi+gNFWuMA4GCwBlJ/7D3EyQQgAAEIACBUAQQvYjeULHGOBAoCPw/kv5+7ydIIAABCEAAAqEIIHoRvaFijXEgUBD4c0l/T9L/Cw8IQAACEIBAQAKIXkRvwHBjqFwJfK+kl3qT//8k3SDJfnJAAAIQgAAE/v/2ri/0jmo7L3vpBUM1+AcNpAQJFvOkIkFCNS8+xNCXgEK0Gp9yi5o/Uq2+xPr/Jpgq6lVj66XCTf1DElAIhRpBYmkUQgiWPBQUUhsigSZVRA1Ka2vKN5x17vqtzJ4zZ87sPTNnfwfE/M6Z2XvNt/da65u11147FQIkvSS9qeYa+8kUgU0i8rKIPCsiO0TkP0Tk2tFmtudE5CUR+XWm2PCxiQARIAJEIB0CJL0kvelmG3vKFoH/EpE/EpH/FZFfish/jv5GBPgaETmeLTJ8cCJABIgAEUiFAEkvSW+qucZ+MkZgq4hsF5GLROTsiPD+n4j8TkR+lTEufHQiQASIABFIhwBJL0lvutnGnrJG4GsRudQhsGy0sS1rYPjwRIAIEAEikAQBkl6S3iQTjZ0QgUdE5CkRuVBEfhKR34rIFsJCBIgAESACRCARAiS9JL2Jphq7yR2BX4jIN6MUB6Q2LBWR07mDwucnAkSACBCBZAiQ9JL0Jpts7IgIPC4ij4nIb0TkYcJBBIgAESACRCAhAiS9JL0Jpxu7yh2BRSLyDyLyF6Oob+548PnbRQAOjR8iAAQuIAznIUD94KQoEKBycCKkQoBGJxXS/e+Hdqf9MTp37hxVrH1Yh9XiBRcUqkX9On/YqB/DmspRpIV+UDmiQMtGSxCg0eG0EDrlaJOA+hUN2uE0TP0KjhX1YzjTOJqkJL3RoGXDJL2cA2UI0ClHmxd06tGgHU7D1C+S3uHM1vSSkvSmxzznHumUcx790bPTKUebBNSvaNAOp2HqF0nvcGZreklJetNjnnOPdMo5jz5Jb+zRp37FRngA7ZP0kvQOYJp2JiJJb2fQZ9kxnXKWw77woemUo00C6lc0aIfTMPWLpHc4szW9pCS96THPuUc65ZxHn5He2KNP/YqN8ADaJ+kl6R3ANO1MRJLezqDPsmM65SyHnZHeRMNO/UoEdJ+7Iekl6e3z/OxaNpLerkcgr/7plPMa79KnpVOONgmoX9GgHU7D1C+S3uHM1vSSkvSmxzznHumUcx59pjfEHn3qV2yEB9A+SS9J7wCmaWcikvR2Bn2WHdMpZznsTG9INOzUr0RA97kbkl6S3j7Pz65lI+ntegTy6p9OOa/xZnpD2vGmfqXFu5e9kfSS9PZyYvZEKJLengxEJmLQKWcy0FWPSaccbRJQv6JBO5yGqV8kvcOZreklJelNj3nOPdIp5zz6o2enU442Cahf0aAdTsPUL5Le4czW9JKS9KbHPOce6ZRzHn2S3tijT/2KjfAA2ifpJekdwDTtTESS3s6gz7JjOuUsh33hQ9MpR5sE1K9o0A6nYeoXSe9wZmt6SUl602Oec490yjmPPiO9sUef+hUb4QG0T9JL0juAadqZiCS9nUGfZcd0ylkOOyO9iYad+pUI6D53Q9JL0tvn+dm1bCS9XY9AXv3TKec13qVPS6ccbRJQv6JBO5yGqV8kvcOZreklJelNj3nOPdIp5zz6TG+IPfrUr9gID6B9kl6S3gFM085EJOntDPosO6ZTznLYmd6QaNipX4mA7nM3JL3tk95PPvlEbr755nHDb731ltx9993F3/jtzTfflBdffFEuvPDCVqbG559/Lk8++aS8+uqrsmjRInnwwQfl9ddflwceeEDOnDlTfH/ZZZdN3dczzzwjy5cvL2RvU+6vv/66aPOxxx6Tm266aWq5Ut5A0psSbfZVOGWreIAECn7HHXfII488MjYk+A4K/vLLL8s111zTCLm3335bDh06dJ4x0v6OHTu2oN1bb71VcE9dY9KG0agyFlb+559/fmysGoFhburaQNEpzzqC7Tv1aBKx4eQIUL/a1Q/Y+c2bN8vevXsLX/Tjjz8WJHT16tWtk8cyyWGvH330Udm+fXtt3xRCwPvetiZn1z5lmucg6Z0GLV47KwIF6QWZ++KLL4q3Qn1Tfvrpp+W6666Tp556qnhbboNQVpHeWQl1W2/4dUlvWxEEyN21gaJTnlWN2nXq0aRhw50gQP1qTz88wdWWbST2s88+G0d6f/jhh4IIf/DBB8Wl8Gvq50A4H3/88Vrfa/u4f+vWrUV78I/PPvus7N69exzptQGce++9dxzgge/bsGFD0RfuA2E/evTo+DtEqq+66qqx3LhOo8n498cff1xEbFWOxYsXF5Fm+5tFeZJPKZPnq6++WhAhR19vvPFGwQGq5IHvxgfzfJoglcpL0tuJWcq204L0WoOBqCqMwbXXXit79uwplnTwNo3JjA8MiBoer3QgxjAKp0+fllWrVhUK/+mnnxbLUFD0NWvWyHfffVca6a0ivUq4L774YnnuuecEEWAYLrztIzqsS1tl11kltEbOLofZ54GhOnHixHhZSJfRvPwa6b3tttsK47R06dJSA2qX4YDNqVOnznv+KgPlsVZDClztEt4sBopOOZr+M70hGrTDaZj61R7prbPiqH5gx44dsm3btnEE2N5rCR6ke+KJJ2Tjxo0S+h7XaHoD/r1ly5bi79D38JkaxQWZxb/VF+Hf+MCHhdIb4F/UV8DWa2Qb99lVWNw/rU8BPmXybNq0afxc3udbmW2kXeXZtWtX4zQKkt7h2LJ5kLRwyiBWqvTLli0b/3vfvn1yyy23yA033FAQu3vuuaeY2FbRrELCYNhlJ33rhUJoGwDN51pNMmRKHPG2q+2AmMKI4K1eFRj/BsFWQmsV1UaZT548WRgOVVT/PGgDfV1++eXj67z8nvTqc1k8cL/mVVU9fxXptc9gr1uxYkVrBopOOZoqk/RGg3Y4DVO/2iW9Sj5DaW+hVUnYTyWrPqqpEobutYGhEOm1EeaqlUC7slpGepWsq7+10e2VK1cuSDOseta6Ob1eHvX5ygmsH4P/V3kgH36bdZWWpHc4tmweJB07ZVU+KJUuaYDAHTx4UPAGqDlMmsRfppD+jdYrZJVBAQn1Ob26FFX2ZqrJ/35Zy77B6m8gqPhec74wcPq8Gqn1z4O/8bHRVCu/J73adpVhndVAWWOjLx9tGCg65WiqTNIbDdrhNEz9apf0TiJZ1s7qSqNKoKkFGsnUlAO78meX/vX7OqT3wIEDpXtW/GodZFHfVkZ6EQFG1NluQrP+2ZL+Jj6lSh60B5+/fv36MQ/wKSKKJbABX5j0EjJJU0l6JyHE39tEYOyUdbKDRCGKirdEXTK/66675J133ilye1QByhTS5iThTdfn8FaR3jrpDRohtoYilMuF/j3p1XQMBRCGB4TevhFbYgkc7Ma7KtKrpNmSXuRshe63kYCqSG/ZJj/N72rLQNEpt6lSC9oi6Y0G7XAapn61R3pDOb12c5lGXD15tLbZbsb2wQSV1kc0J6U3hCK93g/OGumdlfRWyaM4Ap8rrrii8I0h3ICTT41sopUkvU1Q4z1NERg7ZUzs1157Tb7//ntZt25dkcYApUdEE/m9Z8+eXZDPG4r0hiKjVZvh6qQ32HarSK+N9GqEGJFrfK8yW7C8wbN/TxPpLSO9fglt2rdyb+C9rG0ZKDrlpuoz8b7ekF7Mf8w/m+Puo1d2w4+PjOkSp24I0t/tZh2Lhr6saW6/vuTpHD58+PB49/1EFEcXhDbC+n5njTzVlafuddSv9kgvWgpVb8C+ChBdtbOe9GL+YE+IbiLTzds2vQ+BirLv0e8k0gt7b4M3mpqGVUkNfmjQCP61aU5vm6TXy4PnhNzvvvvuAv20aXY2bRF2YVZ9I+mta0l4XRsIjJ2yKj42mtlSLIiugvBiuUPfjqtyei05tRHMWXN665JezcfV5X+ABONiHaYqOogqHL39TZfDEE1F3mwoJ9enN5SR3llzestK8djna8tA0Sm3oUqlbfSK9GKnul3GLSO9obqeZasRoaibRoBAAC655JJihUhtB/pEruA333wzdflDkt5o87Srhhvrh90gDOFtVQYbXHjvvffGFRKw0gf7Dlut/khX/1Qv/NL/NOkNyDG2cukLoU0PwCbs+++/X95///1ib4vKN231Bq0LPCmQ4l9SrV/Db16eUHCqDi51S4v6yUbS25X65dnvAqNT5lR8Pi1gqqre4IuCWyMAA3P8+PFxGTSFPFSnV/OvfMR0UnqDVnnwUahQiRpfvQH3a7Q7JH8d0gtHr/fjWZBKAaPrN/IpoSgzUMBIi7DDsOOj+cwa9fCYT2ugSHqjKX9jp962RJj7WMVB3rzW2p6V9EJGX+7Q6jQiQDfeeON4mVSvRzH/I0eOjCvDWP33tbmt/tjqL2irqqRT08MC2sYd7VG/2o30xhgjtvl7BKx/TYELSW8KlNmHItAbp5zDkNgloraetw0DRafc1mic105v9EvnCSTU5dtZSe+kSC9I75133jmOaqFvRHnxQglSit91NURXXewqkq2yUlY9Be3pcrYt6TTrcmvbs4H6RdLb9pyK0Z7qM9pu8zS7SbKS9E5CiL+3iUBvnHKbD9WXtnwEO5T/2ETeNg0UnXKTEah1T2/0S0nv2rVrxyk7Nh8PT1OW01u2RGuf3C4r2++VUCO1AURXa5oivx5pDzjt0dc5xfJoqJaqXXb1uZqhDUdNl1trjewUF1G/SHqnmC7ZXUrSm92Qd/rAvXHKnaKQeed0ytEmQG/0q6w00n333Sc7d+4soq5Keuvm9E7afGqjyNggq1VhEGW2RfCRumQ3n9apfqKk16cDtVVCqe3ZQP0i6W17Ts1TeyS98zSa/X+W3jjl/kM1vySJ5PgAABGSSURBVBLSKUcb297olyW9GhlF7vqXX37ZiPQCMeTzIp+87OhRX0pw//79xWmMSGOwB6ugHZuOUDfS+/DDD49zgssizMzpjTan22y4N/rR5kOxrekQIOmdDi9ePRsCNDqz4TcXd5P0RhvG3uiXz/3W1JslS5YsOGK8bqQXiNXJ6QX5RDQXh8/YvvRkrKqcXlsTvCqnt+0SSm3PBupXnEgv5jRWEPTkzNDhRm2PZ9P2bAWUOqW+7IsjdKjNXPXUJQyrMCPpbTqjeF8TBCqdMhTj6quvLpZAvUFBZ7b8ke08VOy7iYC85/cINC0EPqnUE51y41n2ZyLyTxV395b0+kgt/i7L6cX3vnwfygHqx9dM1e/tXNVTHLWOqi92X6d6A9q11V/wt63eUFZaijm9jed1Wzf+lYj8VkS+DzTYWD/0YB68pE1KtWnrYWZtp+oQorK2m9r7OnJqJaNUJQxJeuuMCq9JgUDQ6FijooJMIk96HUlvnKFragQnjRtJb6Px+hMR+TccSiQivxaRvSWtNHbqjSTiTb1EIGP9+kFEfiEifzP6z5PfRvphD5NAWchJpDf0UgUfh42Yp0+fFn+IitpalNx76KGHxB5fXHafPe7Yblj25TBxyieIuo/0Whlxv81bRxk/bAh96aWXilQknPymefh79uxZIJstf4l2vv3229I0oNQlDEl6e2mashSq1Oh4o1JFem3tW1/gWw9ssNGgZcuWVdbXXLx4cRHR0QiTngxXFtXxI+YLltvjer1xCxkp36Y3RlrKpawIuUafENHCQQD4lJ2xju9tfdM6Rvn666+XU6dOyYcfflgUFMf9umyMKHzd+qb2+GN91oyd8qxK/3ci8qtRJOsbEdkhIn9vGm3k1GcVivf3C4GM9evB0QvhBShXLCLPO/LbSD9gL1EFBERQj5sPHWOvJLCsJB58gJa6s8cSY/aoTb799tvPO9zI32dJt/o3XdXwBznp4UK+coqm+0AOTUVauXLlOKXBpjeA9Go7mvZj+4P8eiCTnkDnny91CUOS3n7ZpJylKTU63qiESK/dyKIEbNeuXePTbmBo8PGnqalSWjKM75D3h1JGWGa1xgKn1mhtUXvGul2+9G/7/pQ1a9yqjJSdDH4Z1hojyGqfFUYHm2tAzvEBObbG0R6woeRYN/XoqW96ihzIrb9fIxr2KMzQfVX1TUl6W1X3JSJySkT+YNTqdyLy3yKyU0R+IyI/nTt3rtUO2djwEMiY9GKwvhKRy0aj9qMjv9810Q+/Cll2uJFGW2ELqzZK2sohdmZ5f2J9ga844lfS9JS0HTt2yLZt24rNmzZ4g78t6QWJ9QcMKfFW2T3ptXJrAMVWRYG/8P7LPl/qEoYkvcOzW/MqcSnpLUttAABeuf3mGD18Qckf3kI/+uijYikGBNXnNPn6mvZt3R8lqaS37kDY+0E+vZHQs9BDxy6in9ARj/57PbUO0Qf0s3r16oK4W6OD9vT58G81ZjB4Vja/673pb9aIhp5DsRygU/6liFT994cTfg/dO+19uP5qEVlkiC9gBfF9T0T+vIlTrzvHed0wEBjp1zCEjSPlz04//mekH3c20Q9/CmBVeoM/UdST1zKy6Qmn+i6NxvoTQiHPhg0bFiCHlbdXXnlFtm7dWkRdq0jvgQMHxPojbSi0kc2TZMVj/fr1Yx9Tl/TCT6l/iFnCkKQ3jmKx1ekRKCW9oaNFLelFV4hqKsFTUgxyaiOe9kjf0HG7ZfU1PVGzaRSatuAf116D3/RtH6TXGreQkfKll0K5sP57NU44mhgy6Ju9NbC67IXfkNelJN6nZEDu0PHLnhDr8cSKg9539OjRBUa0JumdfvZ0c8c/i8ifiggcZ9l/P1X8FuOed3Ey9AgK9I2o79+KyDMicrqJU+8GVvYaC4EBvlS2CUWSSG8ovcHvg6gqiWcf2hPpqvtC/tLvbQkdotIk0uv9ma9/PQ3pTVHCkKS3TZViW7MgEDXS65dxIKjNXfJGxi5DhYhaaMnGv9H7SG+ZkcAbeNWnaaS3jPTCCKlxRJ8otYO3/ypC6n/zpDcUpSiLRIeuhSyZO+VZ9GediPxORC4eEW0Q4CdF5Pio0UY5i7MIxHv7h0DG+vWXIrJ9lNLQy5zeqkivTbfz6XL2Pk+QfWqeRnF1HwmCNja9AZFjS9p1xRSR21B6Q8if6b11c3oR6cVHU0RilTAk6e2fXcpVoug5vZqnunz58nGuLsDWUjOaG+t3s1rihgiq3h/K6bWkV0skoR/Nja1rpGzOa5kxQ5swRlU5vSHS6w1LWcqHzZP2EQBLeiGHzem199nffH1T5vS2qur/LiJ/LCIfishfi8i/utajk16bfw890Q2UKkeorGAVClW5gPa+spUbu8u9qtrIpB33ftXGbkr1KxwqE67BB7/745FVVvwOXYFuHTx4sLBDsT8Zk97eV2+YlN6gG6vtRuGyQIVdsbPX+uoNuvJZ5u90XvujvzE/ffWGEOm1OvnCCy/IkSNHgtUb1Kfq/C/zIf7UQ1zbpIQhSW9sK8P26yKQpHpD2TK/VmgI1de0hkWL1KsClqU3WOMCxwsjgXIuWuLFG7eQkfLAlVVp0Dxgb6RwL1I+QqRXZdSdttqX3YjhS+NYub3jttUb7H1o18pt65uS9NZVjVrXPSci/ygi/xK4Oirp9VVWbJQH8kwilqEnnJb02gMtJpXHs3M+tCRtI2W6O19fMm2NYP+8Ou9BeC+99NLxXoLQ93q4gW2z1qhPeVHGpDdJnd4ph6PW5VUvbLUa6MFFfvWzByKVisDDKfo6MvMp11R1eucTAj5Vxk459uBHJb1+w6kngaGcQl8SUMkxwPj5558FL2W7d+9eUP+zDKiyYvuhzTf+oIgQIQ+d8lZGcEOkFy+Kdi8BZMe1V1555YKNtaEqNW1PCupXENGZ9CPmS8sQSa8N/ABxHwhpe1631R5Jb1tIsp06CEw8kU1zT+s0xmuGiQCdcrRxm8mpT5LKO/2ySK/Nk7e/l5ULRAk+RD1njfTqJs2m6Q260dSnKHg8qkgv0npw6ACi0JoShRUYrPxoNZlQPfJJuE/7O/UrDumddhx4fT8RIOnt57jMq1RRnfK8gjZvz0WnHG1Eo+lXGWHzebB4Kk0FmqZc4LSk1+b92XzGpqQXcvt84TICXEV6bfklzd+1G4M08hwzWqizivpF0hvNwsxBwyS9czCIA3qEaE55QBhkLyqdcrQpEE2/yohpKL0BZQXXrl1bbHz0G1PKygVOS3ptTq9NWwCqiDRrKT+bx4/TpkI5vX40pkl50L0A27dvl507d8rGjRtl3759xUZYe8IVSW+0OT9Nw9H0YxoheG23CJD0dot/br3T6OQ24iXPS9IbbRJE069QpBdPYisShE5rsk/sI7KzkN5QLdK6Ob2h6ixltVCrIr2o2oKTHM+cOTPewa4kXNMb8DcjvdHmfp2Go+lHnc55TT8QIOntxzjkIgWNTi4jXfGcJL3RJkFU/ZqU0+sjpJYkasUQ5PH68kmzkN6ySK8lmYp0VWUJT2Y11QE5uVpXVAmrJ/m+PjcqrISOpGVOb7R5X7fhqPpRV4i2rou1+S3VPG0Lh2nbIemdFjFePwsCnRodv7s89CCxjEld4NruP3TMc1152r6OpLdtRMftRdWvsuoNvk6vzYX1u7tD5QL1usOHD8vevXvH6QGWcOIJQycsah7xpJxelCE7duzYGCy729znJ5fVG54U6dVSh0qWy07nwtHhKG9YVsqvrVlB/QoiGVU/2hq/uu207Sdsv6kqjdR91javI+ltE022NQmBQRidmMZkEkD4PUb/KZZV6zwbrqFTrovU1NdF1a9UESDMf2wGW7cOB9DNzyeVDlK/5p/02hdA3cwJnbEHqdj68lqhBC96a9askYsuuqhIS6qqH59qvqbWcJLe1Ijn3V9UpzwJ2rL8Pz0BB/fak1+wAUeNiT2Uwe4Wt/3BQNgTZ2xOoD0Mwt9vf8OyKAwRNsPU6R8GC5E1lEpatWqV7NixQ7Zt2yb+IA4l0imiTJPGgKS3DkKNr4muXykiQPv375cVK1YIjtKel0/K1RaS3vknvT444o8Wtoe2nDx5cryJE6lFWEFBqcBNmzbJli1bxieo4R58dIUl5ZxNqeckvSnRZl/RnXIVxJ702rPO7alMMBJab1SNgC5Z+tObtD+b24fv9KQ0OG97fK+9H8uh1ugocba7vqv6x9nqmzdvLpaEQRAs0fYbdFJF6epMcTrlOig1uqZT/WokMW9qHQHqV36k12/ctP4IGyy1ljWQqbPZtG+BkjaVhKS3TTTZ1iQEOnXKnvTaEkbWSFjSizdoW3A/tCEmtKEGS04gszA0MEw2dQG/lZ3FXnXKlO0Hstm2y3ac2wHpy3IVnfIkNWn8e6f61Vhq3tgqAtSvPEmvz0vXDZUo4YePVlkJrULq9ZpvnmJVp9WJX7Mxkt6aQPGyVhDo1ClXlTcKkV5PWkM7zW3bJ06cGL9Z25wpRVA30Bw9elQOHTokKHdkN7Z4YmyJre0fpNeTZmv4bE4X+ibpbWUO97mRTvWrz8DkJBtJb36k1/upOpFeW2rQRoD1e5LenKwGnzUWAp065Sakt26k1xoN/FuPU7bGx+/YDv02TaS3LFKM/kOHCfThmGc65VjqJZ3qV7SnYsNTIUD9ypv0Llq0qEivwwcBlVBOrz8xkDm9U6kZLyYCtRDo1Ck3Ib14KuTkTsrpxXW6KW3JkiXjdAZ/HCsMC4gq/u83H2hJJGuMqvpHTq8lvXYzHXN6a83HebuoU/2aNzCH+jwkvXmQXvUteFpsUob9xyZmrCSiLN6ePXtEa1Zr9QZspMZ/Z8+eLdId9Hu04dMb+rIy2LYeMr2hbUTZXhUCnTrluqRX621q9LZO9QZcq+0vXbp0wSlVtkKDrQ2Ke2z6gxqduv37SLGvY2rTG/q0VEWnHM1IdKpf0Z6KDU+FAPUrD9I71aQYXRw6Ytu31Sd/0eQ5q+4h6W0bUbbXW9Kb89D06a2dTjnaTCTpjQbtcBqmfpH0WgT8vhJ7gEwZUn2q9BND60h6Y6DKNkMI0Cl3MDf6Vm+RTjnaJKB+RYN2OA1Tv0h6hzNb00tK0pse85x7pFPOefRHz06nHG0SUL+iQTuchqlfJL3Dma3pJSXpTY95zj3SKec8+iS9sUef+hUb4QG0T9JL0juAadqZiCS9nUGfZcd0ylkO+8KHplOONgmoX9GgHU7D1C+S3uHM1vSSkvSmxzznHumUcx59Rnpjjz71KzbCA2ifpJekdwDTtDMRSXo7gz7LjumUsxx2RnoTDTv1KxHQfe6GpJekt8/zs2vZSHq7HoG8+qdTzmu8S5+WTjnaJKB+RYN2OA1Tv0h6hzNb00tK0pse85x7pFPOefSZ3hB79KlfsREeQPskvSS9A5imnYlI0tsZ9Fl2TKec5bAzvSHRsFO/EgHd525Iekl6+zw/u5aNpLfrEcirfzrlvMab6Q1px5v6lRbvXvZG0kvS28uJ2ROhSHp7MhCZiEGnnMlAVz0mnXK0SUD9igbtcBqmfpH0Dme2ppeUpDc95jn3SKec8+iPnp1OOdokoH5Fg3Y4DVO/SHqHM1vTS0rSmx7znHukU8559El6Y48+9Ss2wgNon6SXpHcA07QzEUl6O4M+y47plLMc9oUPTaccbRJQv6JBO5yGqV8kvcOZreklJelNj3nOPdIp5zz6jPTGHn3qV2yEB9A+SS9J7wCmaWcikvR2Bn2WHdMpZznsjPQmGnbqVyKg+9wNSS9Jb5/nZ9eykfR2PQJ59U+nnNd4lz4tnXK0SUD9igbtcBqmfpH0Dme2ppeUpDc95jn3SKec8+gzvSH26FO/YiM8gPZJekl6BzBNOxORpLcz6LPsmE45y2FnekOiYad+JQK6z92Q9JL09nl+di0bSW/XI5BX/3TKeY030xvSjve5tN2xtx4jcEGPZetKNOpHV8j3rF8qR88GZI7FodGZ48Gd8tFod6YEjJcTASJABIjA7Aj8P/D9B7ZYfgXiAAAAAElFTkSuQmCC"}}},{"cell_type":"markdown","source":"# Importing Necessary Libraries","metadata":{}},{"cell_type":"code","source":"!pip install transformers \n# This command installs the 'transformers' library from Hugging Face, which provides a collection of pre-trained models and utilities for Natural Language Processing (NLP) tasks","metadata":{"execution":{"iopub.status.busy":"2023-12-27T22:31:56.036366Z","iopub.execute_input":"2023-12-27T22:31:56.037466Z","iopub.status.idle":"2023-12-27T22:32:07.874476Z","shell.execute_reply.started":"2023-12-27T22:31:56.037421Z","shell.execute_reply":"2023-12-27T22:32:07.873158Z"},"trusted":true},"execution_count":191,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n# PyTorch, a library for tensor operations and building neural networks.\n\nfrom transformers import BertTokenizerFast, BertForTokenClassification\n# Provides BERT tokenizer and token classification model for NER tasks.\n\nfrom torch.utils.data import DataLoader\n# Tool for batching and iterating over a dataset efficiently.\n\nfrom transformers import AdamW\n# Adam optimizer with weight decay, for training deep learning models.\n\nfrom tqdm import tqdm\n# Adds progress bars to loops for monitoring training progress.\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T22:32:11.590237Z","iopub.execute_input":"2023-12-27T22:32:11.591167Z","iopub.status.idle":"2023-12-27T22:32:11.596985Z","shell.execute_reply.started":"2023-12-27T22:32:11.591133Z","shell.execute_reply":"2023-12-27T22:32:11.596029Z"},"trusted":true},"execution_count":192,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef load_dataset(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = [line.strip() for line in file if line.strip()]\n    sentences, words, tags = [], [], []\n    for line in lines:\n        if line.startswith(\"-DOCSTART-\"):\n            sentences.append([])\n        else:\n            word_info = line.split()\n            if len(word_info) == 4:\n                word, pos, chunk, tag = word_info\n                words.append(word)\n                tags.append(tag)\n                sentences[-1].append((word, tag))\n    return sentences, words, tags\n\ndef visualize_data_distribution(tags):\n    tag_counts = pd.Series(tags).value_counts()\n    plt.figure(figsize=(12,5))\n    tag_counts.plot(kind='bar')\n    plt.title('Tag Distribution in the Dataset')\n    plt.xlabel('Tags')\n    plt.ylabel('Frequency')\n    plt.xticks(rotation=90)\n    plt.show()\n\n# Paths to the data files\ntrain_path = '../input/conll003-englishversion/train.txt'\nvalid_path = '../input/conll003-englishversion/valid.txt'\ntest_path = '../input/conll003-englishversion/test.txt'\n\n# Loading the datasets\ntrain_sentences, train_words, train_tags = load_dataset(train_path)\nvalid_sentences, valid_words, valid_tags = load_dataset(valid_path)\ntest_sentences, test_words, test_tags = load_dataset(test_path)\n\n# Visualizing the tag distribution in the training set\nvisualize_data_distribution(train_tags)\n\n# Printing out some basic dataset information\nprint(f'Number of training sentences: {len(train_sentences)}')\nprint(f'Number of unique words in training set: {len(set(train_words))}')\nprint(f'Number of unique tags in training set: {len(set(train_tags))}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:14:49.068741Z","iopub.execute_input":"2023-12-27T23:14:49.069487Z","iopub.status.idle":"2023-12-27T23:14:49.887993Z","shell.execute_reply.started":"2023-12-27T23:14:49.069457Z","shell.execute_reply":"2023-12-27T23:14:49.887042Z"},"trusted":true},"execution_count":226,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x500 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAABAgAAAH5CAYAAAAMbwwBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABoCElEQVR4nO3deVhWdf7/8dfNjiigIiCJYrmilLmEmJomiYr2xawRtdGMdEo0FZfUipyyVBrX0bSmkjZLqXRcEnezlMx9F7VcU1BTIEgR5fz+6McZ78BURG6Q5+O67ms65/O+z3mf+4aR+3Wf8zkWwzAMAQAAAACAMs3O1g0AAAAAAADbIyAAAAAAAAAEBAAAAAAAgIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAIBic/ToUVksFsXHx9/xfcXHx8tisejo0aPmuoCAAHXu3PmO71uS1q1bJ4vFonXr1hXL/q7Vpk0btWnTplj2ZbFYNHDgwGLZFwAAdxoBAQCgVLFYLDf1KI4Pptfuz8HBQZUqVVKTJk00ePBg7du3r8j288477xRLqFAYJbm3orJx40aNHTtWaWlpNushICDA/Fmzs7OTp6engoKC1L9/f23atOm2tv3WW29p4cKFRdPobdq3b5/Gjh1rFWwBAIqPxTAMw9ZNAABwsz799FOr5Y8//lgrV67UJ598YrX+sccek4+Pzx3txWKx6LHHHlPv3r1lGIbS09O1c+dOJSQkKCsrSxMnTlRMTIxZbxiGsrOz5ejoKHt7+5veT8OGDeXl5XVLocfVq1eVk5MjZ2dnWSwWSX98yGzYsKGWLFly09spbG+5ubm6fPmynJycZGdXvN9HXL58WZLk5ORUJNv717/+pREjRujIkSMKCAiwGrNYLIqOjtaMGTOKZF/XExAQoIoVK2rYsGGSpN9++0379+9XQkKCUlJSNHToUE2ePLlQ2y5fvryefPLJEhH0fPnll3rqqae0du3aYjsLBADwPw62bgAAgFvx9NNPWy3/8MMPWrlyZb71xaVOnTr59j1hwgR16dJFw4YNU7169dSpUydJf3yYdHFxuaP9ZGVlyc3NTfb29rcUQhQ1Ozu7O36s11NUwUBJc8899+T7WZs4caJ69uypKVOmqHbt2nrhhRds1B0A4G7AJQYAgLvOnDlz9Oijj8rb21vOzs4KDAzUrFmz8tXl5uZq7Nix8vPzU7ly5dS2bVvt27dPAQEBeuaZZwq9/8qVK+uLL76Qg4OD3nzzTXN9QXMQpKSkqG/fvqpWrZqcnZ1VtWpV/d///Z95inVAQID27t2rb7/91jzFPO+b1bx5Br799lsNGDBA3t7eqlatmtVYQadqr1ixQo0aNZKLi4sCAwP19ddfW42PHTvWPOvgWn/e5l/1dr05CBISEtSkSRO5urrKy8tLTz/9tH755RermmeeeUbly5fXL7/8ooiICJUvX15VqlTR8OHDdfXq1Ru8+vnnIMjrZf78+XrzzTdVrVo1ubi4qF27djp8+PBfbmvs2LEaMWKEJKlmzZrmcf75dV24cKEaNmwoZ2dnNWjQQImJifm29csvv+jZZ5+Vj4+PWffhhx/e8Hj+iqurqz755BNVqlRJb775pq49MfRf//qXWrRoocqVK8vV1VVNmjTRl19+afV8i8WirKwsffTRR+ax5f3sHzt2TAMGDFDdunXl6uqqypUr66mnnsp37Dk5OfrnP/+p2rVry8XFRZUrV1bLli21cuVKq7oDBw7oySefVKVKleTi4qKmTZtq0aJF5nh8fLyeeuopSVLbtm2L9XIhAMAfOIMAAHDXmTVrlho0aKDHH39cDg4OWrx4sQYMGKDc3FxFR0ebdaNHj1ZcXJy6dOmisLAw7dy5U2FhYbp06dJt91C9enU98sgjWrt2rTIyMuTu7l5gXbdu3bR3714NGjRIAQEBOnPmjFauXKnjx48rICBAU6dO1aBBg1S+fHm9/PLLkpTv0okBAwaoSpUqio2NVVZW1l/2dejQIXXv3l3PP/+8+vTpozlz5uipp55SYmKiHnvssVs6xpvp7Vrx8fHq27evmjVrpvHjxys1NVXTpk3Thg0btH37dnl6epq1V69eVVhYmIKDg/Wvf/1Lq1at0qRJk3TfffcV+lvyCRMmyM7OTsOHD1d6erri4uLUq1evv7yG/4knntDBgwf1+eefa8qUKfLy8pIkValSxaz5/vvv9fXXX2vAgAGqUKGCpk+frm7duun48eOqXLmyJCk1NVXNmzc3JzWsUqWKli1bpqioKGVkZGjIkCGFOibpj0sEunbtqg8++ED79u1TgwYNJEnTpk3T448/rl69euny5cv64osv9NRTT2nJkiUKDw+XJH3yySd67rnn9NBDD6l///6SpPvuu0+StHnzZm3cuFGRkZGqVq2ajh49qlmzZqlNmzbat2+fypUrJ+mPEGX8+PHmdjIyMrRlyxZt27bN/Jnau3evHn74Yd1zzz0aNWqU3NzcNH/+fEVEROirr75S165d1bp1a7344ouaPn26xowZo/r160uS+b8AgGJgAABQikVHRxt//ufs999/z1cXFhZm3HvvveZySkqK4eDgYERERFjVjR071pBk9OnT54b7lmRER0dfd3zw4MGGJGPnzp2GYRjGkSNHDEnGnDlzDMMwjAsXLhiSjLfffvsv99OgQQPjkUceybd+zpw5hiSjZcuWxpUrVwocO3LkiLmuRo0ahiTjq6++Mtelp6cbVatWNR588EFz3WuvvZbvNb3eNq/X29q1aw1Jxtq1aw3DMIzLly8b3t7eRsOGDY2LFy+adUuWLDEkGbGxsea6Pn36GJKM119/3WqbDz74oNGkSZN8+/qzRx55xKqnvF7q169vZGdnm+unTZtmSDJ27979l9t7++238x13HkmGk5OTcfjwYXPdzp07DUnGv//9b3NdVFSUUbVqVePcuXNWz4+MjDQ8PDwK/Jm9Vo0aNYzw8PDrjk+ZMsWQZPz3v/811/15m5cvXzYaNmxoPProo1br3dzcCvx5L6inpKQkQ5Lx8ccfm+seeOCBv+zNMAyjXbt2RlBQkHHp0iVzXW5urtGiRQujdu3a5rqEhASrnxsAQPHiEgMAwF3H1dXV/O/09HSdO3dOjzzyiH7++Welp6dLklavXq0rV65owIABVs8dNGhQkfVRvnx5SX9MKHe9Pp2cnLRu3TpduHCh0Pvp16/fTc834Ofnp65du5rL7u7u6t27t7Zv366UlJRC93AjW7Zs0ZkzZzRgwACruQnCw8NVr149LV26NN9znn/+eavlVq1a6eeffy50D3379rWan6BVq1aSdFvblKTQ0FDzW3dJuv/+++Xu7m5u1zAMffXVV+rSpYsMw9C5c+fMR1hYmNLT07Vt27bb6qGgn7Vrfw8uXLig9PR0tWrV6qb3de3zc3Jy9Ouvv6pWrVry9PS02oanp6f27t2rQ4cOFbid8+fPa82aNfrb3/6m3377zTz2X3/9VWFhYTp06FC+y0wAALZBQAAAuOts2LBBoaGhcnNzk6enp6pUqaIxY8ZIkhkQHDt2TJJUq1Ytq+dWqlRJFStWLJI+MjMzJUkVKlQocNzZ2VkTJ07UsmXL5OPjo9atWysuLu6WP6jXrFnzpmtr1aqVb36BOnXqSNIdvbVc3utdt27dfGP16tUzx/O4uLhYncYvSRUrVrytIKV69er5tifptrZZ0Hbztp233bNnzyotLU3vvfeeqlSpYvXo27evJOnMmTO31UNBP2tLlixR8+bN5eLiokqVKqlKlSqaNWuW+TtwIxcvXlRsbKz8/f3l7OwsLy8vValSRWlpaVbbeP3115WWlqY6deooKChII0aM0K5du8zxw4cPyzAMvfrqq/mO/7XXXiuS4wcAFA3mIAAA3FV++ukntWvXTvXq1dPkyZPl7+8vJycnffPNN5oyZYpyc3OLrZc9e/bI3t7+Lz/ADxkyRF26dNHChQu1fPlyvfrqqxo/frzWrFmjBx988Kb2c+03vUWhoAkKJd3UBIFF5U7cgeF62zRu847PN9pu3s/c008/rT59+hRYe//9999WD3v27JH0v8Dru+++0+OPP67WrVvrnXfeUdWqVeXo6Kg5c+Zo7ty5N7XNQYMGac6cORoyZIhCQkLk4eEhi8WiyMhIq9+j1q1b66efftJ///tfrVixQu+//76mTJmi2bNn67nnnjNrhw8frrCwsAL39eegDgBgGwQEAIC7yuLFi5Wdna1FixZZfbO7du1aq7oaNWpI+uPbzWs/wP/666+3/Y2yJB0/flzffvutQkJCrnsGQZ777rtPw4YN07Bhw3To0CE1atRIkyZN0qeffirp+h/YCyPv29xrt3nw4EFJf9yVQPrfN+tpaWlWEwf++Vv+W+kt7/VOTk7Wo48+ajWWnJxsjpdEt/v6V6lSRRUqVNDVq1cVGhpaRF39T2ZmphYsWCB/f39zQr+vvvpKLi4uWr58uZydnc3aOXPm5Hv+9Y7vyy+/VJ8+fTRp0iRz3aVLl5SWlpavtlKlSurbt6/69u2rzMxMtW7dWmPHjtVzzz2ne++9V5Lk6Oh4w+Mvyp91AMCt4xIDAMBdJe/b3Gu/FU5PT8/3wahdu3ZycHDId/vDGTNm3HYP58+fV48ePXT16lVzdv+C/P777/numHDfffepQoUKys7ONte5ubkV+KGsME6dOqUFCxaYyxkZGfr444/VqFEj+fr6mj1I0vr16826vFvh/dnN9ta0aVN5e3tr9uzZVse2bNky7d+/35xVvyRyc3OTpEK/B/b29urWrZu++uor85v+a509e7bQvV28eFF///vfdf78eb388svmB2x7e3tZLBarsz6OHj2qhQsX5tvG9d5De3v7fGdX/Pvf/853Jsmvv/5qtVy+fHnVqlXLfJ+9vb3Vpk0bvfvuuzp9+nS+/Vx7/Lf7WgMAbg9nEAAA7irt27eXk5OTunTpon/84x/KzMzUf/7zH3l7e1t9OPHx8dHgwYM1adIkPf744+rQoYN27typZcuWycvL66a/yTx48KA+/fRTGYahjIwM7dy5UwkJCcrMzNTkyZPVoUOHv3xuu3bt9Le//U2BgYFycHDQggULlJqaqsjISLOuSZMmmjVrlsaNG6datWrJ29s737fwN6tOnTqKiorS5s2b5ePjow8//FCpqalWAUr79u1VvXp1RUVFacSIEbK3t9eHH36oKlWq6Pjx41bbu9neHB0dNXHiRPXt21ePPPKIevToYd7mMCAgQEOHDi3U8RSHJk2aSJJefvllRUZGytHRUV26dDE/zN6MCRMmaO3atQoODla/fv0UGBio8+fPa9u2bVq1apXOnz9/w2388ssv5lklmZmZ2rdvnxISEpSSkqJhw4bpH//4h1kbHh5u/vz17NlTZ86c0cyZM1WrVi2r+QHyjm/VqlWaPHmy/Pz8VLNmTQUHB6tz58765JNP5OHhocDAQCUlJWnVqlXmrRvzBAYGqk2bNmrSpIkqVaqkLVu26Msvv9TAgQPNmpkzZ6ply5YKCgpSv379dO+99yo1NVVJSUk6efKkdu7cKUlq1KiR7O3tNXHiRKWnp8vZ2VmPPvqovL29b/q1BgDcBpvdPwEAgCJQ0G0OFy1aZNx///2Gi4uLERAQYEycONH48MMP892q7sqVK8arr75q+Pr6Gq6ursajjz5q7N+/36hcubLx/PPP33DfksyHnZ2d4enpaTz44IPG4MGDjb179+ar//NtDs+dO2dER0cb9erVM9zc3AwPDw8jODjYmD9/vtXzUlJSjPDwcKNChQqGJPMWfnm3Hdy8eXO+fV3vNofh4eHG8uXLjfvvv99wdnY26tWrZyQkJOR7/tatW43g4GDDycnJqF69ujF58uQCt3m93v58m8M88+bNMx588EHD2dnZqFSpktGrVy/j5MmTVjV9+vQx3Nzc8vV0vdsv/tn1bnP45+P88/vxV9544w3jnnvuMezs7KxeA13nVpc1atTId+vA1NRUIzo62vD39zccHR0NX19fo127dsZ77713w/3n3aJSkmGxWAx3d3ejQYMGRr9+/YxNmzYV+JwPPvjAqF27tvk+z5kzp8DX8MCBA0br1q0NV1dXq1t8Xrhwwejbt6/h5eVllC9f3ggLCzMOHDiQ79jGjRtnPPTQQ4anp6fh6upq1KtXz3jzzTeNy5cvW+3np59+Mnr37m34+voajo6Oxj333GN07tzZ+PLLL63q/vOf/xj33nuvYW9vzy0PAaCYWQzjNmfmAQDgLpKWlqaKFStq3Lhxf3l5AAAAwN2GOQgAAGXWxYsX862bOnWqJKlNmzbF2wwAAICNMQcBAKDMmjdvnuLj49WpUyeVL19e33//vT7//HO1b99eDz/8sK3bAwAAKFYEBACAMuv++++Xg4OD4uLilJGRYU5cOG7cOFu3BgAAUOyYgwAAAAAAADAHAQAAAAAAICAAAAAAAABiDoJilZubq1OnTqlChQqyWCy2bgcAAAAAcJczDEO//fab/Pz8ZGf31+cIEBAUo1OnTsnf39/WbQAAAAAAypgTJ06oWrVqf1lDQFCMKlSoIOmPN8bd3d3G3QAAAAAA7nYZGRny9/c3P4/+FQKCYpR3WYG7uzsBAQAAAACg2NzMZe5MUggAAAAAAAgIAAAAAAAAAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAAJDkYOsGcGcEjFpq6xaK1NEJ4bZuAQAAAADuapxBAAAAAAAACAgAAAAAAAABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAACQjQOC9evXq0uXLvLz85PFYtHChQvz1ezfv1+PP/64PDw85ObmpmbNmun48ePm+KVLlxQdHa3KlSurfPny6tatm1JTU622cfz4cYWHh6tcuXLy9vbWiBEjdOXKFauadevWqXHjxnJ2dlatWrUUHx+fr5eZM2cqICBALi4uCg4O1o8//lgkrwMAAAAAALZm04AgKytLDzzwgGbOnFng+E8//aSWLVuqXr16WrdunXbt2qVXX31VLi4uZs3QoUO1ePFiJSQk6Ntvv9WpU6f0xBNPmONXr15VeHi4Ll++rI0bN+qjjz5SfHy8YmNjzZojR44oPDxcbdu21Y4dOzRkyBA999xzWr58uVkzb948xcTE6LXXXtO2bdv0wAMPKCwsTGfOnLkDrwwAAAAAAMXLYhiGYesmJMlisWjBggWKiIgw10VGRsrR0VGffPJJgc9JT09XlSpVNHfuXD355JOSpAMHDqh+/fpKSkpS8+bNtWzZMnXu3FmnTp2Sj4+PJGn27Nl66aWXdPbsWTk5Oemll17S0qVLtWfPHqt9p6WlKTExUZIUHBysZs2aacaMGZKk3Nxc+fv7a9CgQRo1atRNHWNGRoY8PDyUnp4ud3f3W36NbkXAqKV3dPvF7eiEcFu3AAAAAAClzq18Di2xcxDk5uZq6dKlqlOnjsLCwuTt7a3g4GCryxC2bt2qnJwchYaGmuvq1aun6tWrKykpSZKUlJSkoKAgMxyQpLCwMGVkZGjv3r1mzbXbyKvJ28bly5e1detWqxo7OzuFhoaaNQXJzs5WRkaG1QMAAAAAgJKoxAYEZ86cUWZmpiZMmKAOHTpoxYoV6tq1q5544gl9++23kqSUlBQ5OTnJ09PT6rk+Pj5KSUkxa64NB/LG88b+qiYjI0MXL17UuXPndPXq1QJr8rZRkPHjx8vDw8N8+Pv73/oLAQAAAABAMSixAUFubq4k6f/+7/80dOhQNWrUSKNGjVLnzp01e/ZsG3d3c0aPHq309HTzceLECVu3BAAAAABAgUpsQODl5SUHBwcFBgZara9fv755FwNfX19dvnxZaWlpVjWpqany9fU1a/58V4O85RvVuLu7y9XVVV5eXrK3ty+wJm8bBXF2dpa7u7vVAwAAAACAkqjEBgROTk5q1qyZkpOTrdYfPHhQNWrUkCQ1adJEjo6OWr16tTmenJys48ePKyQkRJIUEhKi3bt3W91tYOXKlXJ3dzfDh5CQEKtt5NXkbcPJyUlNmjSxqsnNzdXq1avNGgAAAAAASjMHW+48MzNThw8fNpePHDmiHTt2qFKlSqpevbpGjBih7t27q3Xr1mrbtq0SExO1ePFirVu3TpLk4eGhqKgoxcTEqFKlSnJ3d9egQYMUEhKi5s2bS5Lat2+vwMBA/f3vf1dcXJxSUlL0yiuvKDo6Ws7OzpKk559/XjNmzNDIkSP17LPPas2aNZo/f76WLv3fnQBiYmLUp08fNW3aVA899JCmTp2qrKws9e3bt/heMAAAAAAA7hCbBgRbtmxR27ZtzeWYmBhJUp8+fRQfH6+uXbtq9uzZGj9+vF588UXVrVtXX331lVq2bGk+Z8qUKbKzs1O3bt2UnZ2tsLAwvfPOO+a4vb29lixZohdeeEEhISFyc3NTnz599Prrr5s1NWvW1NKlSzV06FBNmzZN1apV0/vvv6+wsDCzpnv37jp79qxiY2OVkpKiRo0aKTExMd/EhQAAAAAAlEYWwzAMWzdRVtzK/SdvV8CopTcuKkWOTgi3dQsAAAAAUOrcyufQEjsHAQAAAAAAKD4EBAAAAAAAgIAAAAAAAAAQEAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAA2TggWL9+vbp06SI/Pz9ZLBYtXLjwurXPP/+8LBaLpk6darX+/Pnz6tWrl9zd3eXp6amoqChlZmZa1ezatUutWrWSi4uL/P39FRcXl2/7CQkJqlevnlxcXBQUFKRvvvnGatwwDMXGxqpq1apydXVVaGioDh06VOhjBwAAAACgJLFpQJCVlaUHHnhAM2fO/Mu6BQsW6IcffpCfn1++sV69emnv3r1auXKllixZovXr16t///7meEZGhtq3b68aNWpo69atevvttzV27Fi99957Zs3GjRvVo0cPRUVFafv27YqIiFBERIT27Nlj1sTFxWn69OmaPXu2Nm3aJDc3N4WFhenSpUtF8EoAAAAAAGBbFsMwDFs3IUkWi0ULFixQRESE1fpffvlFwcHBWr58ucLDwzVkyBANGTJEkrR//34FBgZq8+bNatq0qSQpMTFRnTp10smTJ+Xn56dZs2bp5ZdfVkpKipycnCRJo0aN0sKFC3XgwAFJUvfu3ZWVlaUlS5aY+23evLkaNWqk2bNnyzAM+fn5adiwYRo+fLgkKT09XT4+PoqPj1dkZORNHWNGRoY8PDyUnp4ud3f323m5bihg1NI7uv3idnRCuK1bAAAAAIBS51Y+h5boOQhyc3P197//XSNGjFCDBg3yjSclJcnT09MMByQpNDRUdnZ22rRpk1nTunVrMxyQpLCwMCUnJ+vChQtmTWhoqNW2w8LClJSUJEk6cuSIUlJSrGo8PDwUHBxs1hQkOztbGRkZVg8AAAAAAEqiEh0QTJw4UQ4ODnrxxRcLHE9JSZG3t7fVOgcHB1WqVEkpKSlmjY+Pj1VN3vKNaq4dv/Z5BdUUZPz48fLw8DAf/v7+f3m8AAAAAADYSokNCLZu3app06YpPj5eFovF1u0UyujRo5Wenm4+Tpw4YeuWAAAAAAAoUIkNCL777judOXNG1atXl4ODgxwcHHTs2DENGzZMAQEBkiRfX1+dOXPG6nlXrlzR+fPn5evra9akpqZa1eQt36jm2vFrn1dQTUGcnZ3l7u5u9QAAAAAAoCQqsQHB3//+d+3atUs7duwwH35+fhoxYoSWL18uSQoJCVFaWpq2bt1qPm/NmjXKzc1VcHCwWbN+/Xrl5OSYNStXrlTdunVVsWJFs2b16tVW+1+5cqVCQkIkSTVr1pSvr69VTUZGhjZt2mTWAAAAAABQmjnYcueZmZk6fPiwuXzkyBHt2LFDlSpVUvXq1VW5cmWrekdHR/n6+qpu3bqSpPr166tDhw7q16+fZs+erZycHA0cOFCRkZHmLRF79uypf/7zn4qKitJLL72kPXv2aNq0aZoyZYq53cGDB+uRRx7RpEmTFB4eri+++EJbtmwxb4VosVg0ZMgQjRs3TrVr11bNmjX16quvys/PL99dFwAAAAAAKI1sGhBs2bJFbdu2NZdjYmIkSX369FF8fPxNbeOzzz7TwIED1a5dO9nZ2albt26aPn26Oe7h4aEVK1YoOjpaTZo0kZeXl2JjY9W/f3+zpkWLFpo7d65eeeUVjRkzRrVr19bChQvVsGFDs2bkyJHKyspS//79lZaWppYtWyoxMVEuLi63+SoAAAAAAGB7FsMwDFs3UVbcyv0nb1fAqKV3dPvF7eiEcFu3AAAAAAClzq18Di2xcxAAAAAAAIDiQ0AAAAAAAAAICAAAAAAAAAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAAJCNA4L169erS5cu8vPzk8Vi0cKFC82xnJwcvfTSSwoKCpKbm5v8/PzUu3dvnTp1ymob58+fV69eveTu7i5PT09FRUUpMzPTqmbXrl1q1aqVXFxc5O/vr7i4uHy9JCQkqF69enJxcVFQUJC++eYbq3HDMBQbG6uqVavK1dVVoaGhOnToUNG9GAAAAAAA2JBNA4KsrCw98MADmjlzZr6x33//Xdu2bdOrr76qbdu26euvv1ZycrIef/xxq7pevXpp7969WrlypZYsWaL169erf//+5nhGRobat2+vGjVqaOvWrXr77bc1duxYvffee2bNxo0b1aNHD0VFRWn79u2KiIhQRESE9uzZY9bExcVp+vTpmj17tjZt2iQ3NzeFhYXp0qVLd+CVAQAAAACgeFkMwzBs3YQkWSwWLViwQBEREdet2bx5sx566CEdO3ZM1atX1/79+xUYGKjNmzeradOmkqTExER16tRJJ0+elJ+fn2bNmqWXX35ZKSkpcnJykiSNGjVKCxcu1IEDByRJ3bt3V1ZWlpYsWWLuq3nz5mrUqJFmz54twzDk5+enYcOGafjw4ZKk9PR0+fj4KD4+XpGRkTd1jBkZGfLw8FB6errc3d0L8zLdtIBRS+/o9ovb0Qnhtm4BAAAAAEqdW/kcWqrmIEhPT5fFYpGnp6ckKSkpSZ6enmY4IEmhoaGys7PTpk2bzJrWrVub4YAkhYWFKTk5WRcuXDBrQkNDrfYVFhampKQkSdKRI0eUkpJiVePh4aHg4GCzpiDZ2dnKyMiwegAAAAAAUBKVmoDg0qVLeumll9SjRw8z9UhJSZG3t7dVnYODgypVqqSUlBSzxsfHx6omb/lGNdeOX/u8gmoKMn78eHl4eJgPf3//WzpmAAAAAACKS6kICHJycvS3v/1NhmFo1qxZtm7npo0ePVrp6enm48SJE7ZuCQAAAACAAjnYuoEbyQsHjh07pjVr1lhdM+Hr66szZ85Y1V+5ckXnz5+Xr6+vWZOammpVk7d8o5prx/PWVa1a1aqmUaNG1+3d2dlZzs7Ot3K4AAAAAADYRIk+gyAvHDh06JBWrVqlypUrW42HhIQoLS1NW7duNdetWbNGubm5Cg4ONmvWr1+vnJwcs2blypWqW7euKlasaNasXr3aatsrV65USEiIJKlmzZry9fW1qsnIyNCmTZvMGgAAAAAASjObBgSZmZnasWOHduzYIemPyQB37Nih48ePKycnR08++aS2bNmizz77TFevXlVKSopSUlJ0+fJlSVL9+vXVoUMH9evXTz/++KM2bNiggQMHKjIyUn5+fpKknj17ysnJSVFRUdq7d6/mzZunadOmKSYmxuxj8ODBSkxM1KRJk3TgwAGNHTtWW7Zs0cCBAyX9cYeFIUOGaNy4cVq0aJF2796t3r17y8/P7y/vugAAAAAAQGlh09scrlu3Tm3bts23vk+fPho7dqxq1qxZ4PPWrl2rNm3aSJLOnz+vgQMHavHixbKzs1O3bt00ffp0lS9f3qzftWuXoqOjtXnzZnl5eWnQoEF66aWXrLaZkJCgV155RUePHlXt2rUVFxenTp06meOGYei1117Te++9p7S0NLVs2VLvvPOO6tSpc9PHy20OC4/bHAIAAADArbuVz6E2DQjKGgKCwiMgAAAAAIBbdyufQ0v0HAQAAAAAAKB4EBAAAAAAAAACAgAAAAAAQEAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABUyIDg559/Luo+AAAAAACADRUqIKhVq5batm2rTz/9VJcuXSrqngAAAAAAQDErVECwbds23X///YqJiZGvr6/+8Y9/6Mcffyzq3gAAAAAAQDEpVEDQqFEjTZs2TadOndKHH36o06dPq2XLlmrYsKEmT56ss2fPFnWfAAAAAADgDrqtSQodHBz0xBNPKCEhQRMnTtThw4c1fPhw+fv7q3fv3jp9+nRR9QkAAAAAAO6g2woItmzZogEDBqhq1aqaPHmyhg8frp9++kkrV67UqVOn9H//939/+fz169erS5cu8vPzk8Vi0cKFC63GDcNQbGysqlatKldXV4WGhurQoUNWNefPn1evXr3k7u4uT09PRUVFKTMz06pm165datWqlVxcXOTv76+4uLh8vSQkJKhevXpycXFRUFCQvvnmm1vuBQAAAACA0qpQAcHkyZMVFBSkFi1a6NSpU/r444917NgxjRs3TjVr1lSrVq0UHx+vbdu2/eV2srKy9MADD2jmzJkFjsfFxWn69OmaPXu2Nm3aJDc3N4WFhVlNjNirVy/t3btXK1eu1JIlS7R+/Xr179/fHM/IyFD79u1Vo0YNbd26VW+//bbGjh2r9957z6zZuHGjevTooaioKG3fvl0RERGKiIjQnj17bqkXAAAAAABKK4thGMatPql27dp69tln9cwzz6hq1aoF1ly+fFmff/65+vTpc3ONWCxasGCBIiIiJP3xjb2fn5+GDRum4cOHS5LS09Pl4+Oj+Ph4RUZGav/+/QoMDNTmzZvVtGlTSVJiYqI6deqkkydPys/PT7NmzdLLL7+slJQUOTk5SZJGjRqlhQsX6sCBA5Kk7t27KysrS0uWLDH7ad68uRo1aqTZs2ffVC8Fyc7OVnZ2trmckZEhf39/paeny93d/aZel8IKGLX0jm6/uB2dEG7rFgAAAACg1MnIyJCHh8dNfQ4t1BkEhw4d0ujRo68bDkiSk5PTTYcDBTly5IhSUlIUGhpqrvPw8FBwcLCSkpIkSUlJSfL09DTDAUkKDQ2VnZ2dNm3aZNa0bt3aDAckKSwsTMnJybpw4YJZc+1+8mry9nMzvRRk/Pjx8vDwMB/+/v6FfTkAAAAAALijChUQzJkzRwkJCfnWJyQk6KOPPrrtpiQpJSVFkuTj42O13sfHxxxLSUmRt7e31biDg4MqVapkVVPQNq7dx/Vqrh2/US8FGT16tNLT083HiRMnbnDUAAAAAADYRqECgvHjx8vLyyvfem9vb7311lu33dTdwtnZWe7u7lYPAAAAAABKokIFBMePH1fNmjXzra9Ro4aOHz9+201Jkq+vryQpNTXVan1qaqo55uvrqzNnzliNX7lyRefPn7eqKWgb1+7jejXXjt+oFwAAAAAASrNCBQTe3t7atWtXvvU7d+5U5cqVb7spSapZs6Z8fX21evVqc11GRoY2bdqkkJAQSVJISIjS0tK0detWs2bNmjXKzc1VcHCwWbN+/Xrl5OSYNStXrlTdunVVsWJFs+ba/eTV5O3nZnoBAAAAAKA0K1RA0KNHD7344otau3atrl69qqtXr2rNmjUaPHjwdWf0L0hmZqZ27NihHTt2SPpjMsAdO3bo+PHjslgsGjJkiMaNG6dFixZp9+7d6t27t/z8/Mw7HdSvX18dOnRQv3799OOPP2rDhg0aOHCgIiMj5efnJ0nq2bOnnJycFBUVpb1792revHmaNm2aYmJizD4GDx6sxMRETZo0SQcOHNDYsWO1ZcsWDRw4UJJuqhcAAAAAAEozh8I86Y033tDRo0fVrl07OTj8sYnc3Fz17t37luYg2LJli9q2bWsu531o79Onj+Lj4zVy5EhlZWWpf//+SktLU8uWLZWYmCgXFxfzOZ999pkGDhyodu3ayc7OTt26ddP06dPNcQ8PD61YsULR0dFq0qSJvLy8FBsbq/79+5s1LVq00Ny5c/XKK69ozJgxql27thYuXKiGDRuaNTfTCwAAAAAApZXFMAyjsE8+ePCgdu7cKVdXVwUFBalGjRpF2dtd51buP3m7AkYtvaPbL25HJ4TbugUAAAAAKHVu5XNooc4gyFOnTh3VqVPndjYBAAAAAABKgEIFBFevXlV8fLxWr16tM2fOKDc312p8zZo1RdIcAAAAAAAoHoUKCAYPHqz4+HiFh4erYcOGslgsRd0XAAAAAAAoRoUKCL744gvNnz9fnTp1Kup+AAAAAACADRTqNodOTk6qVatWUfcCAAAAAABspFABwbBhwzRt2jTdxg0QAAAAAABACVKoSwy+//57rV27VsuWLVODBg3k6OhoNf71118XSXMAAAAAAKB4FCog8PT0VNeuXYu6FwAAAAAAYCOFCgjmzJlT1H0AAAAAAAAbKtQcBJJ05coVrVq1Su+++65+++03SdKpU6eUmZlZZM0BAAAAAIDiUagzCI4dO6YOHTro+PHjys7O1mOPPaYKFSpo4sSJys7O1uzZs4u6TwAAAAAAcAcV6gyCwYMHq2nTprpw4YJcXV3N9V27dtXq1auLrDkAAAAAAFA8CnUGwXfffaeNGzfKycnJan1AQIB++eWXImkMAAAAAAAUn0KdQZCbm6urV6/mW3/y5ElVqFDhtpsCAAAAAADFq1ABQfv27TV16lRz2WKxKDMzU6+99po6depUVL0BAAAAAIBiUqhLDCZNmqSwsDAFBgbq0qVL6tmzpw4dOiQvLy99/vnnRd0jAAAAAAC4wwoVEFSrVk07d+7UF198oV27dikzM1NRUVHq1auX1aSFAAAAAACgdChUQCBJDg4Oevrpp4uyFwAAAAAAYCOFCgg+/vjjvxzv3bt3oZoBAAAAAAC2UaiAYPDgwVbLOTk5+v333+Xk5KRy5coREAAAAAAAUMoU6i4GFy5csHpkZmYqOTlZLVu2ZJJCAAAAAABKoUIFBAWpXbu2JkyYkO/sAgAAAAAAUPIVWUAg/TFx4alTp4pykwAAAAAAoBgUag6CRYsWWS0bhqHTp09rxowZevjhh4ukMQAAAAAAUHwKFRBERERYLVssFlWpUkWPPvqoJk2aVBR9AQAAAACAYlSogCA3N7eo+wAAAAAAADZUpHMQAAAAAACA0qlQZxDExMTcdO3kyZMLswsAAAAAAFCMChUQbN++Xdu3b1dOTo7q1q0rSTp48KDs7e3VuHFjs85isRRNlwAAAAAA4I4qVEDQpUsXVahQQR999JEqVqwoSbpw4YL69u2rVq1aadiwYUXaJAAAAAAAuLMKNQfBpEmTNH78eDMckKSKFStq3Lhx3MUAAAAAAIBSqFABQUZGhs6ePZtv/dmzZ/Xbb7/ddlMAAAAAAKB4FSog6Nq1q/r27auvv/5aJ0+e1MmTJ/XVV18pKipKTzzxRJE1d/XqVb366quqWbOmXF1ddd999+mNN96QYRhmjWEYio2NVdWqVeXq6qrQ0FAdOnTIajvnz59Xr1695O7uLk9PT0VFRSkzM9OqZteuXWrVqpVcXFzk7++vuLi4fP0kJCSoXr16cnFxUVBQkL755psiO1YAAAAAAGypUAHB7Nmz1bFjR/Xs2VM1atRQjRo11LNnT3Xo0EHvvPNOkTU3ceJEzZo1SzNmzND+/fs1ceJExcXF6d///rdZExcXp+nTp2v27NnatGmT3NzcFBYWpkuXLpk1vXr10t69e7Vy5UotWbJE69evV//+/c3xjIwMtW/fXjVq1NDWrVv19ttva+zYsXrvvffMmo0bN6pHjx6KiorS9u3bFRERoYiICO3Zs6fIjhcAAAAAAFuxGNd+HX+LsrKy9NNPP0mS7rvvPrm5uRVZY5LUuXNn+fj46IMPPjDXdevWTa6urvr0009lGIb8/Pw0bNgwDR8+XJKUnp4uHx8fxcfHKzIyUvv371dgYKA2b96spk2bSpISExPVqVMnnTx5Un5+fpo1a5ZefvllpaSkyMnJSZI0atQoLVy4UAcOHJAkde/eXVlZWVqyZInZS/PmzdWoUSPNnj37po4nIyNDHh4eSk9Pl7u7e5G8RtcTMGrpHd1+cTs6IdzWLQAAAABAqXMrn0MLdQZBntOnT+v06dOqXbu23NzcdBtZQ4FatGih1atX6+DBg5KknTt36vvvv1fHjh0lSUeOHFFKSopCQ0PN53h4eCg4OFhJSUmSpKSkJHl6eprhgCSFhobKzs5OmzZtMmtat25thgOSFBYWpuTkZF24cMGsuXY/eTV5+ylIdna2MjIyrB4AAAAAAJREhQoIfv31V7Vr10516tRRp06ddPr0aUlSVFRUkd7icNSoUYqMjFS9evXk6OioBx98UEOGDFGvXr0kSSkpKZIkHx8fq+f5+PiYYykpKfL29rYad3BwUKVKlaxqCtrGtfu4Xk3eeEHGjx8vDw8P8+Hv739Lxw8AAAAAQHEpVEAwdOhQOTo66vjx4ypXrpy5vnv37kpMTCyy5ubPn6/PPvtMc+fO1bZt2/TRRx/pX//6lz766KMi28edNHr0aKWnp5uPEydO2LolAAAAAAAK5FCYJ61YsULLly9XtWrVrNbXrl1bx44dK5LGJGnEiBHmWQSSFBQUpGPHjmn8+PHq06ePfH19JUmpqamqWrWq+bzU1FQ1atRIkuTr66szZ85YbffKlSs6f/68+XxfX1+lpqZa1eQt36gmb7wgzs7OcnZ2vtXDBgAAAACg2BXqDIKsrCyrMwfynD9/vkg/EP/++++ys7Nu0d7eXrm5uZKkmjVrytfXV6tXrzbHMzIytGnTJoWEhEiSQkJClJaWpq1bt5o1a9asUW5uroKDg82a9evXKycnx6xZuXKl6tatq4oVK5o11+4nryZvPwAAAAAAlGaFCghatWqljz/+2Fy2WCzKzc1VXFyc2rZtW2TNdenSRW+++aaWLl2qo0ePasGCBZo8ebK6du1q7nfIkCEaN26cFi1apN27d6t3797y8/NTRESEJKl+/frq0KGD+vXrpx9//FEbNmzQwIEDFRkZKT8/P0lSz5495eTkpKioKO3du1fz5s3TtGnTFBMTY/YyePBgJSYmatKkSTpw4IDGjh2rLVu2aODAgUV2vAAAAAAA2EqhLjGIi4tTu3bttGXLFl2+fFkjR47U3r17df78eW3YsKHImvv3v/+tV199VQMGDNCZM2fk5+enf/zjH4qNjTVrRo4cqaysLPXv319paWlq2bKlEhMT5eLiYtZ89tlnGjhwoNq1ayc7Ozt169ZN06dPN8c9PDy0YsUKRUdHq0mTJvLy8lJsbKz69+9v1rRo0UJz587VK6+8ojFjxqh27dpauHChGjZsWGTHCwAAAACArViMQt6bMD09XTNmzNDOnTuVmZmpxo0bKzo62mouAFi7lftP3q6AUUvv6PaL29EJ4bZuAQAAAABKnVv5HHrLZxDk5OSoQ4cOmj17tl5++eVCNwkAAAAAAEqOW56DwNHRUbt27boTvQAAAAAAABsp1CSFTz/9tD744IOi7gUAAAAAANhIoSYpvHLlij788EOtWrVKTZo0kZubm9X45MmTi6Q5AAAAAABQPG4pIPj5558VEBCgPXv2qHHjxpKkgwcPWtVYLJai6w4AAAAAABSLWwoIateurdOnT2vt2rWSpO7du2v69Ony8fG5I80BAAAAAIDicUtzEPz5jojLli1TVlZWkTYEAAAAAACKX6EmKczz58AAAAAAAACUTrcUEFgslnxzDDDnAAAAAAAApd8tzUFgGIaeeeYZOTs7S5IuXbqk559/Pt9dDL7++uui6xAAAAAAANxxtxQQ9OnTx2r56aefLtJmAAAAAACAbdxSQDBnzpw71QcAAAAAALCh25qkEAAAAAAA3B0ICAAAAAAAAAEBAAAAAAAgIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAKgUBwS+//KKnn35alStXlqurq4KCgrRlyxZz3DAMxcbGqmrVqnJ1dVVoaKgOHTpktY3z58+rV69ecnd3l6enp6KiopSZmWlVs2vXLrVq1UouLi7y9/dXXFxcvl4SEhJUr149ubi4KCgoSN98882dOWgAAAAAAIpZiQ4ILly4oIcffliOjo5atmyZ9u3bp0mTJqlixYpmTVxcnKZPn67Zs2dr06ZNcnNzU1hYmC5dumTW9OrVS3v37tXKlSu1ZMkSrV+/Xv379zfHMzIy1L59e9WoUUNbt27V22+/rbFjx+q9994zazZu3KgePXooKipK27dvV0REhCIiIrRnz57ieTEAAAAAALiDLIZhGLZu4npGjRqlDRs26Lvvvitw3DAM+fn5adiwYRo+fLgkKT09XT4+PoqPj1dkZKT279+vwMBAbd68WU2bNpUkJSYmqlOnTjp58qT8/Pw0a9Ysvfzyy0pJSZGTk5O574ULF+rAgQOSpO7duysrK0tLliwx99+8eXM1atRIs2fPvqnjycjIkIeHh9LT0+Xu7l7o1+VmBIxaeke3X9yOTgi3dQsAAAAAUOrcyufQEn0GwaJFi9S0aVM99dRT8vb21oMPPqj//Oc/5viRI0eUkpKi0NBQc52Hh4eCg4OVlJQkSUpKSpKnp6cZDkhSaGio7OzstGnTJrOmdevWZjggSWFhYUpOTtaFCxfMmmv3k1eTt5+CZGdnKyMjw+oBAAAAAEBJVKIDgp9//lmzZs1S7dq1tXz5cr3wwgt68cUX9dFHH0mSUlJSJEk+Pj5Wz/Px8THHUlJS5O3tbTXu4OCgSpUqWdUUtI1r93G9mrzxgowfP14eHh7mw9/f/5aOHwAAAACA4lKiA4Lc3Fw1btxYb731lh588EH1799f/fr1u+lT+m1t9OjRSk9PNx8nTpywdUsAAAAAABSoRAcEVatWVWBgoNW6+vXr6/jx45IkX19fSVJqaqpVTWpqqjnm6+urM2fOWI1fuXJF58+ft6opaBvX7uN6NXnjBXF2dpa7u7vVAwAAAACAkqhEBwQPP/ywkpOTrdYdPHhQNWrUkCTVrFlTvr6+Wr16tTmekZGhTZs2KSQkRJIUEhKitLQ0bd261axZs2aNcnNzFRwcbNasX79eOTk5Zs3KlStVt25d844JISEhVvvJq8nbDwAAAAAApVmJDgiGDh2qH374QW+99ZYOHz6suXPn6r333lN0dLQkyWKxaMiQIRo3bpwWLVqk3bt3q3fv3vLz81NERISkP8446NChg/r166cff/xRGzZs0MCBAxUZGSk/Pz9JUs+ePeXk5KSoqCjt3btX8+bN07Rp0xQTE2P2MnjwYCUmJmrSpEk6cOCAxo4dqy1btmjgwIHF/roAAAAAAFDUHGzdwF9p1qyZFixYoNGjR+v1119XzZo1NXXqVPXq1cusGTlypLKystS/f3+lpaWpZcuWSkxMlIuLi1nz2WefaeDAgWrXrp3s7OzUrVs3TZ8+3Rz38PDQihUrFB0drSZNmsjLy0uxsbHq37+/WdOiRQvNnTtXr7zyisaMGaPatWtr4cKFatiwYfG8GAAAAAAA3EEWwzAMWzdRVtzK/SdvV8CopXd0+8Xt6IRwW7cAAAAAAKXOrXwOLdGXGAAAAAAAgOJBQAAAAAAAAAgIAAAAAAAAAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAUCkLCCZMmCCLxaIhQ4aY6y5duqTo6GhVrlxZ5cuXV7du3ZSammr1vOPHjys8PFzlypWTt7e3RowYoStXrljVrFu3To0bN5azs7Nq1aql+Pj4fPufOXOmAgIC5OLiouDgYP3444934jABAAAAACh2pSYg2Lx5s959913df//9VuuHDh2qxYsXKyEhQd9++61OnTqlJ554why/evWqwsPDdfnyZW3cuFEfffSR4uPjFRsba9YcOXJE4eHhatu2rXbs2KEhQ4boueee0/Lly82aefPmKSYmRq+99pq2bdumBx54QGFhYTpz5sydP3gAAAAAAO4wi2EYhq2buJHMzEw1btxY77zzjsaNG6dGjRpp6tSpSk9PV5UqVTR37lw9+eSTkqQDBw6ofv36SkpKUvPmzbVs2TJ17txZp06dko+PjyRp9uzZeumll3T27Fk5OTnppZde0tKlS7Vnzx5zn5GRkUpLS1NiYqIkKTg4WM2aNdOMGTMkSbm5ufL399egQYM0atSomzqOjIwMeXh4KD09Xe7u7kX5EuUTMGrpHd1+cTs6IdzWLQAAAABAqXMrn0NLxRkE0dHRCg8PV2hoqNX6rVu3Kicnx2p9vXr1VL16dSUlJUmSkpKSFBQUZIYDkhQWFqaMjAzt3bvXrPnztsPCwsxtXL58WVu3brWqsbOzU2hoqFlTkOzsbGVkZFg9AAAAAAAoiRxs3cCNfPHFF9q2bZs2b96cbywlJUVOTk7y9PS0Wu/j46OUlBSz5tpwIG88b+yvajIyMnTx4kVduHBBV69eLbDmwIED1+19/Pjx+uc//3lzBwoAAAAAgA2V6DMITpw4ocGDB+uzzz6Ti4uLrdu5ZaNHj1Z6err5OHHihK1bAgAAAACgQCU6INi6davOnDmjxo0by8HBQQ4ODvr22281ffp0OTg4yMfHR5cvX1ZaWprV81JTU+Xr6ytJ8vX1zXdXg7zlG9W4u7vL1dVVXl5esre3L7AmbxsFcXZ2lru7u9UDAAAAAICSqEQHBO3atdPu3bu1Y8cO89G0aVP16tXL/G9HR0etXr3afE5ycrKOHz+ukJAQSVJISIh2795tdbeBlStXyt3dXYGBgWbNtdvIq8nbhpOTk5o0aWJVk5ubq9WrV5s1AAAAAACUZiV6DoIKFSqoYcOGVuvc3NxUuXJlc31UVJRiYmJUqVIlubu7a9CgQQoJCVHz5s0lSe3bt1dgYKD+/ve/Ky4uTikpKXrllVcUHR0tZ2dnSdLzzz+vGTNmaOTIkXr22We1Zs0azZ8/X0uX/u9OADExMerTp4+aNm2qhx56SFOnTlVWVpb69u1bTK8GAAAAAAB3TokOCG7GlClTZGdnp27duik7O1thYWF65513zHF7e3stWbJEL7zwgkJCQuTm5qY+ffro9ddfN2tq1qyppUuXaujQoZo2bZqqVaum999/X2FhYWZN9+7ddfbsWcXGxiolJUWNGjVSYmJivokLAQAAAAAojSyGYRi2bqKsuJX7T96ugFFLb1xUihydEG7rFgAAAACg1LmVz6Eleg4CAAAAAABQPAgIAAAAAAAAAQEAAAAAACAgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACoFAcH48ePVrFkzVahQQd7e3oqIiFBycrJVzaVLlxQdHa3KlSurfPny6tatm1JTU61qjh8/rvDwcJUrV07e3t4aMWKErly5YlWzbt06NW7cWM7OzqpVq5bi4+Pz9TNz5kwFBATIxcVFwcHB+vHHH4v8mAEAAAAAKG4lPiD49ttvFR0drR9++EErV65UTk6O2rdvr6ysLLNm6NChWrx4sRISEvTtt9/q1KlTeuKJJ8zxq1evKjw8XJcvX9bGjRv10UcfKT4+XrGxsWbNkSNHFB4errZt22rHjh0aMmSInnvuOS1fvtysmTdvnmJiYvTaa69p27ZteuCBBxQWFqYzZ84Uz4sBAAAAAMAdYjEMw7B1E7fi7Nmz8vb21rfffqvWrVsrPT1dVapU0dy5c/Xkk09Kkg4cOKD69esrKSlJzZs317Jly9S5c2edOnVKPj4+kqTZs2frpZde0tmzZ+Xk5KSXXnpJS5cu1Z49e8x9RUZGKi0tTYmJiZKk4OBgNWvWTDNmzJAk5ebmyt/fX4MGDdKoUaPy9Zqdna3s7GxzOSMjQ/7+/kpPT5e7u/sde40kKWDU0ju6/eJ2dEK4rVsAAAAAgFInIyNDHh4eN/U5tMSfQfBn6enpkqRKlSpJkrZu3aqcnByFhoaaNfXq1VP16tWVlJQkSUpKSlJQUJAZDkhSWFiYMjIytHfvXrPm2m3k1eRt4/Lly9q6datVjZ2dnUJDQ82aPxs/frw8PDzMh7+//+0ePgAAAAAAd0SpCghyc3M1ZMgQPfzww2rYsKEkKSUlRU5OTvL09LSq9fHxUUpKillzbTiQN5439lc1GRkZunjxos6dO6erV68WWJO3jT8bPXq00tPTzceJEycKd+AAAAAAANxhDrZu4FZER0drz549+v77723dyk1xdnaWs7OzrdsAAAAAAOCGSs0ZBAMHDtSSJUu0du1aVatWzVzv6+ury5cvKy0tzao+NTVVvr6+Zs2f72qQt3yjGnd3d7m6usrLy0v29vYF1uRtAwAAAACA0qrEBwSGYWjgwIFasGCB1qxZo5o1a1qNN2nSRI6Ojlq9erW5Ljk5WcePH1dISIgkKSQkRLt377a628DKlSvl7u6uwMBAs+babeTV5G3DyclJTZo0sarJzc3V6tWrzRoAAAAAAEqrEn+JQXR0tObOnav//ve/qlChgnm9v4eHh1xdXeXh4aGoqCjFxMSoUqVKcnd316BBgxQSEqLmzZtLktq3b6/AwED9/e9/V1xcnFJSUvTKK68oOjravATg+eef14wZMzRy5Eg9++yzWrNmjebPn6+lS/93N4CYmBj16dNHTZs21UMPPaSpU6cqKytLffv2Lf4XBgAAAACAIlTiA4JZs2ZJktq0aWO1fs6cOXrmmWckSVOmTJGdnZ26deum7OxshYWF6Z133jFr7e3ttWTJEr3wwgsKCQmRm5ub+vTpo9dff92sqVmzppYuXaqhQ4dq2rRpqlatmt5//32FhYWZNd27d9fZs2cVGxurlJQUNWrUSImJifkmLgQAAAAAoLSxGIZh2LqJsuJW7j95uwJGLb1xUSlydEK4rVsAAAAAgFLnVj6Hlvg5CAAAAAAAwJ1HQAAAAAAAAAgIAAAAAAAAAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAEAEBAAAAAAAQAQEAAAAAABABAQAAAAAAkORg6waAsiZg1FJbt1Ckjk4It3ULAAAAAIoAAQEAXONuCnAIbwAAAHArCAgAAKXC3RTeSAQ4AACg5GEOAgAAAAAAQEAAAAAAAAAICAAAAAAAgJiDAAAA3CbmhwAA4O7AGQQAAAAAAIAzCAAAAO5md9MZHnfb2R1303sj3X3vD1AWcQYBAAAAAAAgIAAAAAAAAAQEAAAAAABABAS3bObMmQoICJCLi4uCg4P1448/2rolAAAAAABuG5MU3oJ58+YpJiZGs2fPVnBwsKZOnaqwsDAlJyfL29vb1u0BAAAAKAJMIFmy3U3vT0l7bziD4BZMnjxZ/fr1U9++fRUYGKjZs2erXLly+vDDD23dGgAAAAAAt4UzCG7S5cuXtXXrVo0ePdpcZ2dnp9DQUCUlJRX4nOzsbGVnZ5vL6enpkqSMjIw726yk3Ozf7/g+ilNxvGbFhfemZLub3h/em5Ltbnp/eG9Ktrvp/eG9KdnupveH96Zku5ven+J4b/L2YRjGDWstxs1UQadOndI999yjjRs3KiQkxFw/cuRIffvtt9q0aVO+54wdO1b//Oc/i7NNAAAAAADyOXHihKpVq/aXNZxBcAeNHj1aMTEx5nJubq7Onz+vypUry2Kx2LCzopGRkSF/f3+dOHFC7u7utm4H1+C9Kbl4b0o23p+Si/em5OK9Kdl4f0ou3puS6257bwzD0G+//SY/P78b1hIQ3CQvLy/Z29srNTXVan1qaqp8fX0LfI6zs7OcnZ2t1nl6et6pFm3G3d39rvjFuRvx3pRcvDclG+9PycV7U3Lx3pRsvD8lF+9NyXU3vTceHh43VcckhTfJyclJTZo00erVq811ubm5Wr16tdUlBwAAAAAAlEacQXALYmJi1KdPHzVt2lQPPfSQpk6dqqysLPXt29fWrQEAAAAAcFsICG5B9+7ddfbsWcXGxiolJUWNGjVSYmKifHx8bN2aTTg7O+u1117LdxkFbI/3puTivSnZeH9KLt6bkov3pmTj/Sm5eG9KrrL83nAXAwAAAAAAwBwEAAAAAACAgAAAAAAAAIiAAAAAAAAAiIAAAAAAAACIgACFcO7cOZ07d87WbQAAAAAAihABAW5KWlqaoqOj5eXlJR8fH/n4+MjLy0sDBw5UWlqardsDAAAAANwmAgLc0Pnz5xUcHKyPPvpI3bp106RJkzRp0iQ98cQTio+PV0hIiC5cuGDrNsusU6dOafjw4crIyMg3lp6erhEjRig1NdUGneFmnD59WgMHDrR1G0Cpc/HiRVu3AJQ4V69e1a5duwr8/fj999+1a9cu5ebm2qAzoGTj7+n/ISDADb3++utycnLSTz/9pHfffVdDhgzRkCFD9N577+nw4cNydHTU66+/bus2y6zJkycrIyND7u7u+cY8PDz022+/afLkyTboDHn27t2rGTNm6L333jPPuDl37pyGDh2qe++9V2vXrrVtg2XUTz/9pGeffdZcrl69uipVqmQ+qlSpouTkZBt2iIJkZ2dr0qRJqlmzpq1bKdOu/SP6m2++0aJFi8zH0qVLbdhZ2fbJJ5/o2WeflZOTU74xJycnPfvss5o7d64NOsOaNWsUGBh43Q+gDRo00HfffWeDziDx97QVA7iBGjVqGImJidcdX7ZsmVGjRo3iawhWGjRoYHz33XfXHd+wYYMRGBhYjB3hWv/9738NR0dHw2KxGBaLxbjvvvuMNWvWGF5eXkZYWJixbNkyW7dYZg0ePNgYNWqUuVy+fHkjLi7OiI+PN+Lj442OHTsa//jHP2zYYdl16dIlY9SoUUaTJk2MkJAQY8GCBYZhGMaHH35oVK1a1ahWrZoxYcIE2zZZhi1evNho1KiRuVy+fHnz/+MsFothZ2dnJCQk2LDDsqtly5bG559/ft3xefPmGa1atSrGjpCnS5cuxuTJk687Pm3aNCMiIqIYO8K1+Hv6fwgIcENOTk7GiRMnrjt+4sQJw9nZuRg7wrXKlStnHDt27Lrjx44dM8qVK1eMHeFazZo1M4YMGWL89ttvxpQpUwyLxWI0bNjQ+PHHH23dWpnXsGFDY9OmTeZy+fLljZ9++slcXrdunVGrVi1btFbmjRw50vDw8DC6detmVK1a1XBwcDD69etnBAUFGZ9//rlx5coVW7dYpnXp0sX44IMPzOU//+5MnDjR6Nixoy1aK/OqVKliHDly5LrjP//8s+Hl5VV8DcFUvXp1Y9++fdcd379/v+Hv71+MHeFa/D39P1xigBvy8vLS0aNHrzt+5MgRVapUqfgaghVXV9e/fH+OHj0qV1fX4msIVpKTkxUdHa3y5ctr0KBBsrOz05QpU9SsWTNbt1bmHT16VH5+fubyc889Jw8PD3M5ICBAJ0+etEVrZV5CQoI+/vhjffnll1qxYoWuXr2qK1euaOfOnYqMjJS9vb2tWyzTdu/erYcffvi64x07dtSWLVuKsSPkycrKKvAU9jy//fabfv/992LsCHlSU1Pl6Oh43XEHBwedPXu2GDvCtfh7+n8ICHBDYWFhevnll3X58uV8Y9nZ2Xr11VfVoUMHG3QGSQoODtYnn3xy3fGPP/5YDz30UDF2hGv99ttv5vVs9vb2cnV11b333mvjriBJdnZ2OnXqlLk8ZcoUVa5c2Vy+0R9zuHNOnjypJk2aSJIaNmwoZ2dnDR06VBaLxcadQfpjclVnZ2dzee3atfL39zeXy5cvr/T0dFu0VubVrl1bGzduvO74999/r9q1axdjR8hzzz33aM+ePdcd37Vrl6pWrVqMHeFa/D39Pw62bgAl3+uvv66mTZuqdu3aio6OVr169WQYhvbv36933nlH2dnZf/kLhTtr+PDheuyxx+Th4aERI0bIx8dH0h8fbuLi4hQfH68VK1bYuMuybfny5eY307m5uVq9enW+PxIef/xxW7RWpjVo0ECrVq267j/4y5cvV8OGDYu5K0h/zMR+7SRrDg4OKl++vA07wrUqVaqkw4cPKyAgQJLUtGlTq/FDhw5xZqGN9OzZU6+88opatGih+++/32ps586dio2N1ciRI23UXdnWqVMn80s1FxcXq7GLFy/qtddeU+fOnW3UHfh7+n8shmEYtm4CJd+RI0c0YMAArVixQnk/MhaLRY899phmzJihWrVq2bjDsu3dd9/V4MGDlZOTI3d3d1ksFqWnp8vR0VFTpkzRCy+8YOsWyyw7uxufqGWxWHT16tVi6AbX+s9//qMhQ4Zo/vz5Cg8PtxpbvHixIiMjNXXqVPXr189GHZZddnZ26tixo/kt9eLFi/Xoo4/Kzc3Nqu7rr7+2RXtlXmRkpH7//XctWrSowPHOnTvLzc1N8+bNK+bOkJOTo/bt2+v7779XaGio6tWrJ0k6cOCAVq1apYcfflgrV67k7CgbSE1NVePGjWVvb6+BAweqbt26kv54b2bOnKmrV69q27Zt5gdTFD/+nv4DAQFuyYULF3To0CFJUq1atfiGoAT55ZdfNH/+fB0+fFiGYahOnTp68sknVa1aNVu3BpRYPXr00Lx581SvXj3zj7Xk5GQlJyerW7dumj9/vo07LJv69u17U3Vz5sy5w52gINu3b1dISIi6dOmikSNHqk6dOpL++N2ZOHGili5dqo0bN6px48Y27rRsysnJ0ZQpUzR37lwdOnTI/JugZ8+eGjJkSIG3QETxOHbsmF544QUtX77c6gu3sLAwzZw5k9u3lgD8PU1AAAAo47744gt98cUXOnjwoKQ/ruHt0aOHIiMjbdwZUHL997//1XPPPafz589bra9YsaLef/99RURE2KYxoBS4cOGC+QG0du3aqlixoq1bAkwEBMBdIiEhQZ9//rn5ISfv24Inn3zSxp2VbQMGDFBcXJx5/fTnn3+uxx9/3DxVOi0tTT179tQ333xjyzaBUufMmTPy9va2dRtl2u+//67ly5ebZxbWrl1b7du3z3cpCGzr0qVLmjdvnrKysvTYY48xSWEJkJaWpsOHD0v644xcT09P2zYEHTx4UGlpaVbzEq1evVrjxo1TVlaWIiIiNGbMGBt2WHwICIBSLjc3Vz169FBCQoLq1KljXm+4f/9+HT58WE899ZQ+//xzZv+2EXt7e50+fdr8IOPu7q4dO3aYdzJITU2Vn58fcxCUQNu2bVNsbKyWLFli61bKnHLlyunYsWOqUqWKJCk8PFzvv/++OcM3vzcl3y+//KJ77rnH1m2UOTExMcrJydG///1vSdLly5f10EMPad++fSpXrpyuXLmilStXKiQkxMadlk1Hjx5VdHR0vksMOnTooBkzZpgTf6L4de3aVUFBQXr99dcl/TH/WoMGDdSqVSvVq1dPH374od544w0NGTLEto0WA25zCJRy06ZN06pVq7Ro0SIdOHBACxcu1MKFC5WcnKwFCxZo5cqVmjZtmq3bLLP+nMGSyZYsy5cv1/DhwzVmzBj9/PPPkv6YMCoiIkLNmjVTbm6ujTssmy5dumT1u7J+/XpdvHjRqobfpZIpJSVFgwYN4ltqG1mxYoUee+wxc/mzzz7T8ePHdejQIV24cEFPPfWUxo0bZ8MOy64TJ06oefPm2rVrl9544w199dVX+uqrr/T6669r586dCgkJ0cmTJ23dZpm1ZcsWdezY0Vz+7LPPVKdOHS1fvlzTpk3T1KlTFR8fb7sGixEBAVDKzZkzR2+//XaBt8Z5/PHHFRcXpw8//NAGnQEl2wcffKCOHTsqPj5eEydOVPPmzfXpp58qJCREvr6+2rNnD5d+lGCcFWU7Fy5cUI8ePeTl5SU/Pz9Nnz5dubm5io2N1b333qvNmzczgaSNHD9+XIGBgebyihUr9OSTT6pGjRqyWCwaPHiwtm/fbsMOy66xY8eqbt26OnTokEaPHq2IiAjztPWDBw+qTp06Gjt2rK3bLLPOnTtnNRHh2rVr1aVLF3O5TZs2Onr0qA06K34EBEApd+jQIYWGhl53PDQ01Lw+FMD/TJs2TRMnTtS5c+c0f/58nTt3Tu+88452796t2bNnq379+rZuESiRRo0apY0bN+qZZ55R5cqVNXToUHXu3Fnbtm3TmjVr9MMPP6h79+62brNMsrOzszq75ocfflDz5s3NZU9PT124cMEWrZV5iYmJevPNN+Xi4pJvzNXVVW+88QahtA1VqlRJp0+flvTH5btbtmyx+t25fPlymTlzzcHWDQC4Pa6urkpLS1P16tULHM/IyCjwHyMUn9jYWJUrV07SH//AvPnmm/Lw8JD0xyRfsI2ffvpJTz31lCTpiSeekIODg95+++0ydSujkspisVidIfDnZdjWsmXLFB8fr0cffVQDBw7Uvffeq0aNGumtt96ydWtlXv369bV48WLFxMRo7969On78uNq2bWuOHzt2TD4+PjbssOw6d+7cX84xcO+99+a7KwiKT5s2bfTGG2/onXfeUUJCgnJzc9WmTRtzfN++fWVmjggCAqCUCwkJ0axZszRr1qwCx2fOnMlkRDbUunVrJScnm8stWrQwr3W/tgbF7+LFi2ZwY7FY5OzsbE6CB9vKu/d0XiiQmZmpBx98UHZ2duY4bOfUqVPmGTYBAQFycXHR008/beOuIEkjR45UZGSkli5dqr1796pTp06qWbOmOf7NN99YzdKO4lO1alXt27fvuiH0nj175OvrW8xdIc+bb76pxx57TDVq1JC9vb2mT59udUeWTz75RI8++qgNOyw+BARAKffyyy+rTZs2+vXXXzV8+HDVq1dPhmFo//79mjRpkv773/9q7dq1tm6zzFq3bp2tW8BfeP/9981bUF65ckXx8fHy8vKyqnnxxRdt0VqZxvXrJZthGHJw+N+fkPb29nJ1dbVhR8jTtWtXffPNN1qyZInat2+vQYMGWY2XK1dOAwYMsFF3ZVtERISGDx+u1atXm3doyXPmzBm99NJLioiIsE1zUEBAgPbv36+9e/eqSpUq8vPzsxr/5z//WWbOMOQ2h8BdYMGCBerfv3++U9MqVqyod999V926dbNRZ5D+uMxj06ZN5u2m/vyHAWwjICDghqetWyyWfGd8AGWdnZ2dGjZsaIYEu3btUr169eTk5GRVt23bNlu0B5RIFy5cUHBwsFJSUvT0009bfaEzd+5c+fr66ocfflClSpVs3SrKOAIC4C7x+++/a/ny5eaEhHXq1FH79u3l5OSkM2fO5EtCUTx27NihTp06KSUlRZJUoUIFzZ8/X2FhYTbuDCjZ5s2bp0WLFuny5ctq166dnn/+eVu3hP/vn//8503Vvfbaa3e4E/zZrl27bqru/vvvv8OdoCAXLlzQmDFjNG/ePKWlpUn6Y+LIv/3tb3rrrbcIB2woJibmpuomT558hzuxPQIC4C63c+dONW7cWFevXrV1K2VSWFiYMjMz9a9//UsuLi564403tHv3bu4sAfyFWbNmKTo6WrVr15arq6t2796tmJgYvf3227ZuDSjR7OzsZLFYCpynI2+9xWLhbwIbMwxDZ8+elSRVqVJFFotFZ86c0fvvv68xY8bYuLuy6drJPK/HYrFozZo1xdCNbREQAHc5AgLb8vLy0ooVK9S4cWNJUlpamipVqqS0tDS5u7vbuDtcuXJFU6ZM0eeff66DBw9K+uPsm549e2rw4MFydHS0cYdlU4MGDfS3v/3N/Ab6008/1T/+8Q9lZWXZuDP82a5du6x+d/hm2raOHTt2U3U1atS4w53gVvH3GkoKAgLgLsc/OLZlZ2enlJQUeXt7m+sqVKigXbt2Wc0sjeJ38eJFPfbYY0pKSlJoaKg5K/v+/fu1atUqPfzww1qxYgW3CbUBV1dX7d+/37ylVG5urlxdXXX06FHuNFFC/Pjjj4qKitK+ffvMb6stFosaNGigDz74QM2aNbNxh0Dpwt9rKCm4iwEA3GH79u0z5yCQZE5K9Ntvv5nr+Nat+E2YMEEnTpzQ9u3b873+O3fu1OOPP64JEyZo7NixtmmwDMvOzra6vZSdnZ2cnJx08eJFG3aFPPv27VO7du1Uv359ffrpp2a4tm/fPk2ZMkXt2rXTDz/8oMDAQBt3WvYcP378puqqV69+hzsBSpfXX3/9pupiY2PvcCe2xxkEQCl3owmJDhw4oB49epBI2wjXg5ZcdevW1VtvvXXdu3wkJCTo5ZdfNk+fRvGxs7NT//79Va5cOXPdzJkz9fTTT8vDw8NcVxYmiyqJ/va3v+nKlSv66quv8t0JxDAMPfHEE3J0dNT8+fNt1GHZZW9vb/73tWd2XLuOf3NKJs4gsC07Ozv5+fnJ29u7wL/ZpD9+l8rC3Vk4gwAo5Ro1anRTH0BhG0eOHLF1C7iOY8eO6aGHHrruePPmzW/62zgUrdatWys5OdlqXYsWLaxuOcn/r9nO2rVrtWzZsgLfA4vFojFjxqhTp0426AwWi0XVqlXTM888oy5dupi3ooTt3WiW/LxJC2EbHTt21Jo1a9S0aVM9++yz6ty5s+zs7Gzdlk1wBgFQyjEhEVA43t7eWrZsmZo0aVLg+ObNm9WpUyf+aAP+xMXFRYcOHZK/v3+B4ydOnFDt2rV16dKlYu4MKSkp+uijjzRnzhylpaXp6aefVlRUlHkZCGznZmbJl/4I4GAbp06d0kcffaT4+HhlZGSod+/eevbZZ1W3bl1bt1asCAgAoJgFBQXpm2++ue4f1yge3bt3N0+TLki3bt1kb2/PadIlxIYNG9S0aVM5OzvbupUy70aX53z55Zd6+eWX850FguL1/fffa86cOUpISFBgYKCioqIUFRVVZr8VBW7F+vXrNWfOHH311VcKCgrSqlWr5Orqauu2igX/DwHchYKCgnTixAlbt4HrOHr0qHJycmzdRpn32muvacWKFWrevLnmz5+vXbt2aefOnfriiy8UHBysFStWmLfZg+117NhRv/zyi63bgKTIyEjFxMRoz549+cZ2796t4cOHq3v37jboDNdq2bKlPvjgAx06dEjlypXT888/r7S0NFu3hWts2LBB2dnZtm4DBWjWrJnatm2r+vXra/v27WXq7zYuTALuQnwABW4sMDBQK1euVFRUlCIjI83rqQ3DUL169bRixQo1aNDAxl0iDyc8lhyjR4/WqlWr1KhRIz322GOqX7++eXeWVatW6aGHHtKYMWNs3WaZt3HjRn344YdKSEhQ3bp1NXPmTHl6etq6LVyjY8eO2rFjh+69915bt4L/LykpSR9++KHmz5+vOnXqqG/fvurZs6fc3d1t3VqxISAAgGLWqlWrMnOaWknXvHlz7d27Vzt27DDvVlCnTh01atTIto0BJZiLi4vWrl2rKVOm6PPPP9e3334r6Y/fnXHjxmno0KFcCmIjp0+f1scff6w5c+bowoUL6tWrlzZs2KCGDRvaujUUgOCz5IiLi1N8fLzOnTunXr166bvvviuzt6BmDgLgLtSpUyd98MEHqlq1qq1bwf/366+/qnLlypL+uE/1+++/r4sXL+rxxx9Xq1atbNwdUPLNnTtX//d//yc3NzdbtwKUWI6OjrrnnnvUp08fPf7443J0dCywrqx+8ClpKlSooJ07d3IGQQlgZ2en6tWrq3PnznJycrpuXVm4vS4BAXCX4ANoybR792516dLFnNX7iy++UIcOHZSVlSU7OztlZWXpyy+/VEREhK1bhZhAsqR44oknbqru66+/vsOd4GYxiWTJcO0EhNdeNnUti8Wiq1evFmtfKBjBZ8nRpk2bG94+12KxaM2aNcXUke0QEAClHB9AS7aOHTvKwcFBo0aN0ieffKIlS5aoffv2ev/99yVJgwYN0tatW/XDDz/YuFNIfJtTUvTt2/em6ubMmXOHO8HNcnd351rqEoBbH5dcBJ8oLQgIgFKOD6Alm5eXl9asWaP7779fmZmZcnd31+bNm9WkSRNJ0oEDB9S8eXNmli4hCAiAwuF3B/hrBJ+lT1k9M4qAACjl+ABastnZ2SklJUXe3t6S8v8RnZqaKj8/P073LCGYvwMoHAKCkotLp4DCKatnRnEXA6CUO3/+vHx9fSVJ5cuXl5ubmypWrGiOV6xYUb/99put2oOU75q2G13jhuJ17fwds2fP1qxZs5i/A7hF7777rnx8fGzdBgrArY+Bwimr36MTEAB3AT6AlmzPPPOMeXrapUuX9Pzzz5sTEmVnZ9uytTLtRvN3TJkyhfk7gAJc71rqL7/80mqZa6kBoPQhIADuAnwALbn69Oljtfz000/nq+ndu3dxtYNrjBw5UkFBQfrss8/0ySefqHPnzurUqZPV/B0TJkwgIAD+xMPDw9Yt4Ba0atVKrq6utm4DKHXK6plRzEEAlHJMegMUDvN3ALhbcetj4NZwl4n/4QwCoJTjgz9QOMzfAeBuw6VTQOFwZtT/cAYBAKBMsrOzU2pqqqpUqSLpj1nYd+3apZo1a0riDhMASh9ufQzgdhEQAADKJDs7O3Xs2NGcv2Px4sV69NFHrebvSExMJCAAUGpw6RSA20VAAAAok5i/A8Ddxs7OTikpKfL29pb0x5lRO3fuNO/jzplRAG6EOQgAAGUSH/wB3I249TGA20FAAAAAANwluPUxgNvBJQYAAADAXYBLpwDcLgICAAAAAAAgO1s3AAAAAAAAbI+AAAAAAAAAEBAAAAAAAAACAgAAAAAAIAICAAAAAAAgAgIAAFCMLBbLXz7Gjh1r6xYBACizHGzdAAAAKDtOnz5t/ve8efMUGxur5ORkc1358uVt0RYAABBnEAAAgGLk6+trPjw8PGSxWMzlrKws9erVSz4+PipfvryaNWumVatWWT3/9OnTCg8Pl6urq2rWrKm5c+cqICBAU6dOlSQZhqGxY8eqevXqcnZ2lp+fn1588UUbHCkAAKUPZxAAAIASITMzU506ddKbb74pZ2dnffzxx+rSpYuSk5NVvXp1SVLv3r117tw5rVu3To6OjoqJidGZM2fMbXz11VeaMmWKvvjiCzVo0EApKSnauXOnrQ4JAIBShYAAAACUCA888IAeeOABc/mNN97QggULtGjRIg0cOFAHDhzQqlWrtHnzZjVt2lSS9P7776t27drmc44fPy5fX1+FhobK0dFR1atX10MPPVTsxwIAQGnEJQYAAKBEyMzM1PDhw1W/fn15enqqfPny2r9/v44fPy5JSk5OloODgxo3bmw+p1atWqpYsaK5/NRTT+nixYu699571a9fPy1YsEBXrlwp9mMBAKA0IiAAAAAlwvDhw7VgwQK99dZb+u6777Rjxw4FBQXp8uXLN70Nf39/JScn65133pGrq6sGDBig1q1bKycn5w52DgDA3YGAAAAAlAgbNmzQM888o65duyooKEi+vr46evSoOV63bl1duXJF27dvN9cdPnxYFy5csNqOq6urunTpounTp2vdunVKSkrS7t27i+swAAAotZiDAAAAlAi1a9fW119/rS5dushisejVV19Vbm6uOV6vXj2Fhoaqf//+mjVrlhwdHTVs2DC5urrKYrFIkuLj43X16lUFBwerXLly+vTTT+Xq6qoaNWrY6rAAACg1OIMAAACUCJMnT1bFihXVokULdenSRWFhYVbzDUjSxx9/LB8fH7Vu3Vpdu3ZVv379VKFCBbm4uEiSPD099Z///EcPP/yw7r//fq1atUqLFy9W5cqVbXFIAACUKhbDMAxbNwEAAFAYJ0+elL+/v1atWqV27drZuh0AAEo1AgIAAFBqrFmzRpmZmQoKCtLp06c1cuRI/fLLLzp48KAcHR1t3R4AAKUacxAAAIBSIycnR2PGjNHPP/+sChUqqEWLFvrss88IBwAAKAKcQQAAAAAAAJikEAAAAAAAEBAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAAAREAAAAAAAABEQAAAAAAAAERAAAAAAAABJ/w9Z1ReOdZIrBAAAAABJRU5ErkJggg=="},"metadata":{}},{"name":"stdout","text":"Number of training sentences: 946\nNumber of unique words in training set: 23623\nNumber of unique tags in training set: 9\n","output_type":"stream"}]},{"cell_type":"markdown","source":" # Define Dataset Loading and Visualization Functions","metadata":{}},{"cell_type":"code","source":"def read_labels(file_path):\n    labels = set()\n    with open(file_path, 'r') as file:\n        for line in file:\n            parts = line.strip().split()\n            # The NER tag is the last element on the line\n            if len(parts) > 1:  # This checks that it's not an empty line\n                labels.add(parts[-1])  # Add the NER tag to the set\n    return labels\n\n# Read labels from the dataset\ntrain_labels = read_labels('/kaggle/input/conll003-englishversion/train.txt')\nvalid_labels = read_labels('/kaggle/input/conll003-englishversion/valid.txt')\ntest_labels = read_labels('/kaggle/input/conll003-englishversion/test.txt')\n\n# Combine all labels to get the unique ones\nall_labels = set(list(train_labels) + list(valid_labels) + list(test_labels))\nnum_labels = len(all_labels)  # This should now give you the correct number of unique labels\n\nprint(f'Number of unique labels: {num_labels}')\nprint('All the Labels : ',all_labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:14:56.447553Z","iopub.execute_input":"2023-12-27T23:14:56.448276Z","iopub.status.idle":"2023-12-27T23:14:56.715108Z","shell.execute_reply.started":"2023-12-27T23:14:56.448246Z","shell.execute_reply":"2023-12-27T23:14:56.714110Z"},"trusted":true},"execution_count":227,"outputs":[{"name":"stdout","text":"Number of unique labels: 9\nAll the Labels :  {'I-MISC', 'I-ORG', 'I-LOC', 'B-ORG', 'I-PER', 'B-MISC', 'B-PER', 'O', 'B-LOC'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Label Interpretation**:\n\nThe **BIO**`(Begin-Inside-Outside)` tagging scheme is utilized in Named Entity Recognition to denote the position of tokens within named entities in text. Each token is prefixed with a label that identifies its role in an entity:\n\n- `B-` (Begin): Signifies the beginning of a named entity.\n- `I-` (Inside): Denotes that the token is part of an entity already initiated by a `B-` prefix.\n- `O` (Outside): Indicates that the token is not part of a named entity.\n\nSpecific entity types are tagged as follows:\n- `ORG` for Organizations: Identifies corporations, agencies, and other groups.\n- `PER` for Persons: Marks individual names.\n- `LOC` for Locations: Pinpoints geographical entities.\n- `MISC` for Miscellaneous: Catches entities that don't fall under the other categories.\n\nThe Unique Labels in my dataset are:\n1. - `O`: Non-entity, used for tokens that are not part of a named entity.\n2. - `B-LOC`: Beginning of a location name.\n3. - `I-LOC`: Token inside a location name.\n4. - `B-MISC`: Beginning of a miscellaneous entity that does not belong to the pre-defined categories of locations, organizations, or person names.\n5. - `I-MISC`: Token inside a miscellaneous entity.\n6. - `B-ORG`: Beginning of an organization name.\n7. - `I-ORG`: Token inside an organization name.\n8. - `B-PER`: Beginning of a person's name.\n9. - `I-PER`: Token inside a person's name.\n","metadata":{}},{"cell_type":"markdown","source":"**Interpretation**: \n\n* This bar chart illustrates the label distribution in the NER training data, revealing a **predominance of 'O' (non-entity) labels.** \n* Such imbalance require using **precision**, **recall**, and **F1-score** metrics for model evaluation, ensuring that the used BERT and BiLSTM models are accurately identifying all entity types, **not just the most frequent ones.**","metadata":{}},{"cell_type":"markdown","source":"### Model Setup and Device Configuration\n\nThe next cell sets up the BERT model for token classification, which will be used for the Named Entity Recognition (NER) task:\n\n- `device`: Determines if a CUDA-compatible GPU is available for training (preferred for performance), otherwise defaults to using the CPU.\n- `tokenizer`: Loads the `BertTokenizerFast`, which is optimized for speed, from the pre-trained 'bert-base-uncased' model. This tokenizer will convert text input into tokens that the BERT model can understand.\n- `model`: Initializes the `BertForTokenClassification` with the pre-trained 'bert-base-uncased' weights and configures it for the number of NER labels in our dataset.\n- `model.to(device)`: Moves the model to the GPU if available, allowing for faster computation.\n","metadata":{}},{"cell_type":"markdown","source":"# Setup Device and Tokenizer","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\nmodel = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:15:00.447600Z","iopub.execute_input":"2023-12-27T23:15:00.448361Z","iopub.status.idle":"2023-12-27T23:15:01.162302Z","shell.execute_reply.started":"2023-12-27T23:15:00.448329Z","shell.execute_reply":"2023-12-27T23:15:01.161291Z"},"trusted":true},"execution_count":228,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":228,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=9, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Define the Custom Dataset Class","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\n\nclass NERDataset(Dataset):\n    def __init__(self, tokenizer, file_path, tag_map):\n        self.tokenizer = tokenizer\n        self.sentences = []\n        self.labels = []\n\n        with open(file_path, 'r') as f:\n            sentence = []\n            label = []\n            for line in f:\n                line = line.strip()\n                if line == \"\":  # New sentence\n                    if sentence:\n                        self.sentences.append(sentence)\n                        self.labels.append(label)\n                        sentence = []\n                        label = []\n                else:\n                    parts = line.split()\n                    if len(parts) > 2:  # This handles cases where the token itself contains spaces\n                        token = \" \".join(parts[:-1])  # Join all but the last part\n                        tag = parts[-1]  # The last part is the tag\n                    else:\n                        token, tag = parts\n                    sentence.append(token)\n                    label.append(tag_map[tag])\n            # Catch the last sentence if the file doesn't end with a newline\n            if sentence:\n                self.sentences.append(sentence)\n                self.labels.append(label)\n        self.tag_map = tag_map\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(\n            sentence,\n            is_split_into_words=True,\n            padding='max_length',\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        )\n\n        # Create a new labels tensor with -100 (ignore index) for padding\n        label_tensor = torch.full((128,), fill_value=-100, dtype=torch.long)\n\n        # Set labels for the `max_length` first tokens, including the [CLS] token\n        label_tensor[1:len(label) + 1] = torch.LongTensor(label)\n\n        # Add the [SEP] token at the end\n        label_tensor[len(label) + 1] = self.tag_map[\"O\"]\n\n        encoding['labels'] = label_tensor\n        \n        return {key: tensor.squeeze(0) for key, tensor in encoding.items()}\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:15:22.586786Z","iopub.execute_input":"2023-12-27T23:15:22.587474Z","iopub.status.idle":"2023-12-27T23:15:22.600125Z","shell.execute_reply.started":"2023-12-27T23:15:22.587443Z","shell.execute_reply":"2023-12-27T23:15:22.599058Z"},"trusted":true},"execution_count":232,"outputs":[]},{"cell_type":"code","source":"# Create a dictionary mapping each label in all_labels to a unique integer.\n# all_labels is sorted, and each label is paired with an index using enumerate.\ntag_map = {tag: idx for idx, tag in enumerate(sorted(all_labels))}\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:15:25.438293Z","iopub.execute_input":"2023-12-27T23:15:25.439164Z","iopub.status.idle":"2023-12-27T23:15:25.443518Z","shell.execute_reply.started":"2023-12-27T23:15:25.439132Z","shell.execute_reply":"2023-12-27T23:15:25.442501Z"},"trusted":true},"execution_count":233,"outputs":[]},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = NERDataset(tokenizer, '/kaggle/input/conll003-englishversion/train.txt', tag_map)\nvalid_dataset = NERDataset(tokenizer, '/kaggle/input/conll003-englishversion/valid.txt', tag_map)\ntest_dataset = NERDataset(tokenizer, '/kaggle/input/conll003-englishversion/test.txt', tag_map)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:15:27.548129Z","iopub.execute_input":"2023-12-27T23:15:27.548522Z","iopub.status.idle":"2023-12-27T23:15:28.023728Z","shell.execute_reply.started":"2023-12-27T23:15:27.548489Z","shell.execute_reply":"2023-12-27T23:15:28.022684Z"},"trusted":true},"execution_count":234,"outputs":[]},{"cell_type":"code","source":"# Print tag_map to ensure all tags have an assigned index\nprint(\"Tag Map:\", tag_map)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:15:30.987869Z","iopub.execute_input":"2023-12-27T23:15:30.988244Z","iopub.status.idle":"2023-12-27T23:15:30.993393Z","shell.execute_reply.started":"2023-12-27T23:15:30.988214Z","shell.execute_reply":"2023-12-27T23:15:30.992438Z"},"trusted":true},"execution_count":235,"outputs":[{"name":"stdout","text":"Tag Map: {'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display tokenization and label shape for a sample\nsample_sentence = train_dataset.sentences[0]\nsample_label = train_dataset.labels[0]  # Use the labels corresponding to the first sentence\n\n# Convert all integer labels in sample_label to their string representations using the reverse of tag_map\nint_to_str_tag_map = {idx: tag for tag, idx in tag_map.items()}\nsample_label = [int_to_str_tag_map[tag] if isinstance(tag, int) else tag for tag in sample_label]\n\nprint(\"Original Sentence:\", sample_sentence)\nprint(\"Original Labels:\", sample_label)\n\nsample_encoding = tokenizer(\n    sample_sentence,\n    is_split_into_words=True,\n    padding='max_length',\n    truncation=True,\n    max_length=128,\n    return_tensors=\"pt\"\n)\n\n# Ensure all elements in sample_label are strings\nassert all(isinstance(tag, str) for tag in sample_label), \"Labels should be strings.\"\n\n# Create the label tensor using string tags\nsample_label_tensor = torch.full((128,), fill_value=-100, dtype=torch.long)\nsample_label_tensor[1:len(sample_label) + 1] = torch.LongTensor(\n    [tag_map[tag] for tag in sample_label]\n)\n\nprint(\"Tokenized Input IDs:\", sample_encoding['input_ids'])\nprint(\"Input IDs Shape:\", sample_encoding['input_ids'].shape)\nprint(\"Label Tensor:\", sample_label_tensor)\nprint(\"Label Tensor Shape:\", sample_label_tensor.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T22:48:08.088094Z","iopub.execute_input":"2023-12-27T22:48:08.088884Z","iopub.status.idle":"2023-12-27T22:48:08.102404Z","shell.execute_reply.started":"2023-12-27T22:48:08.088853Z","shell.execute_reply":"2023-12-27T22:48:08.101394Z"},"trusted":true},"execution_count":210,"outputs":[{"name":"stdout","text":"Original Sentence: ['EU NNP B-NP', 'rejects VBZ B-VP', 'German JJ B-NP', 'call NN I-NP', 'to TO B-VP', 'boycott VB I-VP', 'British JJ B-NP', 'lamb NN I-NP', '. . O']\nOriginal Labels: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\nTokenized Input IDs: tensor([[  101,  7327,  1050, 16275,  1038,  1011, 27937, 19164,  1058,  2497,\n          2480,  1038,  1011, 21210,  2446, 29017,  1038,  1011, 27937,  2655,\n          1050,  2078,  1045,  1011, 27937,  2000,  2000,  1038,  1011, 21210,\n         17757,  1058,  2497,  1045,  1011, 21210,  2329, 29017,  1038,  1011,\n         27937, 12559,  1050,  2078,  1045,  1011, 27937,  1012,  1012,  1051,\n           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0]])\nInput IDs Shape: torch.Size([1, 128])\nLabel Tensor: tensor([-100,    2,    8,    1,    8,    8,    8,    1,    8,    8, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100])\nLabel Tensor Shape: torch.Size([128])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming train_dataset, valid_dataset, test_dataset are the full datasets\n\n# Create the DataLoaders for iterating over the full datasets\n# DataLoader for the full training dataset, with a batch size of 8 samples and shuffling enabled to randomize the order of samples.\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n# DataLoader for the full validation dataset, with a batch size of 8 samples, no shuffling is typically needed for validation.\nvalid_loader = DataLoader(valid_dataset, batch_size=8)\n\n# DataLoader for the full testing dataset, with a batch size of 8 samples, no shuffling is typically needed for testing.\ntest_loader = DataLoader(test_dataset, batch_size=8)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:15:36.480926Z","iopub.execute_input":"2023-12-27T23:15:36.481298Z","iopub.status.idle":"2023-12-27T23:15:36.507835Z","shell.execute_reply.started":"2023-12-27T23:15:36.481261Z","shell.execute_reply":"2023-12-27T23:15:36.506846Z"},"trusted":true},"execution_count":236,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:15:40.679485Z","iopub.execute_input":"2023-12-27T23:15:40.680166Z","iopub.status.idle":"2023-12-27T23:15:40.688405Z","shell.execute_reply.started":"2023-12-27T23:15:40.680136Z","shell.execute_reply":"2023-12-27T23:15:40.687380Z"},"trusted":true},"execution_count":237,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Total number of training batches: {len(train_loader)}\")\nprint(f\"Total number of validation batches: {len(valid_loader)}\")\nprint(f\"Total number of testing batches: {len(test_loader)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:15:43.540831Z","iopub.execute_input":"2023-12-27T23:15:43.541236Z","iopub.status.idle":"2023-12-27T23:15:43.547824Z","shell.execute_reply.started":"2023-12-27T23:15:43.541204Z","shell.execute_reply":"2023-12-27T23:15:43.546713Z"},"trusted":true},"execution_count":238,"outputs":[{"name":"stdout","text":"Total number of training batches: 1874\nTotal number of validation batches: 434\nTotal number of testing batches: 461\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport numpy as np\n\nepochs = 3\ntotal_steps = len(train_loader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps=0,\n                                            num_training_steps=total_steps)\n\n# Define the function to calculate the evaluation metrics\ndef calculate_metrics(preds, labels):\n    preds_flat = np.argmax(preds, axis=2).flatten()\n    labels_flat = labels.flatten()\n    # Only calculate metrics for non-ignored labels\n    valid_indices = labels_flat != -100\n    preds_flat = preds_flat[valid_indices]\n    labels_flat = labels_flat[valid_indices]\n    accuracy = accuracy_score(labels_flat, preds_flat)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels_flat, preds_flat, average='macro')\n    return accuracy, precision, recall, f1\n\n# Training and validation loop\nfor epoch in range(epochs):\n    # Training step\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        model.zero_grad()\n        outputs = model(**batch)\n\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Average training loss: {avg_train_loss:.2f}\")\n\n    # Validation step\n    model.eval()\n    val_loss = 0\n    predictions, true_labels = [], []\n\n    for batch in tqdm(valid_loader, desc=f\"Validation Epoch {epoch}\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n\n        loss = outputs.loss\n        val_loss += loss.item()\n\n        logits = outputs.logits.detach().cpu().numpy()\n        label_ids = batch['labels'].cpu().numpy()\n\n        predictions.extend(logits)\n        true_labels.extend(label_ids)\n\n    avg_val_loss = val_loss / len(valid_loader)\n    accuracy, precision, recall, f1 = calculate_metrics(np.array(predictions), np.array(true_labels))\n\n    print(f\"Validation loss: {avg_val_loss:.2f}\")\n    print(f\"Validation Accuracy: {accuracy:.2f}\")\n    print(f\"Validation Precision: {precision:.2f}\")\n    print(f\"Validation Recall: {recall:.2f}\")\n    print(f\"Validation F1-Score: {f1:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:15:46.549181Z","iopub.execute_input":"2023-12-27T23:15:46.549937Z","iopub.status.idle":"2023-12-27T23:27:14.521449Z","shell.execute_reply.started":"2023-12-27T23:15:46.549903Z","shell.execute_reply":"2023-12-27T23:27:14.520439Z"},"trusted":true},"execution_count":239,"outputs":[{"name":"stderr","text":"Training Epoch 0: 100%|| 1874/1874 [03:32<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.41\n","output_type":"stream"},{"name":"stderr","text":"Validation Epoch 0: 100%|| 434/434 [00:16<00:00, 26.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.28\nValidation Accuracy: 0.91\nValidation Precision: 0.80\nValidation Recall: 0.48\nValidation F1-Score: 0.59\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1: 100%|| 1874/1874 [03:32<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.21\n","output_type":"stream"},{"name":"stderr","text":"Validation Epoch 1: 100%|| 434/434 [00:16<00:00, 26.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.19\nValidation Accuracy: 0.93\nValidation Precision: 0.79\nValidation Recall: 0.68\nValidation F1-Score: 0.73\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|| 1874/1874 [03:32<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.15\n","output_type":"stream"},{"name":"stderr","text":"Validation Epoch 2: 100%|| 434/434 [00:16<00:00, 26.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.17\nValidation Accuracy: 0.95\nValidation Precision: 0.83\nValidation Recall: 0.73\nValidation F1-Score: 0.77\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Saving our BERT Model","metadata":{}},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/my_bert_model')","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:30:30.142567Z","iopub.execute_input":"2023-12-27T23:30:30.143497Z","iopub.status.idle":"2023-12-27T23:30:31.257737Z","shell.execute_reply.started":"2023-12-27T23:30:30.143464Z","shell.execute_reply":"2023-12-27T23:30:31.256919Z"},"trusted":true},"execution_count":240,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation & Performance","metadata":{}},{"cell_type":"code","source":"# Test the model to evaluate its performance\nmodel.eval()  # put model in evaluation mode\ntest_loss, test_accuracy, test_precision, test_recall, test_f1 = 0, 0, 0, 0, 0\n\n# No need to track gradients for validation\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Testing\"):\n        # Transfer to GPU\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        # Forward pass\n        outputs = model(**batch)\n\n        # Compute loss\n        loss = outputs.loss\n        test_loss += loss.item()\n\n        # Get the predictions\n        logits = outputs.logits.detach().cpu().numpy()\n        label_ids = batch['labels'].cpu().numpy()\n\n        # Calculate the evaluation metrics\n        batch_accuracy, batch_precision, batch_recall, batch_f1 = calculate_metrics(logits, label_ids)\n        test_accuracy += batch_accuracy\n        test_precision += batch_precision\n        test_recall += batch_recall\n        test_f1 += batch_f1\n\n# Compute the average loss and average metrics\navg_test_loss = test_loss / len(test_loader)\navg_test_accuracy = test_accuracy / len(test_loader)\navg_test_precision = test_precision / len(test_loader)\navg_test_recall = test_recall / len(test_loader)\navg_test_f1 = test_f1 / len(test_loader)\n\nprint(f\"Test loss: {avg_test_loss:.2f}\")\nprint(f\"Test Accuracy: {avg_test_accuracy:.2f}\")\nprint(f\"Test Precision: {avg_test_precision:.2f}\")\nprint(f\"Test Recall: {avg_test_recall:.2f}\")\nprint(f\"Test F1-Score: {avg_test_f1:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:30:34.125901Z","iopub.execute_input":"2023-12-27T23:30:34.126284Z","iopub.status.idle":"2023-12-27T23:30:52.705381Z","shell.execute_reply.started":"2023-12-27T23:30:34.126254Z","shell.execute_reply":"2023-12-27T23:30:52.704440Z"},"trusted":true},"execution_count":241,"outputs":[{"name":"stderr","text":"Testing:   0%|          | 0/461 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:   3%|         | 12/461 [00:00<00:18, 23.90it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:   3%|         | 15/461 [00:00<00:18, 24.22it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:   4%|         | 18/461 [00:00<00:18, 24.53it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:   5%|         | 21/461 [00:00<00:18, 24.34it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:   5%|         | 24/461 [00:00<00:17, 24.65it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:   6%|         | 27/461 [00:01<00:17, 24.40it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:   7%|         | 30/461 [00:01<00:17, 24.24it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:   7%|         | 33/461 [00:01<00:17, 24.53it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:   8%|         | 39/461 [00:01<00:16, 25.11it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  10%|         | 45/461 [00:01<00:16, 24.67it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  10%|         | 48/461 [00:01<00:16, 24.79it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  11%|         | 51/461 [00:02<00:16, 24.91it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  12%|        | 54/461 [00:02<00:16, 25.03it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  12%|        | 57/461 [00:02<00:16, 24.65it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  13%|        | 60/461 [00:02<00:16, 24.75it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  14%|        | 63/461 [00:02<00:16, 24.79it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  14%|        | 66/461 [00:02<00:16, 24.49it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  15%|        | 69/461 [00:02<00:16, 24.40it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  16%|        | 72/461 [00:02<00:15, 24.72it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  19%|        | 87/461 [00:03<00:14, 25.58it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  21%|        | 96/461 [00:03<00:14, 25.58it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  21%|       | 99/461 [00:03<00:14, 25.67it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  22%|       | 102/461 [00:04<00:13, 25.78it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  23%|       | 105/461 [00:04<00:13, 25.69it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  23%|       | 108/461 [00:04<00:13, 25.34it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  24%|       | 111/461 [00:04<00:13, 25.04it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  25%|       | 114/461 [00:04<00:13, 25.29it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  25%|       | 117/461 [00:04<00:13, 24.77it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  26%|       | 120/461 [00:04<00:13, 24.58it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  27%|       | 123/461 [00:04<00:13, 24.43it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  27%|       | 126/461 [00:05<00:14, 23.73it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  28%|       | 129/461 [00:05<00:14, 23.71it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  29%|       | 132/461 [00:05<00:13, 23.87it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  29%|       | 135/461 [00:05<00:13, 23.84it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  30%|       | 138/461 [00:05<00:13, 23.83it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  31%|       | 141/461 [00:05<00:13, 23.99it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  31%|       | 144/461 [00:05<00:13, 24.05it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  32%|      | 147/461 [00:05<00:13, 23.82it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  33%|      | 150/461 [00:06<00:12, 24.35it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  33%|      | 153/461 [00:06<00:12, 24.47it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  34%|      | 156/461 [00:06<00:12, 24.54it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  35%|      | 162/461 [00:06<00:12, 24.50it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  36%|      | 165/461 [00:06<00:11, 24.76it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  36%|      | 168/461 [00:06<00:11, 24.69it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  38%|      | 174/461 [00:07<00:11, 24.84it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  38%|      | 177/461 [00:07<00:11, 24.74it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  39%|      | 180/461 [00:07<00:11, 24.97it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  40%|      | 183/461 [00:07<00:11, 24.54it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  40%|      | 186/461 [00:07<00:11, 24.58it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  41%|      | 189/461 [00:07<00:11, 24.52it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  42%|     | 192/461 [00:07<00:11, 24.41it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  42%|     | 195/461 [00:07<00:10, 24.37it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  43%|     | 198/461 [00:08<00:10, 24.35it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  44%|     | 201/461 [00:08<00:10, 24.31it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  44%|     | 204/461 [00:08<00:10, 24.22it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  45%|     | 207/461 [00:08<00:10, 23.92it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  46%|     | 210/461 [00:08<00:10, 23.94it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  46%|     | 213/461 [00:08<00:10, 23.45it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  47%|     | 216/461 [00:08<00:10, 23.52it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  48%|     | 222/461 [00:09<00:09, 24.25it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  49%|     | 225/461 [00:09<00:09, 24.46it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  49%|     | 228/461 [00:09<00:09, 24.53it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  50%|     | 231/461 [00:09<00:09, 24.95it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  51%|     | 234/461 [00:09<00:09, 25.15it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  51%|    | 237/461 [00:09<00:08, 25.28it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  52%|    | 240/461 [00:09<00:08, 25.31it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  53%|    | 243/461 [00:09<00:08, 25.15it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  53%|    | 246/461 [00:09<00:08, 25.05it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  54%|    | 249/461 [00:10<00:08, 24.82it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  55%|    | 252/461 [00:10<00:08, 24.90it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  55%|    | 255/461 [00:10<00:08, 24.81it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  56%|    | 258/461 [00:10<00:08, 24.87it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  57%|    | 261/461 [00:10<00:07, 25.00it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  58%|    | 267/461 [00:10<00:07, 25.12it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  59%|    | 273/461 [00:11<00:07, 24.71it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  60%|    | 276/461 [00:11<00:07, 24.58it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  61%|    | 279/461 [00:11<00:07, 24.06it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  61%|    | 282/461 [00:11<00:07, 23.81it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  62%|   | 285/461 [00:11<00:07, 23.74it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  62%|   | 288/461 [00:11<00:07, 24.14it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  63%|   | 291/461 [00:11<00:06, 24.43it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  64%|   | 294/461 [00:11<00:06, 24.59it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  64%|   | 297/461 [00:12<00:06, 24.12it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  65%|   | 300/461 [00:12<00:06, 24.28it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  66%|   | 303/461 [00:12<00:06, 24.45it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  66%|   | 306/461 [00:12<00:06, 24.57it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  67%|   | 309/461 [00:12<00:06, 24.40it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  68%|   | 312/461 [00:12<00:06, 24.69it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  68%|   | 315/461 [00:12<00:05, 24.44it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  69%|   | 318/461 [00:12<00:05, 24.29it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  71%|   | 327/461 [00:13<00:05, 24.93it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  72%|  | 330/461 [00:13<00:05, 24.95it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  72%|  | 333/461 [00:13<00:05, 25.27it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  75%|  | 348/461 [00:14<00:04, 25.63it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  76%|  | 351/461 [00:14<00:04, 25.33it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  77%|  | 354/461 [00:14<00:04, 25.36it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  78%|  | 360/461 [00:14<00:04, 25.16it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  80%|  | 369/461 [00:14<00:03, 25.14it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  81%|  | 372/461 [00:15<00:03, 25.03it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  83%| | 381/461 [00:15<00:03, 25.10it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  83%| | 384/461 [00:15<00:03, 25.12it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  85%| | 393/461 [00:15<00:02, 25.35it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  86%| | 396/461 [00:16<00:02, 25.56it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  87%| | 399/461 [00:16<00:02, 25.68it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  88%| | 405/461 [00:16<00:02, 25.71it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  89%| | 408/461 [00:16<00:02, 25.66it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  89%| | 411/461 [00:16<00:01, 25.83it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  91%| | 420/461 [00:16<00:01, 25.71it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  92%|| 423/461 [00:17<00:01, 25.64it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  94%|| 432/461 [00:17<00:01, 25.15it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  94%|| 435/461 [00:17<00:01, 25.17it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  95%|| 438/461 [00:17<00:00, 25.50it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  96%|| 441/461 [00:17<00:00, 25.61it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  96%|| 444/461 [00:17<00:00, 25.66it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  97%|| 447/461 [00:18<00:00, 25.83it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  98%|| 450/461 [00:18<00:00, 25.85it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  98%|| 453/461 [00:18<00:00, 25.30it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting:  99%|| 456/461 [00:18<00:00, 24.97it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting: 100%|| 459/461 [00:18<00:00, 25.17it/s]/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTesting: 100%|| 461/461 [00:18<00:00, 24.83it/s]","output_type":"stream"},{"name":"stdout","text":"Test loss: 0.22\nTest Accuracy: 0.95\nTest Precision: 0.77\nTest Recall: 0.74\nTest F1-Score: 0.74\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torchcrf","metadata":{"execution":{"iopub.status.busy":"2023-12-27T22:14:01.360055Z","iopub.execute_input":"2023-12-27T22:14:01.360789Z","iopub.status.idle":"2023-12-27T22:14:13.384494Z","shell.execute_reply.started":"2023-12-27T22:14:01.360755Z","shell.execute_reply":"2023-12-27T22:14:13.383259Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting torchcrf\n  Downloading TorchCRF-1.1.0-py3-none-any.whl (5.2 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchcrf) (1.24.3)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchcrf) (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchcrf) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchcrf) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchcrf) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchcrf) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchcrf) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchcrf) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchcrf) (1.3.0)\nInstalling collected packages: torchcrf\nSuccessfully installed torchcrf-1.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:31:11.456672Z","iopub.execute_input":"2023-12-27T23:31:11.457469Z","iopub.status.idle":"2023-12-27T23:31:11.462473Z","shell.execute_reply.started":"2023-12-27T23:31:11.457436Z","shell.execute_reply":"2023-12-27T23:31:11.461481Z"},"trusted":true},"execution_count":243,"outputs":[]},{"cell_type":"markdown","source":"# Define and Instantiate the BiLSTM Model","metadata":{}},{"cell_type":"code","source":"#BiLSTMForTokenClassification Model Definition\nclass BiLSTMForTokenClassification(nn.Module):\n    def __init__(self, num_labels, hidden_size=128, num_layers=2, dropout=0.1):\n        super(BiLSTMForTokenClassification, self).__init__()\n        self.embedding = nn.Embedding(tokenizer.vocab_size, hidden_size)\n        self.bilstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(2 * hidden_size, num_labels)\n\n    def forward(self, input_ids):\n        embeddings = self.embedding(input_ids)\n        embeddings = self.dropout(embeddings)\n        lstm_output, _ = self.bilstm(embeddings)\n        logits = self.fc(lstm_output)\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:31:14.416877Z","iopub.execute_input":"2023-12-27T23:31:14.417537Z","iopub.status.idle":"2023-12-27T23:31:14.424950Z","shell.execute_reply.started":"2023-12-27T23:31:14.417506Z","shell.execute_reply":"2023-12-27T23:31:14.423912Z"},"trusted":true},"execution_count":244,"outputs":[]},{"cell_type":"code","source":"#Model Instantiation and Optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbilstm_model = BiLSTMForTokenClassification(num_labels=num_labels)\nbilstm_model.to(device)\nbilstm_optimizer = optim.AdamW(bilstm_model.parameters(), lr=5e-5)\nbilstm_scheduler = torch.optim.lr_scheduler.StepLR(bilstm_optimizer, step_size=10, gamma=0.5)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:31:17.575940Z","iopub.execute_input":"2023-12-27T23:31:17.576283Z","iopub.status.idle":"2023-12-27T23:31:17.620431Z","shell.execute_reply.started":"2023-12-27T23:31:17.576259Z","shell.execute_reply":"2023-12-27T23:31:17.619683Z"},"trusted":true},"execution_count":245,"outputs":[]},{"cell_type":"markdown","source":"# BiLSTM Training Loop","metadata":{}},{"cell_type":"code","source":"# Training Loop\nepochs = 3\nbilstm_model.train()\nfor epoch in range(epochs):\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n\n        bilstm_optimizer.zero_grad()\n        logits = bilstm_model(input_ids)\n\n        loss_fn = nn.CrossEntropyLoss()\n        loss = loss_fn(logits.view(-1, num_labels), labels.view(-1))\n        total_loss += loss.item()\n\n        loss.backward()\n        bilstm_optimizer.step()\n        bilstm_scheduler.step()\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Average training loss: {avg_train_loss:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:31:20.452931Z","iopub.execute_input":"2023-12-27T23:31:20.453740Z","iopub.status.idle":"2023-12-27T23:32:20.445803Z","shell.execute_reply.started":"2023-12-27T23:31:20.453709Z","shell.execute_reply":"2023-12-27T23:32:20.444833Z"},"trusted":true},"execution_count":246,"outputs":[{"name":"stderr","text":"Training Epoch 0: 100%|| 1874/1874 [00:20<00:00, 93.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 2.12\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1: 100%|| 1874/1874 [00:19<00:00, 93.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 2.12\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|| 1874/1874 [00:19<00:00, 93.97it/s]","output_type":"stream"},{"name":"stdout","text":"Average training loss: 2.12\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BiLSTM Evaluation Loop","metadata":{}},{"cell_type":"code","source":"#BiLSTM Evaluation Loop\nfrom sklearn.metrics import f1_score, accuracy_score\nimport numpy as np\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# Function to calculate metrics for all batches\ndef compute_metrics(predictions, true_labels):\n    # Flatten all the predictions and true values\n    predictions = np.concatenate(predictions, axis=0)\n    true_labels = np.concatenate(true_labels, axis=0)\n    \n    # Only evaluate on non-ignored indices (-100 is used for ignored)\n    valid_indices = true_labels != -100\n    predictions = predictions[valid_indices]\n    true_labels = true_labels[valid_indices]\n    \n    f1 = f1_score(true_labels, predictions, average='macro')\n    accuracy = accuracy_score(true_labels, predictions)\n    \n    return f1, accuracy\n\n# Place the model in evaluation mode\nbilstm_model.eval()\n\n# Initialize lists to gather full prediction and label sets\nall_predictions, all_labels = [], []\n\n# Evaluate data for one epoch\nfor batch in tqdm(valid_loader, desc=\"Evaluating\"):\n    # Add batch to device\n    batch = {k: v.to(device) for k, v in batch.items()}\n\n    # Telling the model not to compute or store gradients\n    with torch.no_grad():\n        # Forward pass, calculate logit predictions\n        outputs = bilstm_model(batch['input_ids'])\n\n    # Move logits and labels to CPU\n    logits = outputs.detach().cpu().numpy()\n    label_ids = batch['labels'].to('cpu').numpy()\n\n    # Store predictions and true labels\n    all_predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n    all_labels.extend([list(l) for l in label_ids])\n\n# Calculate metrics\nf1, accuracy = compute_metrics(all_predictions, all_labels)\n\nprint(f\"Validation F1-Score: {f1:.2f}\")\nprint(f\"Validation Accuracy: {accuracy:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T22:14:47.226263Z","iopub.execute_input":"2023-12-27T22:14:47.227128Z","iopub.status.idle":"2023-12-27T22:14:47.307753Z","shell.execute_reply.started":"2023-12-27T22:14:47.227087Z","shell.execute_reply":"2023-12-27T22:14:47.306844Z"},"trusted":true},"execution_count":181,"outputs":[{"name":"stderr","text":"Evaluating: 100%|| 7/7 [00:00<00:00, 114.43it/s]","output_type":"stream"},{"name":"stdout","text":"torch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128])\nValidation F1-Score: 0.11\nValidation Accuracy: 0.79\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save BiLSTM Model","metadata":{}},{"cell_type":"code","source":"# Save the entire model\ntorch.save(bilstm_model, '/kaggle/working/my_bilstm_model.pth')\n\nprint('Model saved to /kaggle/working/my_bilstm_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:44:36.455965Z","iopub.execute_input":"2023-12-27T23:44:36.456724Z","iopub.status.idle":"2023-12-27T23:44:36.500298Z","shell.execute_reply.started":"2023-12-27T23:44:36.456690Z","shell.execute_reply":"2023-12-27T23:44:36.498876Z"},"trusted":true},"execution_count":247,"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/my_bilstm_model.pth\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Summaries","metadata":{}},{"cell_type":"code","source":"from torchinfo import summary\n\n# Assuming your model variable holds the BertForTokenClassification model\n# and that you have a batch of data available in input_ids\n# We will create a batch of dummy data with two input sequences for the purpose of model summary\ninput_ids = tokenizer(['Hello, this is a test.', 'Here is another sentence.'], \n                      padding=True, truncation=True, max_length=128, return_tensors=\"pt\").input_ids\n\n# Move the tensor to the appropriate device\ninput_ids = input_ids.to(device)\n\n# Use torchinfo to print the summary\nbert_model_summary = summary(model, input_data=(input_ids,), verbose=0)  # You can set verbose to 1 for more detail\nprint(bert_model_summary)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:44:39.128760Z","iopub.execute_input":"2023-12-27T23:44:39.129145Z","iopub.status.idle":"2023-12-27T23:44:39.223372Z","shell.execute_reply.started":"2023-12-27T23:44:39.129116Z","shell.execute_reply":"2023-12-27T23:44:39.222478Z"},"trusted":true},"execution_count":248,"outputs":[{"name":"stdout","text":"=========================================================================================================\nLayer (type:depth-idx)                                  Output Shape              Param #\n=========================================================================================================\nBertForTokenClassification                              [2, 9, 9]                 --\nBertModel: 1-1                                        [2, 9, 768]               --\n    BertEmbeddings: 2-1                              [2, 9, 768]               --\n        Embedding: 3-1                              [2, 9, 768]               23,440,896\n        Embedding: 3-2                              [2, 9, 768]               1,536\n        Embedding: 3-3                              [1, 9, 768]               393,216\n        LayerNorm: 3-4                              [2, 9, 768]               1,536\n        Dropout: 3-5                                [2, 9, 768]               --\n    BertEncoder: 2-2                                 [2, 9, 768]               --\n        ModuleList: 3-6                             --                        85,054,464\nDropout: 1-2                                          [2, 9, 768]               --\nLinear: 1-3                                           [2, 9, 9]                 6,921\n=========================================================================================================\nTotal params: 108,898,569\nTrainable params: 108,898,569\nNon-trainable params: 0\nTotal mult-adds (M): 217.40\n=========================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 14.99\nParams size (MB): 435.59\nEstimated Total Size (MB): 450.58\n=========================================================================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torchinfo import summary\n\n# Load the BiLSTM model\nbilstm_model = torch.load('/kaggle/working/my_bilstm_model.pth')\n\n# Assuming the BiLSTM expects the same input_ids as the BERT model\n# Generate BiLSTM model summary\nbilstm_model_summary = summary(bilstm_model, input_data=(input_ids.to(device),), verbose=0)\nprint(bilstm_model_summary)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:44:43.581608Z","iopub.execute_input":"2023-12-27T23:44:43.582371Z","iopub.status.idle":"2023-12-27T23:44:43.603026Z","shell.execute_reply.started":"2023-12-27T23:44:43.582338Z","shell.execute_reply":"2023-12-27T23:44:43.602038Z"},"trusted":true},"execution_count":249,"outputs":[{"name":"stdout","text":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nBiLSTMForTokenClassification             [2, 9, 9]                 --\nEmbedding: 1-1                         [2, 9, 128]               3,906,816\nDropout: 1-2                           [2, 9, 128]               --\nLSTM: 1-3                              [2, 9, 256]               659,456\nLinear: 1-4                            [2, 9, 9]                 2,313\n==========================================================================================\nTotal params: 4,568,585\nTrainable params: 4,568,585\nNon-trainable params: 0\nTotal mult-adds (M): 19.69\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.06\nParams size (MB): 18.27\nEstimated Total Size (MB): 18.33\n==========================================================================================\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Models Prediction","metadata":{}},{"cell_type":"code","source":"# Load the trained BERT-based model (if not already loaded)\nfrom transformers import BertForTokenClassification\n\nbert_model = BertForTokenClassification.from_pretrained('/kaggle/working/my_bert_model')\n\n# Load the trained BiLSTM-based model (if not already loaded)\n# Assuming you have previously saved it using torch.save\nimport torch\n\nbilstm_model = torch.load('/kaggle/working/my_bilstm_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:46:56.551621Z","iopub.execute_input":"2023-12-27T23:46:56.552354Z","iopub.status.idle":"2023-12-27T23:46:56.789064Z","shell.execute_reply.started":"2023-12-27T23:46:56.552322Z","shell.execute_reply":"2023-12-27T23:46:56.788238Z"},"trusted":true},"execution_count":252,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizerFast\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\n# Example input text\ninput_text = \"Ahmed lives in New York City in US\"\n\n# Load the tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# Define a function to make predictions using the BERT-based model\ndef predict_with_bert_model(model, text):\n    # Tokenize the input text\n    inputs = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n    \n    # Forward pass through the model\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Get the predicted labels\n    predicted_labels = np.argmax(outputs.logits, axis=2).tolist()[0]\n    \n    return predicted_labels\n\n\n\n# Make predictions using the BERT-based model\nbert_predictions = predict_with_bert_model(bert_model, input_text)\n\nprint(\"BERT Predictions:\", bert_predictions)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:55:44.818766Z","iopub.execute_input":"2023-12-27T23:55:44.819183Z","iopub.status.idle":"2023-12-27T23:55:45.023618Z","shell.execute_reply.started":"2023-12-27T23:55:44.819153Z","shell.execute_reply":"2023-12-27T23:55:45.022629Z"},"trusted":true},"execution_count":270,"outputs":[{"name":"stdout","text":"Ahmed lives in New York City in US\nBERT Predictions: [8, 3, 8, 0, 0, 8, 8, 8, 8, 3]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ndef predict_with_bilstm_model(model, text):\n    # Tokenize the input text\n    inputs = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n\n    # Move the inputs to the appropriate device\n    input_ids = inputs['input_ids'].to(model.embedding.weight.device)  # Use the device of the embedding layer\n\n    # Forward pass through the model\n    with torch.no_grad():\n        logits = model(input_ids)\n\n    # Get the predicted labels\n    predicted_labels = torch.argmax(logits, dim=2).squeeze().tolist()\n    return predicted_labels\n\n# Example usage:\ninput_text = \"Ahmed lives in New York City in US\"\nbilstm_predictions = predict_with_bilstm_model(bilstm_model, input_text)\nprint(input_text)\n\nprint(\"BiLSTM Predictions:\", bilstm_predictions)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:56:02.431060Z","iopub.execute_input":"2023-12-27T23:56:02.431790Z","iopub.status.idle":"2023-12-27T23:56:02.440866Z","shell.execute_reply.started":"2023-12-27T23:56:02.431757Z","shell.execute_reply":"2023-12-27T23:56:02.440021Z"},"trusted":true},"execution_count":271,"outputs":[{"name":"stdout","text":"Ahmed lives in New York City in US\nBiLSTM Predictions: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_bert(model, tokenizer, sentence):\n    # Tokenize the input sentence\n    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n\n    # Move tensors to the appropriate device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    # Put the model in evaluation mode and disable gradient calculation\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Get the logits and apply softmax to get probabilities\n    logits = outputs.logits\n    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    predictions = torch.argmax(probabilities, dim=-1)\n\n    return predictions[0].cpu().numpy()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:59:22.668870Z","iopub.execute_input":"2023-12-27T23:59:22.669538Z","iopub.status.idle":"2023-12-27T23:59:22.676640Z","shell.execute_reply.started":"2023-12-27T23:59:22.669507Z","shell.execute_reply":"2023-12-27T23:59:22.675541Z"},"trusted":true},"execution_count":273,"outputs":[]},{"cell_type":"code","source":"def predict_bilstm(model, tokenizer, sentence):\n    # Tokenize the input sentence\n    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n\n    # Extract input_ids and move to the appropriate device\n    input_ids = inputs['input_ids'].to(device)\n\n    # Put the model in evaluation mode and disable gradient calculation\n    model.eval()\n    with torch.no_grad():\n        logits = model(input_ids)\n\n    # Apply softmax to get probabilities and then get predictions\n    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    predictions = torch.argmax(probabilities, dim=-1)\n\n    return predictions[0].cpu().numpy()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T23:59:45.249809Z","iopub.execute_input":"2023-12-27T23:59:45.250183Z","iopub.status.idle":"2023-12-27T23:59:45.257009Z","shell.execute_reply.started":"2023-12-27T23:59:45.250153Z","shell.execute_reply":"2023-12-27T23:59:45.256032Z"},"trusted":true},"execution_count":274,"outputs":[]},{"cell_type":"code","source":"sentence = \"Ahmed lives in New York City in US.\"\nbert_predictions = predict_bert(model, tokenizer, sentence)\nbilstm_predictions = predict_bilstm(bilstm_model, tokenizer, sentence)\n\n# Convert predictions to tag names\nbert_tags = [int_to_str_tag_map[p] for p in bert_predictions]\nbilstm_tags = [int_to_str_tag_map[p] for p in bilstm_predictions]\n\nprint(\"BERT Predictions:\", bert_tags)\nprint(\"BiLSTM Predictions:\", bilstm_tags)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-28T00:00:16.716962Z","iopub.execute_input":"2023-12-28T00:00:16.717391Z","iopub.status.idle":"2023-12-28T00:00:16.740163Z","shell.execute_reply.started":"2023-12-28T00:00:16.717358Z","shell.execute_reply":"2023-12-28T00:00:16.739264Z"},"trusted":true},"execution_count":275,"outputs":[{"name":"stdout","text":"BERT Predictions: ['O', 'B-PER', 'O', 'B-LOC', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-PER']\nBiLSTM Predictions: ['I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'I-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prediction Results Summary\n\n### BERT Model\n- **Non-Entities**: Tokens marked as `'O'` signify non-entities.\n- **Persons**: `'B-PER'` tags indicate the start of a person's name.\n- **Locations**: `'B-LOC'` tags denote the beginning of a location name.\n\n### BiLSTM Model\n- **Inside Locations**: `'I-LOC'` tags represent tokens within a location name.\n- **Start of Locations**: `'B-LOC'` tags mark the start of a location name.\n\n### Insights\n- **BERT**: Shows a balance in identifying persons and locations.\n- **BiLSTM**: More focused on identifying location entities.\n- **Observation**: The variation in predictions between the models underscores their different learning patterns and sensitivities.\n","metadata":{}},{"cell_type":"markdown","source":"## Interpretation of Model Predictions\n\nYou may notice that there are differences in the predictions made by the BiLSTM and BERT models for the same input sentence. This is expected and can be attributed to several factors related to the models' architectures, training data, and tokenization process:\n\n1. **Model Architecture**: The BiLSTM and BERT models have different architectures. BiLSTM is a recurrent neural network (RNN), while BERT is a transformer-based model. These architectural differences can lead to variations in predictions.\n\n2. **Pre-trained Word Embeddings**: BERT uses pre-trained word embeddings that capture rich contextual information from a large corpus of text. In contrast, the BiLSTM model might use different embeddings or rely more on the training data provided, resulting in different understandings of word meanings in context.\n\n3. **Training Data**: The two models may have been trained on different datasets. The quality, quantity, and diversity of the training data can impact a model's performance and its ability to make predictions.\n\n4. **Fine-tuning**: If you fine-tuned the BERT model on a specific task or dataset, it might have learned task-specific patterns that the BiLSTM model doesn't possess.\n\n5. **Tokenization**: Both models use tokenization, but the tokenizers might tokenize the input sentence differently, affecting the input representations and predictions.\n\n6. **Inference Differences**: The way you perform inference and post-processing for each model can also lead to differences in the final predictions.\n\nIn conclusion, it's normal for different models to produce slightly different results due to their unique characteristics and training. The choice of which model to use should be based on the specific task and requirements.\n","metadata":{}}]}